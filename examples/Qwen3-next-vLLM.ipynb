{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Qwen3-next models with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to download and run `Qwen3-next` model using vLLM on NVIDIA GPUs for high-performance inference. vLLM is an open-source library that makes Large Language Model (LLM) inference and serving faster and more efficient by using an advanced memory management and continuous batching. It significantly increases model throughput, reduces GPU memory usage, and lowers infrastructure costs, making it a key tool for deploying LLMs at scale.\n",
    "\n",
    "`Qwen3-next`is a brand-new mdoel architecture that introduces several key improvements over its predesessor: a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. It is an 80-billion-parameter model that activates only 3 billion parameters during inference. Refer to the [model card](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) for more details. The `Qwen3-next` has two variants:\n",
    "\n",
    "- `Qwen3-Next-80B-A3B-Instruct`\n",
    "- `Qwen3-Next-80B-A3B-Thinking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get start with this guide\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vo0LNrj2pGPwXT7m5KYmqmJ0y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "To run the `Qwen3-Next-80B-A3B-Instruct` model, you will need an 4x A100 or 4xH200 NVIDIA \n",
    "\n",
    "### Software\n",
    "- CUDA Toolkit 12.8 or later\n",
    "- Python 3.12 or later\n",
    "- vllm 0.10.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing vLLM\n",
    "\n",
    "To run `Qwen3-Next` models you will need to install the nightly build of vLLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python checking\n",
    "\n",
    "Checking python version and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U pip wheel setuptools uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM via pip\n",
    "%uv pip install vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU[{i}]: {props.name} | SM count: {props.multi_processor_count} | Mem: {props.total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Model with Multi-GPU setup\n",
    "\n",
    "There are multiple ways to load the model. \n",
    "- Activating a server using vllm serve\n",
    "- vLLM python client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `vllm serve` You can run the below command from command line or use subprocess to run it from cell. \n",
    "`vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct \\`\n",
    "  `--tensor-parallel-size 4 \\`\n",
    "  `--served-model-name qwen3-next `\n",
    "\n",
    "We will running it from the cell and hence will be wrapping it with subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait around 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"vllm\", \"serve\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\",\n",
    "    \"--tensor-parallel-size\", \"4\",\n",
    "    \"--served-model-name\", \"qwen3-next\",\n",
    "    \"--host\", \"0.0.0.0\", \"--port\", \"8000\"\n",
    "]\n",
    "\n",
    "subprocess.Popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing using vllm server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using vllm server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def inference(user_prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"qwen3-next\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "user_prompt = \" What is the capital of France and why do people travel go there? \"\n",
    "output = inference(user_prompt)\n",
    "result = output['choices'][0]['message']['content']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using vLLM python client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the previous instance (we have to look better way than kill process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps aux | grep vllm\n",
    "!kill -9 41393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    dtype=\"bfloat16\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=65536,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    tensor_parallel_size=4,\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate: single and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"What is Nemotron Super?\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "Congratulations! You successfully deployed `Qwen3-Next` using vLLM.\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "- Set up your environment with the necessary dependencies.\n",
    "- Use vllm serve to deploy the model.\n",
    "- Run inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
