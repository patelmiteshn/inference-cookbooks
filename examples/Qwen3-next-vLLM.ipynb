{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Qwen3-next models with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to download and run the `Qwen3-Next` models using vLLM on NVIDIA GPUs for high-performance inference. vLLM is an open-source library that makes Large Language Model (LLM) inference and serving faster and more efficient by using advanced memory management and continuous batching. It significantly increases model throughput, reduces GPU memory usage, and lowers infrastructure costs, making it a key tool for deploying LLMs at scale.\n",
    "\n",
    "`Qwen3-Next` is a brand-new model architecture that introduces several key improvements over its predecessor: a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. It is an 80-billion-parameter model that activates only ~3 billion parameters during inference. Refer to the [model card](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) for more details. The `Qwen3-Next` series has two variants:\n",
    "\n",
    "- `Qwen3-Next-80B-A3B-Instruct`\n",
    "- `Qwen3-Next-80B-A3B-Thinking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Installing vLLM](#Installing-vLLM)\n",
    "- [Part 1: Qwen3-Next-Instruct with vLLM](#Part-1:-Qwen3-Next-Instruct-with-vLLM)\n",
    "  - [Launch Instruct Model Server](#Launch-Instruct-Model-Server)\n",
    "  - [Inference using vLLM Server](#Inference-using-vLLM-Server)\n",
    "  - [Inference using vLLM Python Client](#Inference-using-vLLM-Python-Client)\n",
    "- [Part 2: Qwen3-Next-Thinking with vLLM](#Part-2:-Qwen3-Next-Thinking-with-vLLM)\n",
    "  - [Launch Thinking Model Server](#Launch-Thinking-Model-Server)\n",
    "  - [Inference against Thinking Server](#Inference-against-Thinking-Server)\n",
    "  - [Batch Inference against Thinking Server](#Batch-Inference-against-Thinking-Server)\n",
    "- [Conclusion and Next Steps](#Conclusion-and-Next-Steps)\n",
    "- [Resource Notes](#Resource-Notes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "To run the `Qwen3-Next-80B-A3B` models (both `Instruct` and `Thinking`), you will need 4x A100 or 4x H100 NVIDIA GPUs.\n",
    "\n",
    "### Software\n",
    "- CUDA Toolkit 12.1 or later\n",
    "- Python 3.10 or later\n",
    "- vLLM (latest nightly build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing vLLM\n",
    "\n",
    "To run `Qwen3-Next` models you will need to install the nightly build of vLLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Python Environment\n",
    "This notebook requires a Python 3.10+ environment. The following cell prints your kernel's Python version and executable path to confirm you are in the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.11 (main, Sep 18 2025, 19:47:19) [Clang 20.1.4 ]\n",
      "/home/shadeform/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.10.2rc3.dev279+gddc904839 requires setuptools<80,>=77.0.3; python_version > \"3.11\", but you have setuptools 80.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#if you're running this on a brev instance, you may need to install pip\n",
    "%python -m ensurepip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip wheel setuptools uv --quiet\n",
    "%pip install vllm openai aiohttp --extra-index-url https://wheels.vllm.ai/nightly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Num GPUs: 4\n",
      "GPU[0]: NVIDIA H100 PCIe | SM count: 114 | Mem: 85.03 GB\n",
      "GPU[1]: NVIDIA H100 PCIe | SM count: 114 | Mem: 85.03 GB\n",
      "GPU[2]: NVIDIA H100 PCIe | SM count: 114 | Mem: 85.03 GB\n",
      "GPU[3]: NVIDIA H100 PCIe | SM count: 114 | Mem: 85.03 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU[{i}]: {props.name} | SM count: {props.multi_processor_count} | Mem: {props.total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Qwen3-Next-Instruct with vLLM\n",
    "\n",
    "This part of the notebook demonstrates two ways to run inference with the `Instruct` model:\n",
    "1. Launching a vLLM server and making HTTP requests to it.\n",
    "2. Using the vLLM Python client directly for in-process inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Instruct Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started vLLM instruct server, PID=60752\n",
      "Waiting for server to initialize... (approx. 5 minutes)\n",
      "INFO 09-19 23:20:27 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:30 [api_server.py:1896] vLLM API server version 0.10.2\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:30 [utils.py:328] non-default args: {'model_tag': 'Qwen/Qwen3-Next-80B-A3B-Instruct', 'host': '0.0.0.0', 'model': 'Qwen/Qwen3-Next-80B-A3B-Instruct', 'served_model_name': ['qwen3-next'], 'tensor_parallel_size': 4}\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [__init__.py:742] Resolved architecture: Qwen3NextForCausalLM\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [__init__.py:1815] Using max model len 262144\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [config.py:310] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [config.py:321] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [config.py:390] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:20:37 [config.py:411] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.\n",
      "INFO 09-19 23:20:41 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:20:43 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:20:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen3-Next-80B-A3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-Next-80B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3-next, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m WARNING 09-19 23:20:43 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 124 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:20:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c70e0179'), local_subscribe_addr='ipc:///tmp/82c8eb14-cfa1-4280-9e96-b4ffb07ba01e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:20:46 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:20:47 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:20:47 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:20:47 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:20:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0a278787'), local_subscribe_addr='ipc:///tmp/00a5015e-8e79-47fa-8b5c-9f3f1ccd250f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:20:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_370a4178'), local_subscribe_addr='ipc:///tmp/dba4deb2-4b2d-41c5-83ab-067e7a2571b5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:20:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e3692c89'), local_subscribe_addr='ipc:///tmp/fa2a223c-e3b4-41b5-80cf-85444a255277', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:20:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1df40fd7'), local_subscribe_addr='ipc:///tmp/f2f760ad-eb11-4274-8d7e-f9c973bb2e8e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W919 23:20:52.481271408 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:20:52.486181543 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:20:53.916981496 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:20:53.917301256 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 23:20:53 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:20:53 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:20:53 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:20:53 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:20:53 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:20:53 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:20:53 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:20:53 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "WARNING 09-19 23:20:55 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:20:55 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:20:55 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:20:55 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 09-19 23:20:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9e79b2a0'), local_subscribe_addr='ipc:///tmp/ca30a6c2-f1ad-45c3-a5d5-54908540d9f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 23:20:55 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-19 23:20:55 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-19 23:20:55 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-19 23:20:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 23:20:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 23:20:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-19 23:20:55 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "WARNING 09-19 23:20:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:20:55 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:20:55 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:20:55 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:20:55 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:20:55 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:20:56 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:20:56 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:20:56 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:20:56 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:20:56 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:20:56 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:20:56 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:20:56 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:20:56 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:20:56 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:20:56 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:16,  2.44it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:16,  2.33it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:17,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:17,  2.16it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:02<00:16,  2.13it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:17,  2.04it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:03<00:16,  2.08it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:03<00:15,  2.10it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:04<00:15,  2.08it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:04<00:13,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:05<00:10,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:05<00:10,  2.64it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:06<00:10,  2.50it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:06<00:11,  2.32it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:07<00:11,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:07<00:10,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:07<00:10,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:08<00:09,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:08<00:09,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:09<00:09,  2.21it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:09<00:08,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:10<00:08,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:10<00:07,  2.21it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:11<00:07,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:11<00:07,  2.11it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:12<00:06,  2.09it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:12<00:06,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:13<00:05,  2.08it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:13<00:05,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:14<00:04,  2.01it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:14<00:04,  1.98it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:15<00:04,  1.95it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:15<00:03,  1.92it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:16<00:03,  1.97it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:16<00:02,  2.03it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:17<00:02,  1.99it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:17<00:01,  2.01it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:18<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:18<00:00,  2.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:19<00:00,  2.10it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:19<00:00,  2.16it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:21:15 [default_loader.py:268] Loading weights took 19.07 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:21:16 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 19.682496 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:21:16 [default_loader.py:268] Loading weights took 19.91 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:21:16 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 20.554601 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:21:23 [default_loader.py:268] Loading weights took 26.76 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:21:24 [default_loader.py:268] Loading weights took 27.24 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:21:24 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 27.695779 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:21:24 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 28.061191 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:21:29 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/eeca88fe0a/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:21:29 [backends.py:550] Dynamo bytecode transform time: 4.53 s\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:21:30 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/eeca88fe0a/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:550] Dynamo bytecode transform time: 6.36 s\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/eeca88fe0a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:550] Dynamo bytecode transform time: 6.51 s\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/eeca88fe0a/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:21:31 [backends.py:550] Dynamo bytecode transform time: 6.73 s\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:21:32 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:21:32 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:21:32 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:22:01 [backends.py:215] Compiling a graph for dynamic shape takes 31.63 s\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:22:03 [backends.py:215] Compiling a graph for dynamic shape takes 32.02 s\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:22:05 [backends.py:215] Compiling a graph for dynamic shape takes 33.02 s\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:22:19 [backends.py:215] Compiling a graph for dynamic shape takes 47.24 s\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m WARNING 09-19 23:22:20 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m WARNING 09-19 23:22:20 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m WARNING 09-19 23:22:20 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m WARNING 09-19 23:22:20 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:22:21 [monitor.py:34] torch.compile takes 39.75 s in total\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:22:21 [monitor.py:34] torch.compile takes 38.39 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:22:21 [monitor.py:34] torch.compile takes 36.16 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:22:21 [monitor.py:34] torch.compile takes 53.75 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:22:22 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:22:23 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:22:23 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:22:23 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:22:23 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:11<00:00,  5.66it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 67/67 [00:49<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:23:26 [gpu_model_runner.py:3118] Graph capturing finished in 62 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:23:26 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:23:26 [gpu_model_runner.py:3118] Graph capturing finished in 62 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:23:26 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:23:26 [gpu_model_runner.py:3118] Graph capturing finished in 62 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:23:26 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:23:26 [gpu_model_runner.py:3118] Graph capturing finished in 62 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:23:26 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=61201)\u001b[0;0m INFO 09-19 23:23:26 [core.py:218] init engine (profile, create kv cache, warmup model) took 121.92 seconds\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 8958\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m WARNING 09-19 23:23:27 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [api_server.py:1971] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:36] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:23:27 [launcher.py:44] Route: /metrics, Methods: GET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Started server process [60752]\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=61350)\u001b[0;0m INFO 09-19 23:24:35 [multiproc_executor.py:546] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP1 pid=61351)\u001b[0;0m INFO 09-19 23:24:35 [multiproc_executor.py:546] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP2 pid=61352)\u001b[0;0m INFO 09-19 23:24:35 [multiproc_executor.py:546] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP3 pid=61353)\u001b[0;0m INFO 09-19 23:24:35 [multiproc_executor.py:546] Parent process exited, terminating worker\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m WARNING 09-19 23:24:35 [launcher.py:98] port 8000 is used by process psutil.Process(pid=60752, name='vllm', status='running', started='23:20:24') launched with command:\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m WARNING 09-19 23:24:35 [launcher.py:98] /home/shadeform/miniconda3/envs/vllm/bin/python3.10 /home/shadeform/miniconda3/envs/vllm/bin/vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --tensor-parallel-size 4 --served-model-name qwen3-next --host 0.0.0.0 --port 8000\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO 09-19 23:24:35 [launcher.py:101] Shutting down FastAPI HTTP server.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Wait for the server to be ready.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for server to initialize... (approx. 5 minutes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM instruct server should be ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Shutting down\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Waiting for application shutdown.\n",
      "\u001b[1;36m(APIServer pid=60752)\u001b[0;0m INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "serve_cmd_instruct = [\n",
    "    \"vllm\", \"serve\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\",\n",
    "    \"--tensor-parallel-size\", \"4\",\n",
    "    \"--served-model-name\", \"qwen3-next\",\n",
    "    \"--host\", \"0.0.0.0\", \"--port\", \"8000\"\n",
    "]\n",
    "\n",
    "instruct_process = subprocess.Popen(serve_cmd_instruct)\n",
    "print(f\"Started vLLM instruct server, PID={instruct_process.pid}\")\n",
    "\n",
    "# Wait for the server to be ready.\n",
    "print(\"Waiting for server to initialize... (approx. 5 minutes)\")\n",
    "time.sleep(300)\n",
    "print(\"vLLM instruct server should be ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using vLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n",
      "\n",
      "People travel to Paris for a wide variety of reasons, thanks to its rich cultural, historical, and aesthetic appeal. Here are some of the main reasons:\n",
      "\n",
      "1. **Iconic Landmarks**: Paris is home to world-famous attractions such as the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum (which houses the Mona Lisa and Venus de Milo), the Arc de Triomphe, and Montmartre with the Sacré-Cœur Basilica.\n",
      "\n",
      "2. **Art and Culture**: Paris has long been a global center for art, fashion, and literature. It boasts over 100 museums, including the Musée d’Orsay and Centre Pompidou, and hosts major art exhibitions and fashion weeks.\n",
      "\n",
      "3. **Cuisine**: French cuisine is celebrated worldwide, and Paris offers everything from Michelin-starred restaurants to cozy cafés and bustling markets. Visitors come to enjoy croissants, baguettes, cheese, wine, and pastries like macarons and éclairs.\n",
      "\n",
      "4. **Romance**: Paris is often called “The City of Love,” making it a top destination for couples. Its charming streets, riverside walks along the Seine, and romantic ambiance attract lovers from around the globe.\n",
      "\n",
      "5. **History and Architecture**: The city’s well-preserved historic districts, such as the Latin Quarter and Le Marais, along with its grand boulevards and Haussmann-style buildings, offer a glimpse into centuries of European history.\n",
      "\n",
      "6. **Shopping**: Paris is a fashion capital, with luxury boutiques on the Champs-Élysées, designer stores in Saint-Germain-des-Prés, and charming vintage shops scattered throughout the city.\n",
      "\n",
      "7. **Education and Intellectual Heritage**: Paris has been a hub for philosophers, writers, and thinkers (like Sartre, de Beauvoir, and Victor Hugo). It’s home to prestigious institutions like the Sorbonne and attracts students and scholars worldwide.\n",
      "\n",
      "8. **Events and Festivals**: From the Bastille Day fireworks to the Paris Marathon and Nuit Blanche (an all-night arts festival), the city hosts numerous events that draw international visitors.\n",
      "\n",
      "In short, people travel to Paris because it offers a unique blend of beauty, history, culture, and lifestyle that few other cities can match — making it one of the most visited cities in the world.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "user_prompt = \"What is the capital of France and why do people travel go there?\"\n",
    "\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"qwen3-next\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "output = response.json()\n",
    "result = output['choices'][0]['message']['content']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Instruct Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=38282)\u001b[0;0m INFO:     Shutting down\n",
      "\u001b[1;36m(APIServer pid=38282)\u001b[0;0m INFO:     Waiting for application shutdown.\n",
      "\u001b[1;36m(APIServer pid=38282)\u001b[0;0m INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Shutdown of the instruct server\n",
    "if 'instruct_process' in globals() and instruct_process.poll() is None:\n",
    "    instruct_process.kill()\n",
    "    print(f\"Killed instruct server PID {instruct_process.pid}\")\n",
    "else:\n",
    "    print(\"No running instruct server process found to terminate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using vLLM Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadeform/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 22:49:52 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 22:49:53 [utils.py:328] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 65536, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-Next-80B-A3B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 22:50:00 [model.py:543] Resolved architecture: Qwen3NextForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 22:50:00 [model.py:1604] Using max model len 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 22:50:03,224\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 22:50:03 [scheduler.py:218] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-19 22:50:03 [config.py:310] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
      "INFO 09-19 22:50:03 [config.py:321] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n",
      "INFO 09-19 22:50:04 [config.py:390] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 09-19 22:50:04 [config.py:411] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.\n",
      "WARNING 09-19 22:50:04 [__init__.py:2981] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 09-19 22:50:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:09 [core.py:648] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:09 [core.py:75] Initializing a V1 LLM engine (v0.10.2rc3.dev279+gddc904839) with config: model='Qwen/Qwen3-Next-80B-A3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-Next-80B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-Next-80B-A3B-Instruct, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m WARNING 09-19 22:50:09 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 124 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_b055f5c3'), local_subscribe_addr='ipc:///tmp/8763362b-759d-4970-9950-2995537e257f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 22:50:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 22:50:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 22:50:13 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 22:50:13 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 22:50:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0bded9d3'), local_subscribe_addr='ipc:///tmp/c4d5d365-b9f0-4244-8b5d-73ec425e58d3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 22:50:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_88726ccc'), local_subscribe_addr='ipc:///tmp/b66453e5-308d-45d7-b755-d5cdc899a0bd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W919 22:50:18.207418709 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 22:50:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_108f115a'), local_subscribe_addr='ipc:///tmp/3cd384fe-6173-4459-854e-ab109f5d0a55', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 22:50:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_134358bd'), local_subscribe_addr='ipc:///tmp/2eb9dc23-65ff-4e73-b30e-b4f296b6e3ce', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W919 22:50:19.748942916 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 22:50:19.809588300 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 22:50:19.814814678 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 22:50:19 [__init__.py:1439] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 22:50:19 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 22:50:19 [__init__.py:1439] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 22:50:19 [__init__.py:1439] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 22:50:19 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 22:50:19 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 22:50:19 [__init__.py:1439] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 22:50:19 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "WARNING 09-19 22:50:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 22:50:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 22:50:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 22:50:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 09-19 22:50:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_30833ba5'), local_subscribe_addr='ipc:///tmp/cb2d3aeb-77ec-40fa-98c3-de068ac01c69', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 22:50:21 [parallel_state.py:1206] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-19 22:50:21 [parallel_state.py:1206] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-19 22:50:21 [parallel_state.py:1206] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-19 22:50:21 [parallel_state.py:1206] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "WARNING 09-19 22:50:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 22:50:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 22:50:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 22:50:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2519] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2519] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2519] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2519] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2551] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2551] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2551] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:21 [gpu_model_runner.py:2551] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:21 [cuda.py:371] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:21 [cuda.py:371] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:21 [cuda.py:371] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:21 [cuda.py:371] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:21 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:21 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:21 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:21 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:15,  2.54it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:16,  2.37it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:16,  2.27it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:16,  2.28it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:02<00:15,  2.31it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:15,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:03<00:15,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:03<00:14,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:04<00:14,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:04<00:13,  2.31it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:04<00:09,  2.96it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:05<00:10,  2.72it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:05<00:10,  2.59it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:06<00:10,  2.41it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:06<00:11,  2.21it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:07<00:10,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:07<00:10,  2.16it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:08<00:10,  2.16it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:08<00:09,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:09<00:08,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:09<00:08,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:09<00:07,  2.32it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:10<00:07,  2.32it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:10<00:07,  2.27it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:11<00:06,  2.28it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:11<00:06,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:12<00:05,  2.27it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:12<00:05,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:13<00:04,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:13<00:04,  2.19it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:13<00:04,  2.19it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:14<00:03,  2.13it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:14<00:03,  2.11it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:15<00:02,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:15<00:02,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:16<00:01,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:16<00:01,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:17<00:00,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:17<00:00,  2.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:17<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:17<00:00,  2.28it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:40 [default_loader.py:268] Loading weights took 18.03 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:40 [default_loader.py:268] Loading weights took 18.37 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:40 [default_loader.py:268] Loading weights took 18.41 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:40 [gpu_model_runner.py:2570] Model loading took 37.2151 GiB and 18.791120 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:40 [gpu_model_runner.py:2570] Model loading took 37.2151 GiB and 18.894837 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:41 [gpu_model_runner.py:2570] Model loading took 37.2151 GiB and 18.992786 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:41 [default_loader.py:268] Loading weights took 18.86 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:41 [gpu_model_runner.py:2570] Model loading took 37.2151 GiB and 19.569748 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/ee6b3a6278/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:550] Dynamo bytecode transform time: 4.78 s\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/ee6b3a6278/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:550] Dynamo bytecode transform time: 4.83 s\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/ee6b3a6278/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:46 [backends.py:550] Dynamo bytecode transform time: 4.89 s\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/ee6b3a6278/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:550] Dynamo bytecode transform time: 4.93 s\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:49 [backends.py:215] Compiling a graph for dynamic shape takes 2.67 s\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:49 [backends.py:215] Compiling a graph for dynamic shape takes 2.67 s\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:49 [backends.py:215] Compiling a graph for dynamic shape takes 2.69 s\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:49 [backends.py:215] Compiling a graph for dynamic shape takes 2.69 s\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m WARNING 09-19 22:50:51 [fused_moe.py:728] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m WARNING 09-19 22:50:51 [fused_moe.py:728] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m WARNING 09-19 22:50:51 [fused_moe.py:728] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m WARNING 09-19 22:50:51 [fused_moe.py:728] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:51 [monitor.py:34] torch.compile takes 7.50 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:51 [monitor.py:34] torch.compile takes 7.46 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:51 [monitor.py:34] torch.compile takes 7.62 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:51 [monitor.py:34] torch.compile takes 7.58 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:50:52 [gpu_worker.py:299] Available KV cache memory: 31.81 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:50:52 [gpu_worker.py:299] Available KV cache memory: 31.81 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:50:52 [gpu_worker.py:299] Available KV cache memory: 31.81 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:50:52 [gpu_worker.py:299] Available KV cache memory: 31.81 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1043] GPU KV cache size: 694,960 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1047] Maximum concurrency for 65,536 tokens per request: 41.89x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1043] GPU KV cache size: 694,960 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1047] Maximum concurrency for 65,536 tokens per request: 41.89x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1043] GPU KV cache size: 694,960 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1047] Maximum concurrency for 65,536 tokens per request: 41.89x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1043] GPU KV cache size: 694,960 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:50:52 [kv_cache_utils.py:1047] Maximum concurrency for 65,536 tokens per request: 41.89x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:07<00:00,  9.07it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 67/67 [00:15<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:51:16 [gpu_model_runner.py:3370] Graph capturing finished in 24 secs, took 3.14 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:51:16 [gpu_worker.py:392] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.95, 75.23 GiB). Actual usage is 37.22 GiB for weight, 5.61 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.14 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30626413260` to fit into requested memory, or `--kv-cache-memory=34312793600` to fully utilize gpu memory. Current kv cache memory in use is 34160114380 bytes.\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:51:16 [gpu_model_runner.py:3370] Graph capturing finished in 24 secs, took 3.14 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:51:16 [gpu_worker.py:392] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.95, 75.23 GiB). Actual usage is 37.22 GiB for weight, 5.61 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.14 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30626413260` to fit into requested memory, or `--kv-cache-memory=34312793600` to fully utilize gpu memory. Current kv cache memory in use is 34160114380 bytes.\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:51:17 [gpu_model_runner.py:3370] Graph capturing finished in 24 secs, took 3.14 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:51:17 [gpu_worker.py:392] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.95, 75.23 GiB). Actual usage is 37.22 GiB for weight, 5.61 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.14 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30626413260` to fit into requested memory, or `--kv-cache-memory=34312793600` to fully utilize gpu memory. Current kv cache memory in use is 34160114380 bytes.\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:51:17 [gpu_model_runner.py:3370] Graph capturing finished in 24 secs, took 3.14 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:51:17 [gpu_worker.py:392] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.95, 75.23 GiB). Actual usage is 37.22 GiB for weight, 5.61 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.14 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30626413260` to fit into requested memory, or `--kv-cache-memory=34312793600` to fully utilize gpu memory. Current kv cache memory in use is 34160114380 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=47291)\u001b[0;0m INFO 09-19 22:51:17 [core.py:214] init engine (profile, create kv cache, warmup model) took 35.28 seconds\n",
      "INFO 09-19 22:51:17 [llm.py:318] Supported_tasks: ['generate']\n",
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    dtype=\"bfloat16\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=65536,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    tensor_parallel_size=4,\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate: single and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 145.94it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.11s/it, est. speed input: 0.27 toks/s, output: 9.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nemotron Super is a new class of **synthetic data generation** models developed by **NVIDIA**. It is designed to generate **high-quality synthetic data** across a wide range of **modalities**, including **text, images, video, audio, and 3D**. This makes it a powerful tool for creating realistic training data to enhance the performance of **AI models**, especially in scenarios where real-world data is scarce, expensive, or sensitive.\n",
      "\n",
      "Nemotron Super is part of NVIDIA's broader **Nemotron** family of models, which includes **Nemotron-4**, a family of **large language models (LLMs)** optimized for **inference**, **retrieval-augmented generation (RAG)**, **re-ranking**, and **embedding** tasks. While Nemotron-4 focuses on language understanding and generation, **Nemotron Super** extends this capability into **multimodal synthetic data generation**, enabling the creation of **synthetic data**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 3/3 [00:00<00:00, 2508.56it/s]\n",
      "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:105: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m   return fn(*contiguous_args, **contiguous_kwargs)\n",
      "Processed prompts: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, est. speed input: 6.67 toks/s, output: 222.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " <PRESIDIO_ANONYMIZED_PERSON> and I am a student in the Master's program in International Relations at the University of Vienna. I am writing to inquire about the possibility of conducting my Master's thesis research at the IAEA, under the supervision of Dr. Kornel Kleiner. As a student with a strong academic background in international relations and a deep interest in nuclear non-proliferation and disarmament, I believe that conducting my thesis research at the IAEA would provide me with unparalleled access to experts, data, and resources that would significantly enhance the quality of my work. I have attached my CV for your consideration.\n",
      "\n",
      "I am particularly interested in exploring the role of the IAEA in the implementation of the Joint Comprehensive Plan of Action (JCPOA) and its implications for the future of nuclear non-proliferation. I believe that my research could contribute to the IAEA's ongoing efforts to strengthen nuclear safeguards and promote international cooperation in the field of nuclear disarm\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris, a city renowned for its art, fashion, gastronomy, and culture. Paris is not only the political and administrative center of France but also a global hub for tourism and international relations. With iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral, Paris attracts millions of visitors each year. It is also home to numerous universities, research institutions, and the headquarters of many international organizations, making it a vital center for education, science, and diplomacy.\n",
      "\n",
      "Your description of Paris as the capital of France is accurate and well-articulated! To expand slightly on your points:\n",
      "\n",
      "Paris is indeed the political, cultural, and economic heart of France. As the seat of the French government, it hosts key institutions such as the Élysée Palace (official residence of the President), the National Assembly, and the Senate. Beyond its political significance, Paris plays a central role in global affairs — it is one of the world’s leading cities for diplomacy, hosting\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " Quantum computing is a type of computing that uses the principles of quantum physics to process information. Unlike classical computers, which use bits that are either 0 or 1, quantum computers use quantum bits, or \"qubits,\" which can be 0, 1, or both at the same time thanks to a property called superposition.\n",
      "\n",
      "Imagine a coin: In classical computing, it's like the coin is either heads (0) or tails (1). In quantum computing, it’s like the coin is spinning in the air — it’s both heads and tails at the same time until you look at it. This lets quantum computers explore many possibilities all at once, making them potentially much faster for certain types of problems.\n",
      "\n",
      "Another key idea is entanglement: when two qubits are entangled, changing one instantly affects the other, no matter how far apart they are. This allows quantum computers to link qubits together in powerful ways that classical computers can’t.\n",
      "\n",
      "Quantum computers aren’t better\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"What is Nemotron Super?\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Instruct model...\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:558] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP0 pid=47426)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:599] WorkerProc shutting down.\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:558] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP1 pid=47427)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:599] WorkerProc shutting down.\n",
      "\u001b[1;36m(Worker_TP2 pid=47428)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:558] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP3 pid=47429)\u001b[0;0m INFO 09-19 22:58:00 [multiproc_executor.py:558] Parent process exited, terminating worker\n",
      "Deleted llm object\n",
      "Cleared GPU cache\n",
      "Cleanup complete! Ready to load Thinking model.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup: Delete the model and free GPU memory\n",
    "# This is essential before moving to the next part (Thinking model)\n",
    "print(\"Cleaning up Instruct model...\")\n",
    "\n",
    "# Delete the model object\n",
    "if 'llm' in globals():\n",
    "    del llm\n",
    "    print(\"Deleted llm object\")\n",
    "    \n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared GPU cache\")\n",
    "\n",
    "print(\"Cleanup complete! Ready to load Thinking model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Qwen3-Next-Thinking with vLLM\n",
    "\n",
    "We will now launch a separate vLLM server for the `Qwen/Qwen3-Next-80B-A3B-Thinking` model. This variant is optimized for complex reasoning tasks.\n",
    "\n",
    "- Model card: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking\n",
    "- We will use a different port and served model name to avoid conflicts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Thinking Model Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started vLLM thinking server, PID=80693\n",
      "Waiting for server to initialize... (approx. 5 minutes)\n",
      "INFO 09-19 23:28:36 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:38 [api_server.py:1896] vLLM API server version 0.10.2\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:38 [utils.py:328] non-default args: {'model_tag': 'Qwen/Qwen3-Next-80B-A3B-Thinking', 'host': '0.0.0.0', 'port': 8001, 'model': 'Qwen/Qwen3-Next-80B-A3B-Thinking', 'served_model_name': ['qwen3-next-thinking'], 'tensor_parallel_size': 4}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:45 [__init__.py:742] Resolved architecture: Qwen3NextForCausalLM\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:45 [__init__.py:1815] Using max model len 262144\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:45 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:45 [config.py:310] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:45 [config.py:321] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:46 [config.py:390] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:28:46 [config.py:411] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.\n",
      "INFO 09-19 23:28:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:28:52 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:28:52 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen3-Next-80B-A3B-Thinking', speculative_config=None, tokenizer='Qwen/Qwen3-Next-80B-A3B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3-next-thinking, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m WARNING 09-19 23:28:52 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 124 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:28:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d9abc018'), local_subscribe_addr='ipc:///tmp/dd195e6b-ec99-49e4-908b-8eaaef7760f5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:28:55 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:28:55 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:28:55 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:28:56 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-19 23:29:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ea97c76d'), local_subscribe_addr='ipc:///tmp/347a402b-ba7f-4b7b-a7a5-3ec6414bd85c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:29:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_307c0985'), local_subscribe_addr='ipc:///tmp/458e1c2e-63ca-46b5-bf9a-68034662d41e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:29:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fcb1047b'), local_subscribe_addr='ipc:///tmp/66020340-e3f4-46da-9cc7-1ced5ae45c6c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-19 23:29:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_afa2cb11'), local_subscribe_addr='ipc:///tmp/301ad523-fa84-44a9-8b3f-3575be57d85a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W919 23:29:01.515485339 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:29:02.571233515 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:29:02.626167252 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W919 23:29:02.628079068 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 2 is connected to 33 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
      "\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 23:29:02 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:29:02 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:29:02 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:29:02 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:29:02 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:29:02 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 23:29:02 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 09-19 23:29:02 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "WARNING 09-19 23:29:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:29:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:29:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 23:29:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 09-19 23:29:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7643778a'), local_subscribe_addr='ipc:///tmp/99c5adbc-250f-4c03-ae3c-52cb7fcfb461', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 09-19 23:29:04 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-19 23:29:04 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-19 23:29:04 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-19 23:29:04 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "WARNING 09-19 23:29:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 23:29:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 23:29:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-19 23:29:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Thinking...\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Thinking...\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Thinking...\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-Next-80B-A3B-Thinking...\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:04 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:05 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:05 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:05 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:05 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:17,  2.35it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:17,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:17,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:16,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:02<00:16,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:16,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:03<00:15,  2.13it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:03<00:15,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:04<00:15,  2.11it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:04<00:13,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:05<00:10,  2.83it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:05<00:10,  2.61it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:06<00:10,  2.46it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:06<00:11,  2.32it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:07<00:11,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:07<00:11,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:07<00:10,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:08<00:10,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:08<00:09,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:09<00:09,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:09<00:08,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:10<00:08,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:10<00:07,  2.21it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:11<00:07,  2.15it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:11<00:06,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:12<00:06,  2.11it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:12<00:06,  2.15it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:13<00:05,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:13<00:05,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:14<00:04,  2.05it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:14<00:04,  2.03it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:15<00:03,  2.00it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:15<00:03,  1.97it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:16<00:02,  2.03it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:16<00:02,  2.09it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:17<00:01,  2.03it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:17<00:01,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:17<00:00,  2.09it/s]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:18<00:00,  2.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:18<00:00,  2.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:18<00:00,  2.17it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:24 [default_loader.py:268] Loading weights took 19.09 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:24 [default_loader.py:268] Loading weights took 18.94 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:24 [default_loader.py:268] Loading weights took 19.10 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:24 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 19.713313 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:24 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 19.567156 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:25 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 19.743312 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:25 [default_loader.py:268] Loading weights took 19.94 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:26 [gpu_model_runner.py:2392] Model loading took 37.2152 GiB and 20.796459 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:30 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/1023ecabfa/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:30 [backends.py:550] Dynamo bytecode transform time: 4.50 s\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:30 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/1023ecabfa/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:30 [backends.py:550] Dynamo bytecode transform time: 4.51 s\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:31 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:31 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/1023ecabfa/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:550] Dynamo bytecode transform time: 6.79 s\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:539] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/1023ecabfa/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:550] Dynamo bytecode transform time: 7.01 s\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:215] Compiling a graph for dynamic shape takes 2.79 s\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:33 [backends.py:215] Compiling a graph for dynamic shape takes 2.80 s\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:37 [backends.py:215] Compiling a graph for dynamic shape takes 4.38 s\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:38 [backends.py:215] Compiling a graph for dynamic shape takes 4.44 s\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m WARNING 09-19 23:29:39 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m WARNING 09-19 23:29:39 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m WARNING 09-19 23:29:39 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m WARNING 09-19 23:29:39 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/shadeform/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_PCIe.json']\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:39 [monitor.py:34] torch.compile takes 7.31 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:39 [monitor.py:34] torch.compile takes 7.28 s in total\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:39 [monitor.py:34] torch.compile takes 11.17 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:39 [monitor.py:34] torch.compile takes 11.45 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:29:40 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:29:40 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:29:41 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:29:41 [gpu_worker.py:298] Available KV cache memory: 27.89 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1028] GPU KV cache size: 609,008 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:29:41 [kv_cache_utils.py:1032] Maximum concurrency for 262,144 tokens per request: 9.26x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:10<00:00,  6.38it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 67/67 [00:29<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:30:22 [gpu_model_runner.py:3118] Graph capturing finished in 41 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=81204)\u001b[0;0m INFO 09-19 23:30:22 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:30:22 [gpu_model_runner.py:3118] Graph capturing finished in 41 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=81205)\u001b[0;0m INFO 09-19 23:30:22 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:30:22 [gpu_model_runner.py:3118] Graph capturing finished in 41 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=81207)\u001b[0;0m INFO 09-19 23:30:22 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:30:22 [gpu_model_runner.py:3118] Graph capturing finished in 41 secs, took 3.05 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=81206)\u001b[0;0m INFO 09-19 23:30:22 [gpu_worker.py:391] Free memory on device (78.66/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 37.22 GiB for weight, 5.58 GiB for peak activation, 0.59 GiB for non-torch memory, and 3.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=26511116185` to fit into requested memory, or `--kv-cache-memory=34448928256` to fully utilize gpu memory. Current kv cache memory in use is 29942056857 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=81038)\u001b[0;0m INFO 09-19 23:30:22 [core.py:218] init engine (profile, create kv cache, warmup model) took 56.85 seconds\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:23 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 8958\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:23 [async_llm.py:180] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [api_server.py:1692] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m WARNING 09-19 23:30:24 [__init__.py:1695] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [serving_responses.py:130] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [serving_chat.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [api_server.py:1971] Starting vLLM API server 0 on http://0.0.0.0:8001\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:36] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:30:24 [launcher.py:44] Route: /metrics, Methods: GET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     Started server process [80693]\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:46416 - \"GET /health HTTP/1.1\" 200 OK\n",
      "vLLM thinking server should be ready.\n"
     ]
    }
   ],
   "source": [
    "import subprocess, time\n",
    "\n",
    "# Launch Thinking server on a different port\n",
    "serve_cmd_thinking = [\n",
    "    \"vllm\", \"serve\", \"Qwen/Qwen3-Next-80B-A3B-Thinking\",\n",
    "    \"--tensor-parallel-size\", \"4\",\n",
    "    \"--served-model-name\", \"qwen3-next-thinking\",\n",
    "    \"--host\", \"0.0.0.0\", \"--port\", \"8001\"\n",
    "]\n",
    "\n",
    "thinking_process = subprocess.Popen(serve_cmd_thinking)\n",
    "print(f\"Started vLLM thinking server, PID={thinking_process.pid}\")\n",
    "\n",
    "# Wait for the server to be ready.\n",
    "print(\"Waiting for server to initialize... (approx. 5 minutes)\")\n",
    "time.sleep(300)\n",
    "print(\"vLLM thinking server should be ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference against Thinking server\n",
    "\n",
    "Use the OpenAI-compatible endpoint exposed by vLLM on port 8001.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m WARNING 09-19 23:33:33 [protocol.py:82] The following fields were present in the request but ignored: {'extra_body'}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:33:33 [chat_utils.py:538] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:60422 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m WARNING 09-19 23:33:51 [protocol.py:82] The following fields were present in the request but ignored: {'extra_body'}\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:34:04 [loggers.py:123] Engine 000: Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 30.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:46426 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Okay, the user is asking about the Hybrid Attention mechanism in Qwen3-Next. Hmm, I need to recall what I know about this. First, Qwen3-Next is a hypothetical or future version of Qwen, right? Because as of now, the latest version is Qwen3, but there's no official Qwen3-Next yet. Maybe the user is referring to a speculative or upcoming model.\n",
      "\n",
      "Wait, but the user might be confused. Let me check. The current Qwen series includes Qwen, Qwen1.5, Qwen2, Qwen2.5, and Qwen3. There's no Qwen3-Next officially announced. So perhaps the user is mistaken, or maybe they're referring to a specific feature in a different model.\n",
      "\n",
      "But the question is about Hybrid Attention in Qwen3-Next. If there's no such model yet, I should clarify that. However, maybe the user is thinking of a different model, or perhaps it's a hypothetical scenario. Alternatively, maybe they're referring to a concept in attention mechanisms that combines different types, like combining sparse and dense attention, or multi-head attention with some other technique.\n",
      "\n",
      "Wait, in some models, Hybrid Attention might refer to combining different attention mechanisms. For example, some models use a mix of local and global attention, or maybe combining self-attention with convolutional layers. But I need to be precise.\n",
      "\n",
      "But since Qwen3-Next isn't a real model, I should correct the user. Let me think. The correct approach is to state that as of now, there is no official Qwen3-Next model released by Tongyi Lab. The latest version is Qwen3. Therefore, any details about a \"Hybrid Attention mechanism in Qwen3-Next\" would be speculative or incorrect.\n",
      "\n",
      "Alternatively, maybe the user is referring to a feature in Qwen3 itself. Let me check Qwen3's documentation. From what I know, Qwen3 might have improvements in attention mechanisms, but I'm not sure about a specific \"Hybrid Attention\" term. Typically, attention mechanisms in transformers can be hybrid in the sense of combining different types, but Qwen's official docs might not use that exact term.\n",
      "\n",
      "Wait, perhaps the user is confusing it with another model. For example, some models like Longformer or BigBird use hybrid attention (local and global). But Qwen's architecture is based on standard transformer with possible optimizations.\n",
      "\n",
      "So, the correct response is to clarify that there's\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:55666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:34:14 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "THINKING_URL = \"http://localhost:8001/v1/chat/completions\"\n",
    "\n",
    "thinking_request = {\n",
    "    \"model\": \"qwen3-next-thinking\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in complex reasoning.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain the Hybrid Attention mechanism in Qwen3-Next in a few sentences.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.6,  # per model best practices\n",
    "    \"top_p\": 0.95,\n",
    "    # vLLM supports additional sampling params under 'extra_body' via OpenAI-compatible API\n",
    "    \"extra_body\": {\"top_k\": 20, \"min_p\": 0.0},\n",
    "    \"max_tokens\": 512,\n",
    "}\n",
    "\n",
    "resp = requests.post(THINKING_URL, json=thinking_request, timeout=600)\n",
    "resp.raise_for_status()\n",
    "print(resp.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference against Thinking Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending batch of requests to Thinking model concurrently...\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:49582 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:49598 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO:     127.0.0.1:49602 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Batch completed in 7.71 seconds.\n",
      "\n",
      "--- Response for Prompt 1: \"Write a Python function to find the nth Fibonacci number with caching, and explain the time complexity.\" ---\n",
      "Okay, the user wants a Python function to find the nth Fibonacci number using caching, and an explanation of the time complexity. Let me think about how to approach this.\n",
      "\n",
      "First, I remember that the Fibonacci sequence is defined as F(0) = 0, F(1) = 1, and F(n) = F(n-1) + F(n-2) for n > 1. The naive recursive approach is inefficient because it recalculates the same values multiple times, leading to exponential time complexity. Caching (memoization) can optimize this by storing previously computed results.\n",
      "\n",
      "So, the best way to implement caching in Python is probably using a dictionary to store the results. Alternatively, Python has the functools.lru_cache decorator which handles caching automatically. That might be the simplest and most efficient way.\n",
      "\n",
      "Let me check: using lru_cache from functools. The decorator would handle the memoization, so the function would just need to be recursive with the cache. But I should explain how it works.\n",
      "\n",
      "Wait, the user might not know about lru_cache, but it's a standard way. Alternatively, I could implement the cache manually with a dictionary. But using lru_cache is cleaner and efficient.\n",
      "\n",
      "Let me outline the steps:\n",
      "\n",
      "1. Import lru_cache from functools.\n",
      "2. Define a function fib(n) that's recursive.\n",
      "3. Use the @lru_cache decorator on the function.\n",
      "\n",
      "For example:\n",
      "\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache(maxsize=None)\n",
      "def fib(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fib(n-1) + fib(n-2)\n",
      "\n",
      "That's simple. Now, the time complexity. Without caching, the recursive approach is O(2^n) because each call branches into two. But with caching, each Fibonacci number is computed only once. So for n, the function will compute F(0) to F(n) once each. So the time complexity is O(n), because each of the n+1 values is computed once. The space complexity is also O(n) due to the recursion stack and the cache storing n+1 entries.\n",
      "\n",
      "Wait, the recursion depth might be a problem for large n, but for practical purposes, the caching makes it linear time.\n",
      "\n",
      "Alternatively, using iterative approach is also O(n) time and O(1) space, but the user specifically asked for caching, which implies memoization in a recursive approach.\n",
      "\n",
      "But the lru_cache approach is correct for memoization. Let me confirm the time\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Response for Prompt 2: \"Describe the difference between Gated DeltaNet and standard attention.\" ---\n",
      "Okay, the user is asking about the difference between Gated DeltaNet and standard attention. Let me start by recalling what I know about both.\n",
      "\n",
      "First, standard attention, like in Transformers, uses QKV matrices. The attention mechanism computes scores between queries and keys, then applies softmax to get weights, and finally sums the values with those weights. It's quadratic in sequence length, so it gets slow for long sequences. That's a big limitation.\n",
      "\n",
      "Now, Gated DeltaNet. I remember it's a newer model, probably from the DeltaNet paper. It's designed to handle long sequences more efficiently. The key points are the delta rule and gating. Instead of computing all pairwise interactions, DeltaNet uses a recurrence relation. It processes the sequence step by step, updating a state with each new token. The gating part probably controls how much new information is added versus retained from the previous state.\n",
      "\n",
      "Wait, the user might be familiar with Transformers but not with DeltaNet specifics. I should explain both clearly. Let me break down standard attention first: Q, K, V matrices, dot product, softmax, weighted sum. Then for DeltaNet, it's different. It uses a state that's updated incrementally. The delta rule here is about how the state changes based on the current input and the previous state. The gating mechanism likely uses a sigmoid or similar to modulate the update.\n",
      "\n",
      "I think DeltaNet replaces the softmax with a delta function, which is more like a recurrence. The \"gated\" part might involve a forget gate or update gate, similar to GRUs or LSTMs. So instead of attending to all previous tokens at once, it processes sequentially with a state that carries information forward. That would make it linear in time complexity, which is a big advantage for long sequences.\n",
      "\n",
      "Wait, but how exactly does the gating work in DeltaNet? Let me check my knowledge. From what I recall, DeltaNet uses a gated recurrence where each step updates the state based on the current token and the previous state. The gate controls the flow of information. So for each position, it computes a new state using a combination of the current input and the previous state, with the gate deciding how much to keep or update.\n",
      "\n",
      "Standard attention has O(n²) complexity because each token attends to all others. DeltaNet is O(n) because it processes sequentially without pairwise computations. That's a key difference. Also, DeltaNet might not use explicit QKV but rather a recurrence relation. The paper probably introduces a way to compute attention-like outputs\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Response for Prompt 3: \"Compose a short, rhyming poem about a Mixture-of-Experts model.\" ---\n",
      "Okay, the user wants a short rhyming poem about a Mixture-of-Experts (MoE) model. Interesting request! They're probably in tech or AI, maybe a developer or researcher who appreciates creative takes on technical concepts. Or perhaps a teacher wanting to explain MoE in a fun way? \n",
      "\n",
      "Hmm, need to keep it accurate but playful. MoE is all about specialized sub-models (experts) that handle different parts of a task, with a router deciding who speaks. Should highlight: specialization, routing, efficiency, collaboration. \n",
      "\n",
      "Rhyme scheme... maybe AABB? Simple and catchy. Short lines for punchiness. Avoid jargon but keep it technically sound. \"Gating network\" might be too much - \"router\" is friendlier. \"Experts\" is clear enough. \n",
      "\n",
      "First line: establish the core idea. \"Many minds, one task to share\" sets up collaboration. Then introduce the router: \"A wise router decides who's there\" - yes, \"wise\" makes it personable. \n",
      "\n",
      "Next: describe experts specializing. \"One knows math, one knows the sea\" - concrete examples. \"Each has skills for what they see\" keeps it simple. \n",
      "\n",
      "Then the magic: how they combine. \"No single voice speaks all the time\" shows dynamic routing. \"But when the question's in their line\" - good rhyme for \"line\" with \"design\". \n",
      "\n",
      "Final couplet: emphasize efficiency. \"Many minds, but one clear voice\" ties back to the start. \"A smarter choice\" - yes, implies better performance than single model. \n",
      "\n",
      "Check syllables: mostly 8-10 per line, flows when read aloud. \"Sea\" and \"see\" rhyme perfectly. \"Time\" and \"line\" work. \"Voice\" and \"choice\" - clean ending. \n",
      "\n",
      "User didn't specify tone, but \"whisper\" and \"wise\" give gentle, almost magical feel - good for making tech approachable. No need for complex metaphors; the poem should feel like a tiny story. \n",
      "\n",
      "...Did I oversimplify? Maybe, but for a short poem it's fine. Accuracy check: MoE does use gating, experts are specialized, and it's about efficiency. Yep. \n",
      "\n",
      "Final thought: Added \"shhh\" for whimsy - makes it feel like secret knowledge. User might smile at that.\n",
      "</think>\n",
      "\n",
      "Here's a short rhyming poem for you:\n",
      "\n",
      "> Many minds, one task to share,  \n",
      "> A wise\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:34:24 [loggers.py:123] Engine 000: Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 153.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
      "\u001b[1;36m(APIServer pid=80693)\u001b[0;0m INFO 09-19 23:34:34 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Use an async client for concurrent requests\n",
    "async_client = AsyncOpenAI(base_url=\"http://localhost:8001/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "batch_prompts_thinking = [\n",
    "    \"Write a Python function to find the nth Fibonacci number with caching, and explain the time complexity.\",\n",
    "    \"Describe the difference between Gated DeltaNet and standard attention.\",\n",
    "    \"Compose a short, rhyming poem about a Mixture-of-Experts model.\",\n",
    "]\n",
    "\n",
    "print(\"Sending batch of requests to Thinking model concurrently...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tasks = [\n",
    "    async_client.chat.completions.create(\n",
    "        model=\"qwen3-next-thinking\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": p}\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        extra_body={\"top_k\": 20, \"min_p\": 0.0},\n",
    "    ) for p in batch_prompts_thinking\n",
    "]\n",
    "responses = await asyncio.gather(*tasks)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Batch completed in {end_time - start_time:.2f} seconds.\\n\")\n",
    "\n",
    "for i, resp in enumerate(responses):\n",
    "    print(f\"--- Response for Prompt {i+1}: \\\"{batch_prompts_thinking[i]}\\\" ---\")\n",
    "    if resp.choices:\n",
    "        print(resp.choices[0].message.content.strip())\n",
    "    else:\n",
    "        print(\"Empty response.\")\n",
    "    print(\"-\" * (40 + len(batch_prompts_thinking[i])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Thinking Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown of the thinking server\n",
    "if 'thinking_process' in globals() and thinking_process.poll() is None:\n",
    "    thinking_process.kill()\n",
    "    print(f\"Killed thinking server PID {thinking_process.pid}\")\n",
    "else:\n",
    "    print(\"No running thinking server process found to terminate.\")\n",
    "\n",
    "subprocess.run(['pkill', '-f', 'vllm'], check=False)\n",
    "subprocess.run(['pkill', '-f', 'VLLM'], check=False)\n",
    "print(\" Killed all vLLM processes - GPU memory should be freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown\n"
    }
   },
   "source": [
    "## Resource Notes\n",
    "\n",
    "- **Hardware**: Qwen3-Next-80B-A3B is a large model. Multi-GPU tensor parallel (`--tensor-parallel-size`) is highly recommended for acceptable performance.\n",
    "- **Quantization**: For environments with limited resources, consider using quantized versions of the model (e.g., AWQ, GPTQ, INT4/INT8) if available. These can significantly reduce memory usage at the cost of some accuracy.\n",
    "- **Offloading**: For development or low-throughput scenarios, you can explore smaller models from the Qwen3 family or use model offloading to run the 80B parameter model on systems with less VRAM.\n",
    "- **Network**: Ensure you have sufficient network and disk bandwidth for the initial model download, as the weights are very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Congratulations! You successfully deployed the `Qwen3-Next` models using vLLM.\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "- Set up your environment with the necessary dependencies.\n",
    "- Launch and manage a vLLM server for the Instruct model.\n",
    "- Run inference via the OpenAI-compatible HTTP API.\n",
    "- Launch a second vLLM server for the Thinking model and run inference.\n",
    "- Run batch inference for higher throughput.\n",
    "\n",
    "You can adapt tensor parallelism, ports, and sampling parameters to your hardware and application needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
