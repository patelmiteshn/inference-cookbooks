{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Qwen3-next models with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to download and run `Qwen3-next` model using vLLM on NVIDIA GPUs for high-performance inference. vLLM is an open-source library that makes Large Language Model (LLM) inference and serving faster and more efficient by using an advanced memory management and continuous batching. It significantly increases model throughput, reduces GPU memory usage, and lowers infrastructure costs, making it a key tool for deploying LLMs at scale.\n",
    "\n",
    "`Qwen3-next`is a brand-new mdoel architecture that introduces several key improvements over its predesessor: a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. It is an 80-billion-parameter model that activates only 3 billion parameters during inference. Refer to the [model card] (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) for more details. The `Qwen3-next` has two variants:\n",
    "\n",
    "- `Qwen3-Next-80B-A3B-Instruct`\n",
    "- `Qwen3-Next-80B-A3B-Thinking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get start with this guide\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-30i1YjHsRWT109HL6eYxLUeHIwF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "To run the `Qwen3-Next-80B-A3B-Instruct` model, you will need an 4x A100 or 4xH200 NVIDIA \n",
    "\n",
    "### Software\n",
    "- CUDA Toolkit 12.8 or later\n",
    "- Python 3.12 or later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing vLLM\n",
    "\n",
    "To run `Qwen3-Next` models you will need to install the nightly build of vLLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM via pip\n",
    "```bash\n",
    "# uv venv\n",
    "# source .venv/bin/activate\n",
    "# uv pip install vllm --extra-index-url https://wheels.vllm.ai/nightly\n",
    "pip install vllm --extra-index-url https://wheels.vllm.ai/nightly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Model with Multi-GPU setup\n",
    "\n",
    "You can run the below command from command line or use subprocess to run it from this cell. \n",
    "`vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct \\`\n",
    "  `--tensor-parallel-size 4 \\`\n",
    "  `--served-model-name qwen3-next `\n",
    "\n",
    "We will running it from the cell and hence will be wrapping it with subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"vllm\", \"serve\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\",\n",
    "    \"--tensor-parallel-size\", \"4\",\n",
    "    \"--served-model-name\", \"qwen3-next\"\n",
    "]\n",
    "subprocess.Popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing using simple python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def inference(user_prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"qwen3-next\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "user_prompt = \" What is the capital of France and why do people travel go there? \"\n",
    "output = inference(user_prompt)\n",
    "result = output['choices'][0]['message']['content']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "Congratulations! You successfully deployed `Qwen3-Next` using vLLM.\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "- Set up your environment with the necessary dependencies.\n",
    "- Use vllm serve to deploy the model.\n",
    "- Run inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
