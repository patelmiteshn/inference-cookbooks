{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d3eff2-00bd-489e-bdf3-603d852c9bfc",
   "metadata": {},
   "source": [
    "# Running Mistral Large 3 675B Instruct with SGLang on NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bb76f-8b8e-427b-8b24-91f9544bb9e7",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide on how to run the  **Mistral Large 3 675B Instruct** model using SGLang. \n",
    "\n",
    "Mistral Large 3 is a state-of-the-art general-purpose multimodal granular Mixture-of-Experts model with 41B active parameters and 675B total parameters.\n",
    "\n",
    "This model is the instruct post-trained version, fine-tuned for instruction tasks, making it ideal for chat, agentic and instruction based use cases. Designed for reliability and long-context comprehension - it is engineered for production-grade assistants, retrieval-augmented systems, scientific workloads, and complex enterprise workflows.\n",
    "\n",
    "## Launch on NVIDIA Brev\n",
    "\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ITIC3pJeCMnsea4ihqV0uyU8K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad50a91-2239-4736-9c58-34243888e1d3",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "  - [Verifying your system](#Verifying-your-system)\n",
    "  - [Install SGLang and dependencies](#Install-SGLang-and-dependencies)\n",
    "- [Launch SGLang server](#Launch-SGLang-server)\n",
    "- [Client setup](#Client-setup)\n",
    "- [Testing some scenarios](#Testing-some-scenarios)\n",
    "  - [Instruction following](#Instruction-following)\n",
    "  - [Vision reasoning](#Vision-reasoning)\n",
    "  - [Function calling](#Function-calling)\n",
    "- [Cleaning up](#Cleaning-up)\n",
    "- [Conclusion and resources](#Conclusion-and-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388552c4-65e9-45b6-ab8c-c948295868bc",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ece8ec-2911-463f-91ae-d5564f251745",
   "metadata": {},
   "source": [
    "Mistral Large 3 is deployable on-premises at [FP8](https://huggingface.co/mistralai/Mistral-Large-3-675B-FP8-Instruct-2512) on a single node of B200 or H200 GPUs, with H200 having a reduced context window.\n",
    "\n",
    "This notebook is configured by default to run on a machine with 8 GPUs and sufficient VRAM to hold the 675B parameter model. If your hardware is different, be sure to adjust the `--tensor-parallel-size` (tensor parallelism) and other resource-related flags in the server launch command.\n",
    "\n",
    "### Verifying your system\n",
    "\n",
    "Let's verify your system is ready for **Mistral Large 3 675B Instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ea1d3c-79e9-480d-b1a5-d75859aaf592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "======================================================================\n",
      "OS: Linux 6.8.0-60-generic\n",
      "Python: 3.11.14\n",
      "======================================================================\n",
      "GPU DETAILS (nvidia-smi)\n",
      "======================================================================\n",
      "Number of GPUs detected: 8\n",
      "\n",
      "GPU[0]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[1]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[2]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[3]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[4]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[5]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[6]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[7]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "Total GPU Memory (All GPUs): 1432.49 GB\n",
      "\n",
      "======================================================================\n",
      "NVLINK STATUS\n",
      "======================================================================\n",
      "✅ NVLink detected & queryable\n",
      "\n",
      "GPU 0: NVIDIA B200 (UUID: GPU-14fb5c6f-4d76-786f-1bfc-a56ac72ad161)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 1: NVIDIA B200 (UUID: GPU-7dfdd14f-0a82-317b-aa2d-716b2f040bb9)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 2: NVIDIA B200 (UUID: GPU-11042832-327a-9324-7c6a-01af568deb3c)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 3: NVIDIA B200 (UUID: GPU-af32c297-6a70-4bbf-f714-bd18f8e31c74)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 4: NVIDIA B200 (UUID: GPU-34621968-3e34-a414-f63f-5238486a5b28)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 5: NVIDIA B200 (UUID: GPU-24195252-a5cd-faad-87e5-b9e90a46647e)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 6: NVIDIA B200 (UUID: GPU-7bb5df8e-e99c-810e-b943-53138b7f3599)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 7: NVIDIA B200 (UUID: GPU-1f783cea-5607-aac8-7894-513f3d7175a7)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION RECOMMENDATIONS\n",
      "======================================================================\n",
      "✅ Enough VRAM for large models — recommended EP/DP execution\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"=\"*70)\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "# Check if nvidia-smi exists\n",
    "if shutil.which(\"nvidia-smi\") is None:\n",
    "    print(\"❌ nvidia-smi not found — NVIDIA drivers are missing or not in PATH.\")\n",
    "    print(\"   Unable to detect GPU hardware.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU DETAILS (nvidia-smi)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Query GPU name + total memory\n",
    "    query_cmd = [\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=name,memory.total\",\n",
    "        \"--format=csv,noheader,nounits\"\n",
    "    ]\n",
    "\n",
    "    output = subprocess.check_output(query_cmd, text=True)\n",
    "    lines = [line.strip() for line in output.splitlines() if line.strip()]\n",
    "    total_memory_gb = 0.0\n",
    "\n",
    "    print(f\"Number of GPUs detected: {len(lines)}\")\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        name, mem_mib = [x.strip() for x in line.split(\",\")]\n",
    "        mem_gb = float(mem_mib) / 1024\n",
    "        total_memory_gb += mem_gb\n",
    "\n",
    "        print(f\"\\nGPU[{i}]:\")\n",
    "        print(f\"  Name: {name}\")\n",
    "        print(f\"  Total Memory: {mem_gb:.2f} GB\")\n",
    "\n",
    "        if \"H200\" in name:\n",
    "            print(\"  Status: ✅ Hopper architecture - Supported\")\n",
    "        elif \"B200\" in name or \"GB200\" in name:\n",
    "            print(\"  Status: ✅ Blackwell architecture - Optimal\")\n",
    "        else:\n",
    "            print(\"  Status: ⚠️ Unknown/older architecture — May have limitations\")\n",
    "\n",
    "    print(f\"\\nTotal GPU Memory (All GPUs): {total_memory_gb:.2f} GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to parse GPU info from nvidia-smi\")\n",
    "    print(e)\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NVLINK STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    nvlink = subprocess.check_output([\"nvidia-smi\", \"nvlink\", \"--status\"],\n",
    "                                     text=True, stderr=subprocess.STDOUT)\n",
    "    print(\"✅ NVLink detected & queryable\\n\")\n",
    "    print(nvlink.strip())\n",
    "except:\n",
    "    print(\"⚠️ NVLink not detected or unavailable\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURATION RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if total_memory_gb >= 1100:\n",
    "    print(\"✅ Enough VRAM for large models — recommended EP/DP execution\")\n",
    "elif total_memory_gb >= 900:\n",
    "    print(\"⚠️ Borderline for largest models — FP8 or TP recommended\")\n",
    "elif total_memory_gb > 0:\n",
    "    print(\"❌ VRAM too low for full-size models — use smaller/quantized checkpoints\")\n",
    "else:\n",
    "    print(\"❌ No GPUs detected — GPU is required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a875ac-8e60-481e-b612-e9212b0a3ac7",
   "metadata": {},
   "source": [
    "### Install SGLang and dependencies\n",
    "\n",
    "Install the latest SGLang release (0.3 or newer) so you get the TensorRT-LLM MLA and FlashInfer kernels that power the Mistral Large 3 MoE stack on B200/H200 systems. The commands below use `uv` to make sure compatible wheels are resolved, but you can substitute plain `pip install` if your environment already has the right CUDA toolchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace6b38-c5fd-4232-83b2-7c92917b25c6",
   "metadata": {},
   "source": [
    "Method 1. Via `pip install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb2876-a3d7-4f24-a791-771d3e010821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install uv\n",
    "%uv pip install \"sglang\" --prerelease=allow --quiet\n",
    "%pip install transformers accelerate huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e519fac-6c5d-4f64-9c08-f58b82e4f9af",
   "metadata": {},
   "source": [
    "Method 2. From source\n",
    "\n",
    "Clone the official repository (`https://github.com/sgl-project/sglang.git`) and check out the latest release branch before installing the Python package in editable mode so you pick up the CUDA/TensorRT plugins that ship with SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53a3fc-c951-41ca-a79f-d04471a93e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last release branch\n",
    "#%git clone -b <github-repo>\n",
    "#% cd <cloned-repo>\n",
    "\n",
    "# Install the python packages\n",
    "%pip install --upgrade pip\n",
    "%pip install accelerate\n",
    "%pip install -e \"python\"\n",
    "%pip install transformers accelerate huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3e7f4-6c2b-4ccd-ac51-e8a271af9853",
   "metadata": {},
   "source": [
    "## Launch SGLang server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a933547-9da2-4d4c-bc90-29a47b59eedd",
   "metadata": {},
   "source": [
    "We will launch an OpenAI-compatible server. It can be executed directly in this notebook or you can copy the `python3 -m sglang.launch_server` command with the parameters and execute it in a separate terminal window. Make sure to specify the parameters and adjust the values based on your setup.\n",
    "\n",
    "The startup will take long time as the checkpoints need to be loaded.\n",
    "\n",
    "Set the `ML3_MODEL`/`MODEL_NAME` environment variables (or edit the defaults below) to one of the official checkpoints published by Mistral so the server downloads the correct weights:\n",
    "\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512` (FP8 baseline for B200/H200)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4` (NVFP4 for H100/A100)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-BF16` (full BF16 precision)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-Eagle` (draft model for speculative decoding)\n",
    "\n",
    "You can also point `ML3_MODEL` to a local path if you have already mirrored the model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa688d-d691-48ad-98d7-a71d50b906f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:36:58] INFO server_args.py:1021: Use trtllm_mla as attention backend on sm100 for DeepseekV3ForCausalLM\n",
      "[2025-11-28 17:36:58] INFO server_args.py:1030: Enable FlashInfer AllReduce Fusion on sm100 for DeepseekV3ForCausalLM\n",
      "[2025-11-28 17:36:58] INFO server_args.py:1056: Use flashinfer_trtllm as MoE runner backend on sm100 for DeepseekV3ForCausalLM\n",
      "[2025-11-28 17:36:58] INFO model_config.py:890: Downcasting torch.float32 to torch.bfloat16.\n",
      "[2025-11-28 17:36:58] WARNING server_args.py:1357: TensorRT-LLM MLA only supports page_size of 32 or 64, changing page_size from None to 64.\n",
      "[2025-11-28 17:36:58] WARNING server_args.py:1507: FlashInfer TRTLLM MoE is enabled. --disable-shared-experts-fusion is automatically set.\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[2025-11-28 17:37:18 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-11-28 17:37:18 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/137 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 1/137 [00:00<00:44,  3.06it/s]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 2/137 [00:00<00:44,  3.00it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 3/137 [00:00<00:44,  3.01it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 4/137 [00:01<00:43,  3.06it/s]\n",
      "Loading safetensors checkpoint shards:   4% Completed | 5/137 [00:01<00:42,  3.09it/s]\n",
      "Loading safetensors checkpoint shards:   4% Completed | 6/137 [00:01<00:43,  2.98it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 7/137 [00:02<00:45,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 8/137 [00:02<00:45,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 9/137 [00:03<00:46,  2.75it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 10/137 [00:03<00:43,  2.89it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 11/137 [00:03<00:44,  2.83it/s]\n",
      "Loading safetensors checkpoint shards:   9% Completed | 12/137 [00:04<00:43,  2.87it/s]\n",
      "Loading safetensors checkpoint shards:   9% Completed | 13/137 [00:04<00:44,  2.79it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 14/137 [00:04<00:43,  2.82it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 15/137 [00:05<00:43,  2.77it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 16/137 [00:05<00:43,  2.75it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 17/137 [00:06<00:46,  2.58it/s]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 18/137 [00:06<00:56,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 19/137 [00:07<00:52,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 20/137 [00:07<00:50,  2.31it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 21/137 [00:07<00:48,  2.40it/s]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 22/137 [00:08<00:46,  2.49it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 23/137 [00:08<00:44,  2.57it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 24/137 [00:09<00:54,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 25/137 [00:09<00:50,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 26/137 [00:10<00:47,  2.33it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 27/137 [00:10<00:46,  2.38it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 28/137 [00:10<00:45,  2.37it/s]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 29/137 [00:11<00:43,  2.48it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 30/137 [00:11<00:42,  2.51it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 31/137 [00:11<00:40,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 32/137 [00:12<00:39,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 33/137 [00:12<00:39,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 34/137 [00:13<00:38,  2.67it/s]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 35/137 [00:13<00:36,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 36/137 [00:13<00:35,  2.85it/s]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 37/137 [00:13<00:31,  3.16it/s]\n",
      "Loading safetensors checkpoint shards:  28% Completed | 38/137 [00:14<00:32,  3.08it/s]\n",
      "Loading safetensors checkpoint shards:  28% Completed | 39/137 [00:14<00:32,  3.02it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 40/137 [00:15<00:32,  2.96it/s]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 41/137 [00:15<00:33,  2.89it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 42/137 [00:15<00:33,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 43/137 [00:16<00:32,  2.91it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 44/137 [00:16<00:31,  2.95it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 45/137 [00:16<00:31,  2.94it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 46/137 [00:17<00:31,  2.93it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 47/137 [00:17<00:30,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 48/137 [00:17<00:29,  2.99it/s]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 49/137 [00:18<00:30,  2.87it/s]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 50/137 [00:18<00:28,  3.00it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 51/137 [00:18<00:29,  2.95it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 52/137 [00:19<00:29,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 53/137 [00:19<00:28,  2.94it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 54/137 [00:19<00:28,  2.88it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 55/137 [00:20<00:28,  2.88it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 56/137 [00:20<00:28,  2.83it/s]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 57/137 [00:20<00:28,  2.80it/s]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 58/137 [00:21<00:27,  2.88it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 59/137 [00:21<00:26,  2.96it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 60/137 [00:21<00:27,  2.76it/s]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 61/137 [00:22<00:28,  2.68it/s]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 62/137 [00:22<00:28,  2.66it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 63/137 [00:23<00:27,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 64/137 [00:23<00:27,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 65/137 [00:23<00:27,  2.60it/s]\n",
      "Loading safetensors checkpoint shards:  48% Completed | 66/137 [00:24<00:27,  2.59it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 67/137 [00:25<00:37,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 68/137 [00:25<00:33,  2.04it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 69/137 [00:25<00:30,  2.20it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 70/137 [00:26<00:28,  2.36it/s]\n",
      "Loading safetensors checkpoint shards:  52% Completed | 71/137 [00:26<00:26,  2.54it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 72/137 [00:27<00:25,  2.51it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 73/137 [00:27<00:25,  2.47it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 74/137 [00:27<00:25,  2.49it/s]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 75/137 [00:28<00:24,  2.58it/s]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 76/137 [00:28<00:23,  2.59it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 77/137 [00:29<00:23,  2.53it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 78/137 [00:29<00:22,  2.66it/s]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 79/137 [00:29<00:21,  2.65it/s]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 80/137 [00:30<00:21,  2.66it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 81/137 [00:30<00:20,  2.77it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 82/137 [00:30<00:20,  2.73it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 83/137 [00:31<00:19,  2.80it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 84/137 [00:31<00:18,  2.87it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 85/137 [00:31<00:18,  2.85it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 86/137 [00:32<00:17,  2.91it/s]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 87/137 [00:32<00:17,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 88/137 [00:32<00:18,  2.59it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 89/137 [00:33<00:19,  2.52it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 90/137 [00:33<00:18,  2.56it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 91/137 [00:34<00:18,  2.53it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 92/137 [00:34<00:16,  2.66it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 93/137 [00:34<00:16,  2.71it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 94/137 [00:35<00:16,  2.64it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 95/137 [00:35<00:15,  2.73it/s]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 96/137 [00:35<00:14,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 97/137 [00:36<00:13,  2.94it/s]\n",
      "Loading safetensors checkpoint shards:  72% Completed | 98/137 [00:36<00:12,  3.03it/s]\n",
      "Loading safetensors checkpoint shards:  72% Completed | 99/137 [00:36<00:12,  3.00it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 100/137 [00:37<00:12,  3.00it/s]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 101/137 [00:37<00:11,  3.04it/s]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 102/137 [00:37<00:11,  2.92it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 103/137 [00:38<00:11,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 104/137 [00:38<00:11,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 105/137 [00:38<00:10,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 106/137 [00:39<00:10,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 107/137 [00:39<00:09,  3.03it/s]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 108/137 [00:39<00:10,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 109/137 [00:40<00:09,  2.99it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 110/137 [00:40<00:09,  2.98it/s]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 111/137 [00:40<00:08,  2.93it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 112/137 [00:41<00:08,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 113/137 [00:41<00:08,  2.79it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 114/137 [00:42<00:08,  2.87it/s]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 115/137 [00:42<00:07,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 116/137 [00:42<00:07,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 117/137 [00:43<00:07,  2.75it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 118/137 [00:43<00:07,  2.64it/s]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 119/137 [00:44<00:10,  1.73it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 120/137 [00:44<00:08,  1.92it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 121/137 [00:45<00:07,  2.13it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 122/137 [00:45<00:06,  2.26it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 123/137 [00:46<00:06,  2.33it/s]\n",
      "Loading safetensors checkpoint shards:  91% Completed | 124/137 [00:46<00:05,  2.47it/s]\n",
      "Loading safetensors checkpoint shards:  91% Completed | 125/137 [00:46<00:04,  2.60it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 126/137 [00:47<00:04,  2.61it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 127/137 [00:47<00:03,  2.68it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 128/137 [00:47<00:03,  2.71it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 129/137 [00:48<00:02,  2.79it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 130/137 [00:48<00:02,  2.80it/s]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 131/137 [00:48<00:02,  2.69it/s]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 132/137 [00:49<00:01,  2.73it/s]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 133/137 [00:49<00:01,  2.79it/s]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 134/137 [00:49<00:01,  2.92it/s]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 135/137 [00:50<00:00,  2.88it/s]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 136/137 [00:50<00:00,  2.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 137/137 [00:51<00:00,  2.82it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 137/137 [00:51<00:00,  2.68it/s]\n",
      "\n",
      "Capturing batches (bs=4 avail_mem=89.25 GB):   0%|          | 0/3 [00:00<?, ?it/s]/root/.pyenv/versions/3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "rank 3 allocated ipc_handles: [['0x7e92ca000000', '0x7e92bc000000', '0x7e92ae000000', '0x7e9362000000', '0x7e92a0000000', '0x7e9292000000', '0x7e9284000000', '0x7e9276000000'], ['0x7e944d000000', '0x7e944d200000', '0x7e944d400000', '0x7e944ce00000', '0x7e944d600000', '0x7e944d800000', '0x7e944da00000', '0x7e944dc00000'], ['0x7e9222000000', '0x7e91f8000000', '0x7e91ce000000', '0x7e924c000000', '0x7e91a4000000', '0x7e917a000000', '0x7e9150000000', '0x7e9126000000']]\n",
      "[2025-11-28 17:40:16.563] [info] lamportInitialize start: buffer: 0x7e924c000000, size: 352321536\n",
      "rank 5 allocated ipc_handles: [['0x76a43e000000', '0x76a430000000', '0x76a422000000', '0x76a414000000', '0x76a406000000', '0x76a4d6000000', '0x76a3f8000000', '0x76a3ea000000'], ['0x76a5ef000000', '0x76a5ef200000', '0x76a5ef400000', '0x76a5ef600000', '0x76a5ef800000', '0x76a5eee00000', '0x76a5efa00000', '0x76a5efc00000'], ['0x76a396000000', '0x76a36c000000', '0x76a342000000', '0x76a318000000', '0x76a2ee000000', '0x76a3c0000000', '0x76a2c4000000', '0x76a29a000000']]\n",
      "[2025-11-28 17:40:16.612] [info] lamportInitialize start: buffer: 0x76a3c0000000, size: 352321536\n",
      "rank 6 allocated ipc_handles: [['0x7b45da000000', '0x7b45cc000000', '0x7b45be000000', '0x7b45b0000000', '0x7b45a2000000', '0x7b4594000000', '0x7b4672000000', '0x7b4586000000'], ['0x7b4745000000', '0x7b4745200000', '0x7b4745400000', '0x7b4745600000', '0x7b4745800000', '0x7b4745a00000', '0x7b4744e00000', '0x7b4745c00000'], ['0x7b4532000000', '0x7b4508000000', '0x7b44de000000', '0x7b44b4000000', '0x7b448a000000', '0x7b4460000000', '0x7b455c000000', '0x7b4436000000']]\n",
      "[2025-11-28 17:40:16.660] [info] lamportInitialize start: buffer: 0x7b455c000000, size: 352321536\n",
      "rank 2 allocated ipc_handles: [['0x7d5f70000000', '0x7d5f62000000', '0x7d6008000000', '0x7d5f54000000', '0x7d5f46000000', '0x7d5f38000000', '0x7d5f2a000000', '0x7d5f1c000000'], ['0x7d60df000000', '0x7d60df200000', '0x7d60dee00000', '0x7d60df400000', '0x7d60df600000', '0x7d60df800000', '0x7d60dfa00000', '0x7d60dfc00000'], ['0x7d5ec8000000', '0x7d5e9e000000', '0x7d5ef2000000', '0x7d5e74000000', '0x7d5e4a000000', '0x7d5e20000000', '0x7d5df6000000', '0x7d5dcc000000']]\n",
      "[2025-11-28 17:40:16.713] [info] lamportInitialize start: buffer: 0x7d5ef2000000, size: 352321536\n",
      "rank 4 allocated ipc_handles: [['0x78a4d6000000', '0x78a4c8000000', '0x78a4ba000000', '0x78a4ac000000', '0x78a566000000', '0x78a49e000000', '0x78a490000000', '0x78a482000000'], ['0x78a625000000', '0x78a625200000', '0x78a625400000', '0x78a625600000', '0x78a624e00000', '0x78a625800000', '0x78a625a00000', '0x78a625c00000'], ['0x78a42e000000', '0x78a404000000', '0x78a3da000000', '0x78a3b0000000', '0x78a458000000', '0x78a386000000', '0x78a35c000000', '0x78a332000000']]\n",
      "[2025-11-28 17:40:16.763] [info] lamportInitialize start: buffer: 0x78a458000000, size: 352321536\n",
      "rank 7 allocated ipc_handles: [['0x7a107e000000', '0x7a1070000000', '0x7a1062000000', '0x7a1054000000', '0x7a1046000000', '0x7a1038000000', '0x7a102a000000', '0x7a1116000000'], ['0x7a1455a00000', '0x7a1455c00000', '0x7a1455e00000', '0x7a1457800000', '0x7a1457a00000', '0x7a1457c00000', '0x7a1457e00000', '0x7a1455800000'], ['0x7a0fd6000000', '0x7a0fac000000', '0x7a0f82000000', '0x7a0f58000000', '0x7a0f2e000000', '0x7a0f04000000', '0x7a0eda000000', '0x7a1000000000']]\n",
      "[2025-11-28 17:40:16.813] [info] lamportInitialize start: buffer: 0x7a1000000000, size: 352321536\n",
      "rank 1 allocated ipc_handles: [['0x7270e6000000', '0x72717e000000', '0x7270d8000000', '0x7270ca000000', '0x7270bc000000', '0x7270ae000000', '0x7270a0000000', '0x727092000000'], ['0x72725b000000', '0x72725ae00000', '0x72725b200000', '0x72725b400000', '0x72725b600000', '0x72725b800000', '0x72725ba00000', '0x72725bc00000'], ['0x72703e000000', '0x727068000000', '0x727014000000', '0x726fea000000', '0x726fc0000000', '0x726f96000000', '0x726f6c000000', '0x726f42000000']]\n",
      "[2025-11-28 17:40:16.864] [info] lamportInitialize start: buffer: 0x727068000000, size: 352321536\n",
      "rank 0 allocated ipc_handles: [['0x776dee000000', '0x776d56000000', '0x776d48000000', '0x776d3a000000', '0x776d2c000000', '0x776d1e000000', '0x776d10000000', '0x776d02000000'], ['0x777115800000', '0x777115a00000', '0x777115c00000', '0x777115e00000', '0x777117800000', '0x777117a00000', '0x777117c00000', '0x777117e00000'], ['0x776cd8000000', '0x776cae000000', '0x776c84000000', '0x776c5a000000', '0x776c30000000', '0x776c06000000', '0x776bdc000000', '0x776bb2000000']]\n",
      "[2025-11-28 17:40:16.914] [info] lamportInitialize start: buffer: 0x776cd8000000, size: 352321536\n",
      "/root/.pyenv/versions/3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 7 workspace[0] 0x7a107e000000\n",
      "Rank 7 workspace[1] 0x7a1070000000\n",
      "Rank 7 workspace[2] 0x7a1062000000\n",
      "Rank 7 workspace[3] 0x7a1054000000\n",
      "Rank 7 workspace[4] 0x7a1046000000\n",
      "Rank 7 workspace[5] 0x7a1038000000\n",
      "Rank 7 workspace[6] 0x7a102a000000\n",
      "Rank 7 workspace[7] 0x7a1116000000\n",
      "Rank 7 workspace[8] 0x7a1455a00000\n",
      "Rank 7 workspace[9] 0x7a1455c00000\n",
      "Rank 7 workspace[10] 0x7a1455e00000\n",
      "Rank 7 workspace[11] 0x7a1457800000\n",
      "Rank 7 workspace[12] 0x7a1457a00000\n",
      "Rank 7 workspace[13] 0x7a1457c00000\n",
      "Rank 7 workspace[14] 0x7a1457e00000\n",
      "Rank 7 workspace[15] 0x7a1455800000\n",
      "Rank 7 workspace[16] 0x7a0fd6000000\n",
      "Rank 7 workspace[17] 0x7a0fac000000\n",
      "Rank 7 workspace[18] 0x7a0f82000000\n",
      "Rank 7 workspace[19] 0x7a0f58000000\n",
      "Rank 7 workspace[20] 0x7a0f2e000000\n",
      "Rank 7 workspace[21] 0x7a0f04000000\n",
      "Rank 7 workspace[22] 0x7a0eda000000\n",
      "Rank 7 workspace[23] 0x7a1000000000\n",
      "Rank 7 workspace[24] 0x7a298f28c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 5 workspace[0] 0x76a43e000000\n",
      "Rank 5 workspace[1] 0x76a430000000\n",
      "Rank 5 workspace[2] 0x76a422000000\n",
      "Rank 5 workspace[3] 0x76a414000000\n",
      "Rank 5 workspace[4] 0x76a406000000\n",
      "Rank 5 workspace[5] 0x76a4d6000000\n",
      "Rank 5 workspace[6] 0x76a3f8000000\n",
      "Rank 5 workspace[7] 0x76a3ea000000\n",
      "Rank 5 workspace[8] 0x76a5ef000000\n",
      "Rank 5 workspace[9] 0x76a5ef200000\n",
      "Rank 5 workspace[10] 0x76a5ef400000\n",
      "Rank 5 workspace[11] 0x76a5ef600000\n",
      "Rank 5 workspace[12] 0x76a5ef800000\n",
      "Rank 5 workspace[13] 0x76a5eee00000\n",
      "Rank 5 workspace[14] 0x76a5efa00000\n",
      "Rank 5 workspace[15] 0x76a5efc00000\n",
      "Rank 5 workspace[16] 0x76a396000000\n",
      "Rank 5 workspace[17] 0x76a36c000000\n",
      "Rank 5 workspace[18] 0x76a342000000\n",
      "Rank 5 workspace[19] 0x76a318000000\n",
      "Rank 5 workspace[20] 0x76a2ee000000\n",
      "Rank 5 workspace[21] 0x76a3c0000000\n",
      "Rank 5 workspace[22] 0x76a2c4000000\n",
      "Rank 5 workspace[23] 0x76a29a000000\n",
      "Rank 5 workspace[24] 0x76bd8f28c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 2 workspace[0] 0x7d5f70000000\n",
      "Rank 2 workspace[1] 0x7d5f62000000\n",
      "Rank 2 workspace[2] 0x7d6008000000\n",
      "Rank 2 workspace[3] 0x7d5f54000000\n",
      "Rank 2 workspace[4] 0x7d5f46000000\n",
      "Rank 2 workspace[5] 0x7d5f38000000\n",
      "Rank 2 workspace[6] 0x7d5f2a000000\n",
      "Rank 2 workspace[7] 0x7d5f1c000000\n",
      "Rank 2 workspace[8] 0x7d60df000000\n",
      "Rank 2 workspace[9] 0x7d60df200000\n",
      "Rank 2 workspace[10] 0x7d60dee00000\n",
      "Rank 2 workspace[11] 0x7d60df400000\n",
      "Rank 2 workspace[12] 0x7d60df600000\n",
      "Rank 2 workspace[13] 0x7d60df800000\n",
      "Rank 2 workspace[14] 0x7d60dfa00000\n",
      "Rank 2 workspace[15] 0x7d60dfc00000\n",
      "Rank 2 workspace[16] 0x7d5ec8000000\n",
      "Rank 2 workspace[17] 0x7d5e9e000000\n",
      "Rank 2 workspace[18] 0x7d5ef2000000\n",
      "Rank 2 workspace[19] 0x7d5e74000000\n",
      "Rank 2 workspace[20] 0x7d5e4a000000\n",
      "Rank 2 workspace[21] 0x7d5e20000000\n",
      "Rank 2 workspace[22] 0x7d5df6000000\n",
      "Rank 2 workspace[23] 0x7d5dcc000000\n",
      "Rank 2 workspace[24] 0x7d788128c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 3 workspace[0] 0x7e92ca000000\n",
      "Rank 3 workspace[1] 0x7e92bc000000\n",
      "Rank 3 workspace[2] 0x7e92ae000000\n",
      "Rank 3 workspace[3] 0x7e9362000000\n",
      "Rank 3 workspace[4] 0x7e92a0000000\n",
      "Rank 3 workspace[5] 0x7e9292000000\n",
      "Rank 3 workspace[6] 0x7e9284000000\n",
      "Rank 3 workspace[7] 0x7e9276000000\n",
      "Rank 3 workspace[8] 0x7e944d000000\n",
      "Rank 3 workspace[9] 0x7e944d200000\n",
      "Rank 3 workspace[10] 0x7e944d400000\n",
      "Rank 3 workspace[11] 0x7e944ce00000\n",
      "Rank 3 workspace[12] 0x7e944d600000\n",
      "Rank 3 workspace[13] 0x7e944d800000\n",
      "Rank 3 workspace[14] 0x7e944da00000\n",
      "Rank 3 workspace[15] 0x7e944dc00000\n",
      "Rank 3 workspace[16] 0x7e9222000000\n",
      "Rank 3 workspace[17] 0x7e91f8000000\n",
      "Rank 3 workspace[18] 0x7e91ce000000\n",
      "Rank 3 workspace[19] 0x7e924c000000\n",
      "Rank 3 workspace[20] 0x7e91a4000000\n",
      "Rank 3 workspace[21] 0x7e917a000000\n",
      "Rank 3 workspace[22] 0x7e9150000000\n",
      "Rank 3 workspace[23] 0x7e9126000000\n",
      "Rank 3 workspace[24] 0x7eabf328c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 0 workspace[0] 0x776dee000000\n",
      "Rank 0 workspace[1] 0x776d56000000\n",
      "Rank 0 workspace[2] 0x776d48000000\n",
      "Rank 0 workspace[3] 0x776d3a000000\n",
      "Rank 0 workspace[4] 0x776d2c000000\n",
      "Rank 0 workspace[5] 0x776d1e000000\n",
      "Rank 0 workspace[6] 0x776d10000000\n",
      "Rank 0 workspace[7] 0x776d02000000\n",
      "Rank 0 workspace[8] 0x777115800000\n",
      "Rank 0 workspace[9] 0x777115a00000\n",
      "Rank 0 workspace[10] 0x777115c00000\n",
      "Rank 0 workspace[11] 0x777115e00000\n",
      "Rank 0 workspace[12] 0x777117800000\n",
      "Rank 0 workspace[13] 0x777117a00000\n",
      "Rank 0 workspace[14] 0x777117c00000\n",
      "Rank 0 workspace[15] 0x777117e00000\n",
      "Rank 0 workspace[16] 0x776cd8000000\n",
      "Rank 0 workspace[17] 0x776cae000000\n",
      "Rank 0 workspace[18] 0x776c84000000\n",
      "Rank 0 workspace[19] 0x776c5a000000\n",
      "Rank 0 workspace[20] 0x776c30000000\n",
      "Rank 0 workspace[21] 0x776c06000000\n",
      "Rank 0 workspace[22] 0x776bdc000000\n",
      "Rank 0 workspace[23] 0x776bb2000000\n",
      "Rank 0 workspace[24] 0x77863f28c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 6 workspace[0] 0x7b45da000000\n",
      "Rank 6 workspace[1] 0x7b45cc000000\n",
      "Rank 6 workspace[2] 0x7b45be000000\n",
      "Rank 6 workspace[3] 0x7b45b0000000\n",
      "Rank 6 workspace[4] 0x7b45a2000000\n",
      "Rank 6 workspace[5] 0x7b4594000000\n",
      "Rank 6 workspace[6] 0x7b4672000000\n",
      "Rank 6 workspace[7] 0x7b4586000000\n",
      "Rank 6 workspace[8] 0x7b4745000000\n",
      "Rank 6 workspace[9] 0x7b4745200000\n",
      "Rank 6 workspace[10] 0x7b4745400000\n",
      "Rank 6 workspace[11] 0x7b4745600000\n",
      "Rank 6 workspace[12] 0x7b4745800000\n",
      "Rank 6 workspace[13] 0x7b4745a00000\n",
      "Rank 6 workspace[14] 0x7b4744e00000\n",
      "Rank 6 workspace[15] 0x7b4745c00000\n",
      "Rank 6 workspace[16] 0x7b4532000000\n",
      "Rank 6 workspace[17] 0x7b4508000000\n",
      "Rank 6 workspace[18] 0x7b44de000000\n",
      "Rank 6 workspace[19] 0x7b44b4000000\n",
      "Rank 6 workspace[20] 0x7b448a000000\n",
      "Rank 6 workspace[21] 0x7b4460000000\n",
      "Rank 6 workspace[22] 0x7b455c000000\n",
      "Rank 6 workspace[23] 0x7b4436000000\n",
      "Rank 6 workspace[24] 0x7b5eef28c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 1 workspace[0] 0x7270e6000000\n",
      "Rank 1 workspace[1] 0x72717e000000\n",
      "Rank 1 workspace[2] 0x7270d8000000\n",
      "Rank 1 workspace[3] 0x7270ca000000\n",
      "Rank 1 workspace[4] 0x7270bc000000\n",
      "Rank 1 workspace[5] 0x7270ae000000\n",
      "Rank 1 workspace[6] 0x7270a0000000\n",
      "Rank 1 workspace[7] 0x727092000000\n",
      "Rank 1 workspace[8] 0x72725b000000\n",
      "Rank 1 workspace[9] 0x72725ae00000\n",
      "Rank 1 workspace[10] 0x72725b200000\n",
      "Rank 1 workspace[11] 0x72725b400000\n",
      "Rank 1 workspace[12] 0x72725b600000\n",
      "Rank 1 workspace[13] 0x72725b800000\n",
      "Rank 1 workspace[14] 0x72725ba00000\n",
      "Rank 1 workspace[15] 0x72725bc00000\n",
      "Rank 1 workspace[16] 0x72703e000000\n",
      "Rank 1 workspace[17] 0x727068000000\n",
      "Rank 1 workspace[18] 0x727014000000\n",
      "Rank 1 workspace[19] 0x726fea000000\n",
      "Rank 1 workspace[20] 0x726fc0000000\n",
      "Rank 1 workspace[21] 0x726f96000000\n",
      "Rank 1 workspace[22] 0x726f6c000000\n",
      "Rank 1 workspace[23] 0x726f42000000\n",
      "Rank 1 workspace[24] 0x7289f928c400\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 4 workspace[0] 0x78a4d6000000\n",
      "Rank 4 workspace[1] 0x78a4c8000000\n",
      "Rank 4 workspace[2] 0x78a4ba000000\n",
      "Rank 4 workspace[3] 0x78a4ac000000\n",
      "Rank 4 workspace[4] 0x78a566000000\n",
      "Rank 4 workspace[5] 0x78a49e000000\n",
      "Rank 4 workspace[6] 0x78a490000000\n",
      "Rank 4 workspace[7] 0x78a482000000\n",
      "Rank 4 workspace[8] 0x78a625000000\n",
      "Rank 4 workspace[9] 0x78a625200000\n",
      "Rank 4 workspace[10] 0x78a625400000\n",
      "Rank 4 workspace[11] 0x78a625600000\n",
      "Rank 4 workspace[12] 0x78a624e00000\n",
      "Rank 4 workspace[13] 0x78a625800000\n",
      "Rank 4 workspace[14] 0x78a625a00000\n",
      "Rank 4 workspace[15] 0x78a625c00000\n",
      "Rank 4 workspace[16] 0x78a42e000000\n",
      "Rank 4 workspace[17] 0x78a404000000\n",
      "Rank 4 workspace[18] 0x78a3da000000\n",
      "Rank 4 workspace[19] 0x78a3b0000000\n",
      "Rank 4 workspace[20] 0x78a458000000\n",
      "Rank 4 workspace[21] 0x78a386000000\n",
      "Rank 4 workspace[22] 0x78a35c000000\n",
      "Rank 4 workspace[23] 0x78a332000000\n",
      "Rank 4 workspace[24] 0x78bdcb28c400\n",
      "Capturing batches (bs=1 avail_mem=87.29 GB): 100%|██████████| 3/3 [00:07<00:00,  2.43s/it]\n",
      "[2025-11-28 17:40:21] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
      "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
      "                    \n",
      "SGLang server ready on port 39763 with served name 'mistralai/Mistral-Large-3-675B-Instruct-2512'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, terminate_process\n",
    "\n",
    "model_path = os.environ.get(\"ML3_MODEL\", \"mistralai/Mistral-Large-3-675B-Instruct-2512\")\n",
    "port = int(os.environ.get(\"SGLANG_PORT\", \"30000\"))\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"mistralai/Mistral-Large-3-675B-Instruct-2512\")\n",
    "\n",
    "server_cmd = f\"\"\"\n",
    "python3 -m sglang.launch_server \\\n",
    "    --model {model_path} \\\n",
    "    --host 0.0.0.0 --port {port} \\\n",
    "    --tensor-parallel-size 8 \\\n",
    "    --disable-radix-cache \\\n",
    "    --stream-interval 20 \\\n",
    "    --mem-fraction-static 0.95 \\\n",
    "    --max-running-requests 1024 \\\n",
    "    --cuda-graph-max-bs 16 \\\n",
    "    --served-model-name {MODEL_NAME} \\\n",
    "    --log-level warning \\\n",
    "    --chat-template mistral\n",
    "\"\"\"\n",
    "\n",
    "server_process, detected_port = launch_server_cmd(server_cmd)\n",
    "wait_for_server(f\"http://localhost:{detected_port}\")\n",
    "print(f\"SGLang server ready on port {detected_port} with served name '{MODEL_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a31b2-61a3-426c-9af9-fcb7a759c679",
   "metadata": {},
   "source": [
    "## Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a835dd-8659-4eea-9166-795e630de3a0",
   "metadata": {},
   "source": [
    "Once the server is running, connect using the OpenAI Python client. The endpoint exposes an OpenAI-compatible interface, so the standard OpenAI Python client will work without any additional adapters or client-side modifications. You simply point the client to your local server URL and provide the API key expected by vLLM (it can be any non-empty string unless you explicitly enforce authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38238534-2ef0-4620-b5b1-9c98dbb732e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SGLang server at http://localhost:39763/v1\n",
      "Using model: mistralai/Mistral-Large-3-675B-Instruct-2512\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url = f\"http://localhost:{detected_port}/v1\"\n",
    "api_key = \"dummy\"  # SGLang server doesn't require an API key by default\n",
    "\n",
    "# Connect to SGLang server\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "print(f\"Connected to SGLang server at {base_url}\")\n",
    "print(f\"Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963d689-223a-4efe-96c7-4dc4c3876e5a",
   "metadata": {},
   "source": [
    "## Testing some scenarios\n",
    "\n",
    "According to its authors, Mistral Large 3 is perfect for:\n",
    "\n",
    "* Long document understanding\n",
    "* Daily-driver AI assistants\n",
    "* Agentic and tool-use capabilities\n",
    "* Enterprise knowledge work\n",
    "* General coding assistant\n",
    "\n",
    "Let's test some of its features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddffb17-c48b-4b59-992b-901b038ae4c1",
   "metadata": {},
   "source": [
    "### Instruction following\n",
    "\n",
    "To guide the model toward a specific behavior or response style, you can supply a system prompt that defines rules, tone, formatting expectations, and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d49a02f-99ae-4c3c-a434-05bfe40b75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:41:28] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants me to invent a fun board game and explain the rules in under 120 words. Let me think about this.\n",
      "\n",
      "First, I need to come up with a concept for a board game that's fun and easy to understand. Maybe something with a mix of strategy and luck, and a bit of humor or creativity to make it engaging.\n",
      "\n",
      "How about a game called \"Roll & Rescue\"? It's a cooperative game where players work together to rescue animals from a sinking ship. The ship is the board, and it's divided into different rooms, each with a certain number of animals. Players roll dice to move around the ship and collect animals, but the ship is sinking, so they have to hurry!\n",
      "\n",
      "But I need to make sure the rules are concise and fit within 120 words. Let me draft the rules:\n",
      "\n",
      "1. Players: 2-4 players, ages 6 and up.\n",
      "2. Objective: Work together to rescue all the animals from the sinking ship before it's too late.\n",
      "3. Setup: Place the ship board in the center. Each player chooses a character and places it on the starting space. Place animal tokens in each room according to the room's capacity.\n",
      "4. Gameplay: On your turn, roll the dice to move your character. You can move up to the number rolled. If you land on a room with animals, you can rescue one animal by placing it on your character's space.\n",
      "5. Sinking Ship: After each turn, the ship sinks a little. Roll the sinking die and move the sinking marker forward. If the sinking marker reaches the end before all animals are rescued, the ship sinks and everyone loses.\n",
      "6. Winning: If all animals are rescued before the ship sinks, everyone wins!\n",
      "\n",
      "But this is a bit long. Let me try to condense it:\n",
      "\n",
      "\"Roll & Rescue\" is a cooperative board game for 2-4 players. Players work together to rescue animals from a sinking ship. Each player chooses a character and rolls dice to move around the ship board. Landing on a room with animals allows you to rescue one. After each turn, roll the sinking die to advance the sinking marker. If the marker reaches the end before all animals are rescued, the ship sinks and everyone loses. If all animals are rescued before the ship sinks, everyone wins!\n",
      "\n",
      "This is about 60 words. I can add more details to make it 120 words:\n",
      "\n",
      "\"Roll & Rescue\" is a cooperative board game for 2-4 players, ages 6 and up. The goal is to rescue all the animals from a sinking ship before it's too late. Each player chooses a character (e.g., Captain, Sailor, Doctor) and places it on the starting space of the ship board. On your turn, roll the movement die (1-6) to move your character around the ship. You can move up to the number rolled. The ship is divided into rooms, each with a certain number of animal tokens (e.g., 3 cats, 2 dogs, 1 parrot). If you land on a room with animals, you can rescue one animal by placing it on your character's space. However, after each turn, roll the sinking die (1-3) to advance the sinking marker. If the sinking marker reaches the end of the ship before all animals are rescued, the ship sinks and everyone loses. If all animals are rescued before the ship sinks, everyone wins! Use character abilities to help each other and strategize to save all the animals.\n",
      "\n",
      "This is around 120 words. Let me count:\n",
      "\n",
      "1. \"Roll & Rescue\" is a cooperative board game for 2-4 players, ages 6 and up.\n",
      "2. The goal is to rescue all the animals from a sinking ship before it's too late.\n",
      "3. Each player chooses a character (e.g., Captain, Sailor, Doctor) and places it on the starting space of the ship board.\n",
      "4. On your turn, roll the movement die (1-6) to move your character around the ship.\n",
      "5. You can move up to the number rolled.\n",
      "6. The ship is divided into rooms, each with a certain number of animal tokens (e.g., 3 cats, 2 dogs, 1 parrot).\n",
      "7. If you land on a room with animals, you can rescue one animal by placing it on your character's space.\n",
      "8. However, after each turn, roll the sinking die (1-3) to advance the sinking marker.\n",
      "9. If the sinking marker reaches the end of the ship before all animals are rescued, the ship sinks and everyone loses.\n",
      "10. If all animals are rescued before the ship sinks, everyone wins!\n",
      "11. Use character abilities to help each other and strategize to save all the animals.\n",
      "\n",
      "That's about 110 words. I can add a bit more to reach 120 words.\n",
      "\n",
      "Let me try to make it more concise and fun:\n",
      "\n",
      "\"Roll & Rescue\" is a cooperative board game for 2-4 players. The goal is to rescue all the animals from a sinking ship before it goes under! Each player picks a character (Captain, Sailor, Doctor, or Engineer) and rolls a die to move around the ship board. Land on a room with animals to rescue one. But watch out! After each turn, roll the sinking die to move the sinking marker. If it reaches the end before all animals are saved, the ship sinks and everyone loses. Use your character's special ability to help the team. The Captain can move extra spaces, the Sailor can rescue more animals, the Doctor can heal characters, and the Engineer can slow the sinking. Work together to save all the animals and win!\n",
      "\n",
      "This is about 100 words. I can add a few more details to make it 120 words.Welcome to \"Roll & Rescue\"!\n",
      "\n",
      "**Objective:** Work together to rescue all the animals from a sinking ship before it's too late.\n",
      "\n",
      "**Players:** 2-4 players, ages 6 and up.\n",
      "\n",
      "**Setup:** Place the ship board in the center. Each player chooses a character (Captain, Sailor, Doctor, or Engineer) and places it on the starting space. Place animal tokens in each room of the ship.\n",
      "\n",
      "**Gameplay:** On your turn, roll the movement die (1-6) to move your character around the ship. Land on a room with animals to rescue one. Use your character's special ability to help the team:\n",
      "- Captain: Move 2 extra spaces once per turn.\n",
      "- Sailor: Rescue 2 animals instead of 1.\n",
      "- Doctor: Heal a character to allow them to move again.\n",
      "- Engineer: Slow the sinking by moving the sinking marker back 1 space.\n",
      "\n",
      "**Sinking Ship:** After each turn, roll the sinking die (1-3) to advance the sinking marker. If the marker reaches the end before all animals are rescued, the ship sinks and everyone loses.\n",
      "\n",
      "**Winning:** If all animals are rescued before the ship sinks, everyone wins!\n",
      "\n",
      "Have fun and good luck with your rescue mission! 🚢🐶🐱🐦🐷Here's a fun board game idea:\n",
      "\n",
      "**Game Name:** Roll & Rescue\n",
      "\n",
      "**Objective:** Work together to rescue all the animals from a sinking ship before it goes under!\n",
      "\n",
      "**Players:** 2-4 players, ages 6 and up.\n",
      "\n",
      "**Setup:** Place the ship board in the center. Each player chooses a character (Captain, Sailor, Doctor, or Engineer) and places it on the starting space. Place animal tokens in each room of the ship.\n",
      "\n",
      "**Gameplay:** On your turn, roll the movement die (1-6) to move around the ship. Land on a room with animals to rescue one. Use your character's special ability to help the team:\n",
      "- Captain: Move 2 extra spaces once per turn.\n",
      "- Sailor: Rescue 2 animals instead of 1.\n",
      "- Doctor: Heal a character to allow them to move again.\n",
      "- Engineer: Slow the sinking by moving the sinking marker back 1 space.\n",
      "\n",
      "**Sinking Ship:** After each turn, roll the sinking die (1-3) to advance the sinking marker. If it reaches the end before all animals are rescued, everyone loses.\n",
      "\n",
      "**Winning:** Rescue all animals before the ship sinks to win!\n",
      "\n",
      "Have fun! 🚢🐾\n"
     ]
    }
   ],
   "source": [
    "def load_system_prompt(filename: str) -> str:\n",
    "    with open(filename, \"r\") as file:\n",
    "        system_prompt = file.read()\n",
    "    return system_prompt\n",
    "\n",
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "SYSTEM_PROMPT = load_system_prompt(\"SYSTEM_PROMPT.txt\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"Invent a fun board game and explain the rules in under 120 words.\"}\n",
    "    ],\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384a4d4-7d8a-44f9-85da-94cea2a094e0",
   "metadata": {},
   "source": [
    "### Vision reasoning\n",
    "\n",
    "Vision reasoning refers to the model’s ability to interpret visual inputs and apply logical inference on top of what it sees — not just recognizing objects, but understanding relationships, spatial context, cause-and-effect, and intent within an image. Instead of simply labelling elements, the model can describe scenes, infer actions, identify patterns, and answer questions that require comprehension rather than pattern-matching alone. This enables more advanced use cases such as analyzing diagrams, extracting information from charts, interpreting UI layouts, or evaluating photos for consistency and meaning. In short, vision reasoning bridges visual perception and conceptual understanding, allowing the model to think about images rather than merely see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1e6f43-2cf8-4088-a499-a4e2bff6afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:42:18] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's see. The image shows a cluttered workshop or studio, filled with various tools, materials, and sculptures. It looks like an artist's or craftsman's space, with a lot of unfinished projects and a generally chaotic atmosphere.\n",
      "\n",
      "First, I need to identify the key elements in the scene:\n",
      "1. The workshop is filled with sculptures, some of which are heads or busts.\n",
      "2. There are tools and materials scattered around, indicating ongoing work.\n",
      "3. The lighting is warm and somewhat dim, creating a cozy but mysterious ambiance.\n",
      "4. There's a ladder and some wooden beams, suggesting that the space might be in a loft or an old building.\n",
      "\n",
      "Now, thinking about an unusual scenario that could happen next. The typical things that might happen in a workshop are the artist continuing to work, someone cleaning up, or perhaps an accident with the tools. But the user wants something creative and unusual.\n",
      "\n",
      "Given the presence of multiple sculptures of heads, perhaps something related to those could be the basis of an unusual event. Maybe the heads are not just sculptures but are actually alive in some way. They could start talking or moving, revealing that they are magical or perhaps the result of some experiment gone awry.\n",
      "\n",
      "Alternatively, considering the clutter and the old building, perhaps the workshop is a portal to another dimension or time period. The artist might accidentally knock over a specific item that triggers this portal, leading to unexpected visitors or events.\n",
      "\n",
      "Another idea could be that the workshop is a front for something else. Maybe it's a secret laboratory where the artist is actually working on something much more unusual, like creating a new form of life or a time machine.\n",
      "\n",
      "Given the warm lighting and the cozy atmosphere, perhaps the workshop is sentient itself. It could react to the clutter and the artist's presence in a unique way, maybe by rearranging itself or revealing hidden compartments.\n",
      "\n",
      "Let's go with the idea that the heads are alive. Here's a possible scenario:\n",
      "\n",
      "Suddenly, one of the heads on the shelf begins to speak. The artist, initially startled, realizes that the heads are not just sculptures but are actually enchanted. They start to communicate with each other and the artist, revealing that they were once people who were transformed into these sculptures by a mysterious force. The artist might have unknowingly used a magical material or tool in their work, leading to this transformation.\n",
      "\n",
      "As more heads start to talk, the workshop becomes a hub of activity, with the artist trying to figure out how to reverse the spell or perhaps communicate with the heads to understand what happened. The scenario could take a turn where the heads start to give advice or warnings about something, adding a layer of mystery and intrigue to the scene.\n",
      "\n",
      "Alternatively, the heads could start to argue or fight among themselves, causing chaos in the workshop. The artist might need to intervene to prevent damage to their workspace and the sculptures.\n",
      "\n",
      "But to make it even more unusual, perhaps the heads are not just alive but are also from different time periods. They could start to interact with each other, discussing historical events or future predictions, and the artist might find themselves in the middle of a time-traveling debate.\n",
      "\n",
      "Let's try to craft a response based on this idea.In this cluttered workshop, an unusual scenario could unfold as follows:\n",
      "\n",
      "Suddenly, one of the busts on the shelf begins to speak. The artist, initially taken aback, soon realizes that all the heads in the workshop are not mere sculptures but are actually enchanted. They start to move and communicate with each other, revealing that they were once people from different time periods who were transformed into these busts by a mysterious force.\n",
      "\n",
      "As the heads begin to converse, the workshop becomes a lively scene of historical and futuristic debate. The artist might find themselves in the middle of a discussion between a pharaoh from ancient Egypt, a medieval knight, and a futuristic scientist. Each head shares their experiences and knowledge from their respective eras, and the artist tries to understand how this transformation occurred and if there's a way to reverse it.\n",
      "\n",
      "However, the situation takes an unexpected turn when the heads start to argue about the best way to proceed. The pharaoh insists on being returned to his time to prevent a historical catastrophe, the knight wants to be restored to his body to continue his quest, and the scientist is eager to study the phenomenon further.\n",
      "\n",
      "Amidst the chaos, the artist notices that one of the tools they've been using, perhaps a chisel or a brush, is glowing faintly. This tool might be the key to reversing the spell or communicating with the heads. As the artist picks it up, the heads fall silent, and the workshop seems to hold its breath, waiting for what happens next.\n",
      "\n",
      "But instead of reversing the spell, the artist accidentally activates a hidden function of the tool, causing the heads to merge into a single, multi-faceted entity. This new being possesses the combined knowledge and experiences of all the heads, creating a powerful and unique presence in the workshop.\n",
      "\n",
      "The artist now faces a dilemma: how to handle this new entity and whether to try to separate the heads again or embrace this new form of creation.\n",
      "\n",
      "```markdown\n",
      "In this scene, the artist might suddenly notice that one of the busts on the shelf has started to speak. As they investigate, they realize that all the heads in the workshop are enchanted and were once living people from different time periods. The heads begin to move and communicate with each other, sharing stories and knowledge from their respective eras. The artist, intrigued and somewhat alarmed, tries to understand the cause of this transformation.\n",
      "\n",
      "Amidst the growing chaos, the artist spots a glowing tool, perhaps a chisel or a brush, which seems to be the key to this mystery. As they pick it up, the heads fall silent, and the workshop appears to be holding its breath. Instead of reversing the spell, the artist accidentally merges the heads into a single, multi-faceted entity with the combined knowledge and experiences of all the busts.\n",
      "\n",
      "The artist now faces a unique challenge: how to interact with this new being and whether to attempt to separate the heads or embrace this extraordinary creation.\n",
      "```\n",
      "\n",
      "This scenario adds a magical and historical twist to the workshop setting, making it unusual and creative.\n"
     ]
    }
   ],
   "source": [
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "SYSTEM_PROMPT = load_system_prompt(\"SYSTEM_PROMPT.txt\")\n",
    "# Feel free to replace with any image of your choice!\n",
    "IMAGE_URL = \"https://blogs.nvidia.com/wp-content/uploads/2020/11/marbles-at-night.jpg\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Describe what can happen next in this scene. Be creative and think of an unusual scenario\",\n",
    "            },\n",
    "            {\"type\": \"image_url\", \n",
    "             \"image_url\": {\"url\": IMAGE_URL}\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019c348-4653-4ba3-b60a-0a0ffef3f22e",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Function calling allows the model to generate structured outputs that trigger real functions in your application, turning natural-language queries into executable actions. Instead of returning plain text, the model produces arguments in a predefined schema, enabling you to safely map intent to code paths — such as querying a database, retrieving documents, sending notifications, or performing calculations. This turns the model into a reasoning layer that interprets user requests, decides when a tool should be invoked, and returns well-formed call signatures that programs can act on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5074f6-ddc0-4fbd-970a-c481efbfeafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:42:44] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to depict the interior of a modern, industrial-style kitchen, likely in a restaurant. The signage on the wall in the background includes Japanese characters (「いらっしゃいませ」, which translates to \"Welcome\" in English). This suggests that the country shown in the image is Japan. Additionally, the overall design and layout of the kitchen are consistent with those commonly found in Japanese eateries. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:42:45] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "IMAGE_URL = \"https://cdna.artstation.com/p/assets/images/images/050/827/584/large/rafael-chies-14.jpg?1655798602\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_population\",\n",
    "            \"description\": \"Get the up-to-date population of a given country.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"country\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The country to find the population of.\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unit for the population.\",\n",
    "                        \"enum\": [\"millions\", \"thousands\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"country\", \"unit\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rewrite\",\n",
    "            \"description\": \"Rewrite a given text for improved clarity\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The input text to rewrite\",\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Can you tell me which country is shown in this image?\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": IMAGE_URL,\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "assistant_message = resp.choices[0].message.content\n",
    "print(assistant_message, \"\\n\")\n",
    "\n",
    "messages.extend([\n",
    "    {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "    {\"role\": \"user\", \"content\": \"What is the population of that country in millions?\"},\n",
    "])\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819a1b2-a001-4be3-a34e-95805e385284",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "If you launched the server from this notebook, run the following cell to terminate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dbcb2c-6b11-4e1f-880d-213b917bb127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No running server process found to terminate.\n"
     ]
    }
   ],
   "source": [
    "if 'server_process' in globals() and server_process.poll() is None:\n",
    "    server_process.kill()\n",
    "    print(f\"Killed instruct server PID {server_process.pid}\")\n",
    "else:\n",
    "    print(\"No running server process found to terminate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da0f7e-8b2f-469b-b569-acc4659cf8ca",
   "metadata": {},
   "source": [
    "## Conclusion and resources\n",
    "\n",
    "Congratulations! You successfully deployed the **Mistral Large 3 675B Instruct** model using SGLang.\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "\n",
    "- Set up your environment and install SGLang.\n",
    "- Launch and manage an OpenAI-compatible server to run model.\n",
    "- Perform instruction following, vision reasoning, and function calling tasks using the OpenAI client.\n",
    "\n",
    "You can adapt tensor parallelism, ports, and sampling parameters to your hardware and application needs.\n",
    "\n",
    "Refer to the following resources if you want to learn more\n",
    "\n",
    "### Documentation\n",
    "- 📚 [Mistral Large 3 Model Card](https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512)\n",
    "- 🏗️ [NVIDIA SGLang Guide](https://docs.nvidia.com/deeplearning/frameworks/sglang-release-notes/overview.html)\n",
    "\n",
    "### Code and kernels\n",
    "- 💾 [Flashinfer kernel library](https://github.com/flashinfer-ai/flashinfer)\n",
    "- ⚡  [FlashMLA Implementation](https://github.com/deepseek-ai/FlashMLA)\n",
    "- 🧪 [Mistral Examples](https://github.com/mistralai)\n",
    "\n",
    "### Community\n",
    "- 📧 [NVIDIA Developer Forums](https://forums.developer.nvidia.com/)\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "**Authors:** [Katja Sirazitdinova](https://github.com/katjasrz), [Jay Rodge](https://github.com/jayrodge), [Mitesh Patel](https://github.com/patelmiteshn), Developer Advocates @ NVIDIA\n",
    "\n",
    "Special thanks to the Mistral and SGLang teams for their incredible work on these technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fade82-fb97-4ee3-b3f8-e3860260b921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
