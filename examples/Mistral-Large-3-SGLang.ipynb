{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d3eff2-00bd-489e-bdf3-603d852c9bfc",
   "metadata": {},
   "source": [
    "# Running Mistral Large 3 675B Instruct with SGLang on NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bb76f-8b8e-427b-8b24-91f9544bb9e7",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide on how to run the  **Mistral Large 3 675B Instruct** model using SGLang. \n",
    "\n",
    "Mistral Large 3 is a state-of-the-art general-purpose multimodal granular Mixture-of-Experts model with 41B active parameters and 675B total parameters.\n",
    "\n",
    "This model is the instruct post-trained version, fine-tuned for instruction tasks, making it ideal for chat, agentic and instruction based use cases. Designed for reliability and long-context comprehension - it is engineered for production-grade assistants, retrieval-augmented systems, scientific workloads, and complex enterprise workflows.\n",
    "\n",
    "## Launch on NVIDIA Brev\n",
    "\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ITIC3pJeCMnsea4ihqV0uyU8K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad50a91-2239-4736-9c58-34243888e1d3",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "  - [Verifying your system](#Verifying-your-system)\n",
    "  - [Install SGLang and dependencies](#Install-SGLang-and-dependencies)\n",
    "- [Launch SGLang server](#Launch-SGLang-server)\n",
    "- [Client setup](#Client-setup)\n",
    "- [Testing some scenarios](#Testing-some-scenarios)\n",
    "  - [Instruction following](#Instruction-following)\n",
    "  - [Vision reasoning](#Vision-reasoning)\n",
    "  - [Function calling](#Function-calling)\n",
    "- [Cleaning up](#Cleaning-up)\n",
    "- [Conclusion and resources](#Conclusion-and-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388552c4-65e9-45b6-ab8c-c948295868bc",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ece8ec-2911-463f-91ae-d5564f251745",
   "metadata": {},
   "source": [
    "Mistral Large 3 is deployable on-premises at [FP8](https://huggingface.co/mistralai/Mistral-Large-3-675B-FP8-Instruct-2512) on a single node of B200 or H200 GPUs, with H200 having a reduced context window.\n",
    "\n",
    "This notebook is configured by default to run on a machine with 8 GPUs and sufficient VRAM to hold the 675B parameter model. If your hardware is different, be sure to adjust the `--tensor-parallel-size` (tensor parallelism) and other resource-related flags in the server launch command.\n",
    "\n",
    "### Verifying your system\n",
    "\n",
    "Let's verify your system is ready for **Mistral Large 3 675B Instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ea1d3c-79e9-480d-b1a5-d75859aaf592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "======================================================================\n",
      "OS: Linux 6.8.0-60-generic\n",
      "Python: 3.11.14\n",
      "======================================================================\n",
      "GPU DETAILS (nvidia-smi)\n",
      "======================================================================\n",
      "Number of GPUs detected: 8\n",
      "\n",
      "GPU[0]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[1]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[2]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[3]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[4]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[5]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[6]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "GPU[7]:\n",
      "  Name: NVIDIA B200\n",
      "  Total Memory: 179.06 GB\n",
      "  Status: ✅ Blackwell architecture - Optimal\n",
      "\n",
      "Total GPU Memory (All GPUs): 1432.49 GB\n",
      "\n",
      "======================================================================\n",
      "NVLINK STATUS\n",
      "======================================================================\n",
      "✅ NVLink detected & queryable\n",
      "\n",
      "GPU 0: NVIDIA B200 (UUID: GPU-14fb5c6f-4d76-786f-1bfc-a56ac72ad161)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 1: NVIDIA B200 (UUID: GPU-7dfdd14f-0a82-317b-aa2d-716b2f040bb9)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 2: NVIDIA B200 (UUID: GPU-11042832-327a-9324-7c6a-01af568deb3c)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 3: NVIDIA B200 (UUID: GPU-af32c297-6a70-4bbf-f714-bd18f8e31c74)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 4: NVIDIA B200 (UUID: GPU-34621968-3e34-a414-f63f-5238486a5b28)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 5: NVIDIA B200 (UUID: GPU-24195252-a5cd-faad-87e5-b9e90a46647e)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 6: NVIDIA B200 (UUID: GPU-7bb5df8e-e99c-810e-b943-53138b7f3599)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "GPU 7: NVIDIA B200 (UUID: GPU-1f783cea-5607-aac8-7894-513f3d7175a7)\n",
      "\t Link 0: 50 GB/s\n",
      "\t Link 1: 50 GB/s\n",
      "\t Link 2: 50 GB/s\n",
      "\t Link 3: 50 GB/s\n",
      "\t Link 4: 50 GB/s\n",
      "\t Link 5: 50 GB/s\n",
      "\t Link 6: 50 GB/s\n",
      "\t Link 7: 50 GB/s\n",
      "\t Link 8: 50 GB/s\n",
      "\t Link 9: 50 GB/s\n",
      "\t Link 10: 50 GB/s\n",
      "\t Link 11: 50 GB/s\n",
      "\t Link 12: 50 GB/s\n",
      "\t Link 13: 50 GB/s\n",
      "\t Link 14: 50 GB/s\n",
      "\t Link 15: 50 GB/s\n",
      "\t Link 16: 50 GB/s\n",
      "\t Link 17: 50 GB/s\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION RECOMMENDATIONS\n",
      "======================================================================\n",
      "✅ Enough VRAM for large models — recommended EP/DP execution\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"=\"*70)\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "# Check if nvidia-smi exists\n",
    "if shutil.which(\"nvidia-smi\") is None:\n",
    "    print(\"❌ nvidia-smi not found — NVIDIA drivers are missing or not in PATH.\")\n",
    "    print(\"   Unable to detect GPU hardware.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU DETAILS (nvidia-smi)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Query GPU name + total memory\n",
    "    query_cmd = [\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=name,memory.total\",\n",
    "        \"--format=csv,noheader,nounits\"\n",
    "    ]\n",
    "\n",
    "    output = subprocess.check_output(query_cmd, text=True)\n",
    "    lines = [line.strip() for line in output.splitlines() if line.strip()]\n",
    "    total_memory_gb = 0.0\n",
    "\n",
    "    print(f\"Number of GPUs detected: {len(lines)}\")\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        name, mem_mib = [x.strip() for x in line.split(\",\")]\n",
    "        mem_gb = float(mem_mib) / 1024\n",
    "        total_memory_gb += mem_gb\n",
    "\n",
    "        print(f\"\\nGPU[{i}]:\")\n",
    "        print(f\"  Name: {name}\")\n",
    "        print(f\"  Total Memory: {mem_gb:.2f} GB\")\n",
    "\n",
    "        if \"H200\" in name:\n",
    "            print(\"  Status: ✅ Hopper architecture - Supported\")\n",
    "        elif \"B200\" in name or \"GB200\" in name:\n",
    "            print(\"  Status: ✅ Blackwell architecture - Optimal\")\n",
    "        else:\n",
    "            print(\"  Status: ⚠️ Unknown/older architecture — May have limitations\")\n",
    "\n",
    "    print(f\"\\nTotal GPU Memory (All GPUs): {total_memory_gb:.2f} GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to parse GPU info from nvidia-smi\")\n",
    "    print(e)\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NVLINK STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    nvlink = subprocess.check_output([\"nvidia-smi\", \"nvlink\", \"--status\"],\n",
    "                                     text=True, stderr=subprocess.STDOUT)\n",
    "    print(\"✅ NVLink detected & queryable\\n\")\n",
    "    print(nvlink.strip())\n",
    "except:\n",
    "    print(\"⚠️ NVLink not detected or unavailable\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURATION RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if total_memory_gb >= 1100:\n",
    "    print(\"✅ Enough VRAM for large models — recommended EP/DP execution\")\n",
    "elif total_memory_gb >= 900:\n",
    "    print(\"⚠️ Borderline for largest models — FP8 or TP recommended\")\n",
    "elif total_memory_gb > 0:\n",
    "    print(\"❌ VRAM too low for full-size models — use smaller/quantized checkpoints\")\n",
    "else:\n",
    "    print(\"❌ No GPUs detected — GPU is required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a875ac-8e60-481e-b612-e9212b0a3ac7",
   "metadata": {},
   "source": [
    "### Install SGLang and dependencies\n",
    "\n",
    "Install the latest SGLang release (0.3 or newer) so you get the TensorRT-LLM MLA and FlashInfer kernels that power the Mistral Large 3 MoE stack on B200/H200 systems. The commands below use `uv` to make sure compatible wheels are resolved, but you can substitute plain `pip install` if your environment already has the right CUDA toolchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace6b38-c5fd-4232-83b2-7c92917b25c6",
   "metadata": {},
   "source": [
    "Method 1. Via `pip install`\n",
    "\n",
    "This is still WIP as the update is coming soon. Method 2 is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb2876-a3d7-4f24-a791-771d3e010821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install uv\n",
    "# %uv pip install \"sglang\" --prerelease=allow --quiet\n",
    "# %pip install transformers accelerate huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e519fac-6c5d-4f64-9c08-f58b82e4f9af",
   "metadata": {},
   "source": [
    "Method 2. From source (Recommended)\n",
    "\n",
    "Clone the official repository (`https://github.com/sgl-project/sglang.git`) and check out the latest release branch before installing the Python package in editable mode so you pick up the CUDA/TensorRT plugins that ship with SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53a3fc-c951-41ca-a79f-d04471a93e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%git` not found.\n"
     ]
    }
   ],
   "source": [
    "# Use the nvfp4_support branch\n",
    "!git clone -b dcampora/nvfp4_support https://github.com/dcampora/sglang.git && cd sglang\n",
    "\n",
    "# Install the python packages\n",
    "%pip install --upgrade pip\n",
    "%pip install accelerate\n",
    "%pip install -e \"python\"\n",
    "%pip install transformers accelerate huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3e7f4-6c2b-4ccd-ac51-e8a271af9853",
   "metadata": {},
   "source": [
    "## Launch SGLang server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a933547-9da2-4d4c-bc90-29a47b59eedd",
   "metadata": {},
   "source": [
    "We will launch an OpenAI-compatible server. It can be executed directly in this notebook or you can copy the `python3 -m sglang.launch_server` command with the parameters and execute it in a separate terminal window. Make sure to specify the parameters and adjust the values based on your setup.\n",
    "\n",
    "The startup will take long time as the checkpoints need to be loaded.\n",
    "\n",
    "Set the `MODEL_NAME` environment variables (or edit the defaults below) to one of the official checkpoints published by Mistral so the server downloads the correct weights:\n",
    "\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512` (FP8 baseline for B200/H200)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4` (NVFP4 for H100/A100)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-BF16` (full BF16 precision)\n",
    "- `mistralai/Mistral-Large-3-675B-Instruct-2512-Eagle` (draft model for speculative decoding)\n",
    "\n",
    "You can also point `ML3_MODEL` to a local path if you have already mirrored the model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fa688d-d691-48ad-98d7-a71d50b906f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:10:54] INFO _client.py:1025: HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-Large-3-675B-Instruct-2512/revision/main \"HTTP/1.1 200 OK\"\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 18384.55it/s]\n",
      "[2025-12-09 20:10:54] INFO server_args.py:1047: Use trtllm_mla as attention backend on sm100 for DeepseekV3ForCausalLM\n",
      "[2025-12-09 20:10:54] INFO server_args.py:1056: Enable FlashInfer AllReduce Fusion on sm100 for DeepseekV3ForCausalLM\n",
      "[2025-12-09 20:10:54] INFO _client.py:1025: HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512/resolve/main/generation_config.json \"HTTP/1.1 404 Not Found\"\n",
      "[2025-12-09 20:10:54] INFO model_config.py:907: Downcasting torch.float32 to torch.bfloat16.\n",
      "[2025-12-09 20:10:54] WARNING server_args.py:1425: TensorRT-LLM MLA only supports page_size of 32 or 64, changing page_size from None to 64.\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 16951.58it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 109963.03it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 30488.19it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 25137.10it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 27855.91it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 89512.59it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 31775.03it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 50274.19it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 21430.75it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 131072.00it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 107153.75it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 135300.13it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 23227.95it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 139810.13it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 206761.46it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 212754.55it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 188205.95it/s]\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3[Gloo] Rank  is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[Gloo] Rank \n",
      "6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 76\n",
      " is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[2025-12-09 20:11:12 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:12 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:12 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:12 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:12 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:12 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:13 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2025-12-09 20:11:13 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/272 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 2/272 [00:00<00:16, 16.75it/s]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 4/272 [00:00<00:15, 17.01it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 6/272 [00:00<00:15, 17.02it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 9/272 [00:00<00:14, 18.45it/s]\n",
      "Loading safetensors checkpoint shards:   4% Completed | 11/272 [00:00<00:14, 17.71it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 13/272 [00:00<00:15, 16.26it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 15/272 [00:00<00:16, 15.73it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 17/272 [00:01<00:17, 14.89it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 19/272 [00:01<00:17, 14.86it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 21/272 [00:01<00:17, 14.22it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 23/272 [00:01<00:16, 14.92it/s]\n",
      "Loading safetensors checkpoint shards:   9% Completed | 25/272 [00:01<00:17, 14.48it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 27/272 [00:01<00:16, 15.09it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 29/272 [00:01<00:15, 15.60it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 31/272 [00:01<00:15, 16.04it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 33/272 [00:02<00:15, 15.82it/s]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 35/272 [00:02<00:16, 14.79it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 37/272 [00:02<00:15, 15.42it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 39/272 [00:02<00:14, 15.77it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 41/272 [00:02<00:14, 15.70it/s]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 43/272 [00:02<00:13, 16.36it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 45/272 [00:02<00:13, 17.06it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 47/272 [00:02<00:13, 16.66it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 49/272 [00:03<00:13, 16.93it/s]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 51/272 [00:03<00:13, 16.81it/s]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 53/272 [00:03<00:12, 17.39it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 55/272 [00:03<00:31,  6.99it/s]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 57/272 [00:04<00:25,  8.38it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 59/272 [00:04<00:21,  9.94it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 61/272 [00:04<00:18, 11.38it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 63/272 [00:04<00:16, 12.37it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 66/272 [00:04<00:14, 13.91it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 68/272 [00:04<00:13, 14.99it/s]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 70/272 [00:04<00:13, 15.25it/s]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 72/272 [00:05<00:13, 15.23it/s]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 74/272 [00:05<00:12, 15.35it/s]\n",
      "Loading safetensors checkpoint shards:  28% Completed | 76/272 [00:05<00:12, 16.09it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 78/272 [00:05<00:12, 15.74it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 80/272 [00:05<00:11, 16.10it/s]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 82/272 [00:05<00:11, 16.46it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 84/272 [00:05<00:11, 15.92it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 86/272 [00:05<00:11, 15.52it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 88/272 [00:06<00:11, 15.87it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 90/272 [00:06<00:11, 15.90it/s]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 92/272 [00:06<00:11, 16.18it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 94/272 [00:06<00:11, 15.91it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 96/272 [00:06<00:11, 15.68it/s]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 98/272 [00:06<00:11, 15.62it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 100/272 [00:06<00:10, 15.99it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 102/272 [00:06<00:10, 16.40it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 104/272 [00:07<00:10, 15.92it/s]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 106/272 [00:07<00:10, 15.27it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 108/272 [00:07<00:10, 15.40it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 110/272 [00:07<00:10, 15.45it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 112/272 [00:07<00:10, 15.48it/s]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 114/272 [00:07<00:10, 14.80it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 116/272 [00:07<00:10, 15.10it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 118/272 [00:07<00:10, 14.38it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 120/272 [00:08<00:10, 15.18it/s]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 122/272 [00:08<00:09, 15.36it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 124/272 [00:08<00:09, 15.08it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 126/272 [00:08<00:10, 14.53it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 128/272 [00:08<00:10, 14.31it/s]\n",
      "Loading safetensors checkpoint shards:  48% Completed | 130/272 [00:08<00:09, 14.46it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 132/272 [00:08<00:09, 14.92it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 134/272 [00:09<00:11, 11.84it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 136/272 [00:09<00:10, 12.46it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 138/272 [00:09<00:10, 13.38it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 140/272 [00:09<00:09, 13.58it/s]\n",
      "Loading safetensors checkpoint shards:  52% Completed | 142/272 [00:09<00:09, 13.50it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 144/272 [00:09<00:08, 14.25it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 146/272 [00:09<00:08, 14.80it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 148/272 [00:10<00:08, 14.53it/s]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 150/272 [00:10<00:08, 14.12it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 152/272 [00:10<00:08, 14.73it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 154/272 [00:10<00:07, 14.94it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 156/272 [00:10<00:07, 15.79it/s]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 158/272 [00:10<00:07, 15.43it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 160/272 [00:10<00:07, 15.49it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 162/272 [00:11<00:07, 15.21it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 164/272 [00:11<00:07, 15.35it/s]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 166/272 [00:11<00:06, 15.43it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 168/272 [00:12<00:16,  6.12it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 170/272 [00:12<00:13,  7.54it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 172/272 [00:12<00:11,  9.03it/s]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 174/272 [00:12<00:09, 10.16it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 176/272 [00:12<00:08, 11.33it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 178/272 [00:12<00:07, 12.91it/s]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 180/272 [00:12<00:06, 14.22it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 182/272 [00:12<00:06, 14.73it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 184/272 [00:13<00:05, 14.85it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 186/272 [00:13<00:05, 14.59it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 188/272 [00:13<00:05, 14.60it/s]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 190/272 [00:13<00:05, 15.63it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 192/272 [00:13<00:05, 15.41it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 194/272 [00:13<00:05, 15.02it/s]\n",
      "Loading safetensors checkpoint shards:  72% Completed | 196/272 [00:13<00:05, 14.09it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 198/272 [00:13<00:04, 14.80it/s]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 200/272 [00:14<00:04, 15.27it/s]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 202/272 [00:14<00:04, 15.24it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 204/272 [00:14<00:04, 15.59it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 206/272 [00:14<00:04, 15.50it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 208/272 [00:14<00:04, 15.28it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 210/272 [00:14<00:03, 15.56it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 212/272 [00:14<00:03, 15.04it/s]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 214/272 [00:15<00:03, 14.81it/s]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 216/272 [00:15<00:03, 14.25it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 218/272 [00:15<00:03, 14.34it/s]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 220/272 [00:15<00:03, 14.43it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 222/272 [00:15<00:03, 14.01it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 224/272 [00:15<00:03, 13.99it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 226/272 [00:15<00:03, 14.53it/s]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 228/272 [00:16<00:03, 14.24it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 230/272 [00:16<00:03, 13.95it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 232/272 [00:16<00:04, 10.00it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 234/272 [00:16<00:03, 10.54it/s]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 236/272 [00:16<00:03, 11.20it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 238/272 [00:16<00:02, 11.65it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 240/272 [00:17<00:02, 12.22it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 242/272 [00:17<00:02, 12.65it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 244/272 [00:17<00:02, 13.15it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 246/272 [00:17<00:02, 12.82it/s]\n",
      "Loading safetensors checkpoint shards:  91% Completed | 248/272 [00:17<00:01, 13.64it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 250/272 [00:17<00:01, 13.99it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 252/272 [00:17<00:01, 13.85it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 254/272 [00:18<00:01, 14.25it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 256/272 [00:18<00:01, 13.74it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 258/272 [00:18<00:01, 13.97it/s]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 260/272 [00:18<00:00, 14.47it/s]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 262/272 [00:18<00:00, 15.41it/s]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 264/272 [00:18<00:00, 16.23it/s]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 266/272 [00:18<00:00, 16.79it/s]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 268/272 [00:18<00:00, 17.33it/s]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 270/272 [00:19<00:00, 17.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 272/272 [00:19<00:00, 17.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 272/272 [00:19<00:00, 14.19it/s]\n",
      "\n",
      "Capturing batches (bs=16 avail_mem=7.18 GB):   0%|          | 0/6 [00:00<?, ?it/s]/home/shadeform/miniconda/envs/sglang/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "rank 0 allocated ipc_handles: [['0x7bf9d2000000', '0x7bf9b4000000', '0x7bf9a6000000', '0x7bf998000000', '0x7bf98a000000', '0x7bf97c000000', '0x7bf96e000000', '0x7bf960000000'], ['0x7c0fe1c00000', '0x7c0fe1e00000', '0x7c0fe3a00000', '0x7c0fe3c00000', '0x7c0fe3e00000', '0x7c0fe5a00000', '0x7c0fe5c00000', '0x7c0fe5e00000'], ['0x7bf936000000', '0x7bf90c000000', '0x7bf8e2000000', '0x7bf8b8000000', '0x7bf88e000000', '0x7bf864000000', '0x7bf83a000000', '0x7bf810000000']]rank 1 allocated ipc_handles: [['0x7cdd60000000', '0x7cdd7e000000', '0x7cdd52000000', '0x7cdd44000000', '0x7cdd36000000', '0x7cdd28000000', '0x7cdd1a000000', '0x7cdd0c000000'], ['0x7cf301000000', '0x7cf300e00000', '0x7cf301200000', '0x7cf301400000', '0x7cf301600000', '0x7cf301800000', '0x7cf301a00000', '0x7cf301c00000'], ['0x7cdcb8000000', '0x7cdce2000000', '0x7cdc8e000000', '0x7cdc64000000', '0x7cdc3a000000', '0x7cdc10000000', '0x7cdbe6000000', '0x7cdbbc000000']]\n",
      "rank 5 allocated ipc_handles: [['0x7680dc000000', '0x7680ce000000', '0x7680c0000000', '0x7680b2000000', '0x7680a4000000', '0x7680fa000000', '0x768096000000', '0x768088000000'], ['0x76970d000000', '0x76970d200000', '0x76970d400000', '0x76970d600000', '0x76970d800000', '0x76970ce00000', '0x76970da00000', '0x76970dc00000'], ['0x768034000000', '0x76800a000000', '0x767fe0000000', '0x767fb6000000', '0x767f8c000000', '0x76805e000000', '0x767f62000000', '0x767f38000000']]rank 6 allocated ipc_handles: [['0x73a034000000', '0x73a026000000', '0x73a018000000', '0x73a00a000000', '0x739ffc000000', '0x739fee000000', '0x73a052000000', '0x739fe0000000'], ['0x73b5e1000000', '0x73b5e1200000', '0x73b5e1400000', '0x73b5e1600000', '0x73b5e1800000', '0x73b5e1a00000', '0x73b5e0e00000', '0x73b5e1c00000'], ['0x739f8c000000', '0x739f62000000', '0x739f38000000', '0x739f0e000000', '0x739ee4000000', '0x739eba000000', '0x739fb6000000', '0x739e90000000']]\n",
      "\n",
      "rank 3 allocated ipc_handles: [['0x6ff4d8000000', '0x6ff4ca000000', '0x6ff4bc000000', '0x6ff4f6000000', '0x6ff4ae000000', '0x6ff4a0000000', '0x6ff492000000', '0x6ff484000000'], ['0x700a91000000', '0x700a91200000', '0x700a91400000', '0x700a90e00000', '0x700a91600000', '0x700a91800000', '0x700a91a00000', '0x700a91c00000'], ['0x6ff430000000', '0x6ff406000000', '0x6ff3dc000000', '0x6ff45a000000', '0x6ff3b2000000', '0x6ff388000000', '0x6ff35e000000', '0x6ff334000000']]\n",
      "rank 7 allocated ipc_handles: [['0x7e8008000000', '0x7e7ffa000000', '0x7e7fec000000', '0x7e7fde000000', '0x7e7fd0000000', '0x7e7fc2000000', '0x7e7fb4000000', '0x7e8026000000'], ['0x7e9629e00000', '0x7e962ba00000', '0x7e962bc00000', '0x7e962be00000', '0x7e962da00000', '0x7e962dc00000', '0x7e962de00000', '0x7e9629c00000'], ['0x7e7f60000000', '0x7e7f36000000', '0x7e7f0c000000', '0x7e7ee2000000', '0x7e7eb8000000', '0x7e7e8e000000', '0x7e7e64000000', '0x7e7f8a000000']]\n",
      "rank 2 allocated ipc_handles: [['0x7c5af4000000', '0x7c5ae6000000', '0x7c5b12000000', '0x7c5ad8000000', '0x7c5aca000000', '0x7c5abc000000', '0x7c5aae000000', '0x7c5aa0000000'], ['0x7c70d1000000', '0x7c70d1200000', '0x7c70d0e00000', '0x7c70d1400000', '0x7c70d1600000', '0x7c70d1800000', '0x7c70d1a00000', '0x7c70d1c00000'], ['0x7c5a4c000000', '0x7c5a22000000', '0x7c5a76000000', '0x7c59f8000000', '0x7c59ce000000', '0x7c59a4000000', '0x7c597a000000', '0x7c5950000000']]rank 4 allocated ipc_handles: [['0x70bb90000000', '0x70bb82000000', '0x70bb74000000', '0x70bb66000000', '0x70bbae000000', '0x70bb58000000', '0x70bb4a000000', '0x70bb3c000000'], ['0x70d155000000', '0x70d155200000', '0x70d155400000', '0x70d155600000', '0x70d154e00000', '0x70d155800000', '0x70d155a00000', '0x70d155c00000'], ['0x70bae8000000', '0x70babe000000', '0x70ba94000000', '0x70ba6a000000', '0x70bb12000000', '0x70ba40000000', '0x70ba16000000', '0x70b9ec000000']]\n",
      "\n",
      "\n",
      "[2025-12-09 20:11:38.999] [info] lamportInitialize start: buffer: 0x739fb6000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 6 workspace[0] 0x73a034000000\n",
      "Rank 6 workspace[1] 0x73a026000000\n",
      "Rank 6 workspace[2] 0x73a018000000\n",
      "Rank 6 workspace[3] 0x73a00a000000\n",
      "Rank 6 workspace[4] 0x739ffc000000\n",
      "Rank 6 workspace[5] 0x739fee000000\n",
      "Rank 6 workspace[6] 0x73a052000000\n",
      "Rank 6 workspace[7] 0x739fe0000000\n",
      "Rank 6 workspace[8] 0x73b5e1000000\n",
      "Rank 6 workspace[9] 0x73b5e1200000\n",
      "Rank 6 workspace[10] 0x73b5e1400000\n",
      "Rank 6 workspace[11] 0x73b5e1600000\n",
      "Rank 6 workspace[12] 0x73b5e1800000\n",
      "Rank 6 workspace[13] 0x73b5e1a00000\n",
      "Rank 6 workspace[14] 0x73b5e0e00000\n",
      "Rank 6 workspace[15] 0x73b5e1c00000\n",
      "Rank 6 workspace[16] 0x739f8c000000\n",
      "Rank 6 workspace[17] 0x739f62000000\n",
      "Rank 6 workspace[18] 0x739f38000000\n",
      "Rank 6 workspace[19] 0x739f0e000000\n",
      "Rank 6 workspace[20] 0x739ee4000000\n",
      "Rank 6 workspace[21] 0x739eba000000\n",
      "Rank 6 workspace[22] 0x739fb6000000\n",
      "Rank 6 workspace[23] 0x739e90000000\n",
      "Rank 6 workspace[24] 0x73cd8f28c400\n",
      "[2025-12-09 20:11:39.049] [info] lamportInitialize start: buffer: 0x7e7f8a000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 7 workspace[0] 0x7e8008000000\n",
      "Rank 7 workspace[1] 0x7e7ffa000000\n",
      "Rank 7 workspace[2] 0x7e7fec000000\n",
      "Rank 7 workspace[3] 0x7e7fde000000\n",
      "Rank 7 workspace[4] 0x7e7fd0000000\n",
      "Rank 7 workspace[5] 0x7e7fc2000000\n",
      "Rank 7 workspace[6] 0x7e7fb4000000\n",
      "Rank 7 workspace[7] 0x7e8026000000\n",
      "Rank 7 workspace[8] 0x7e9629e00000\n",
      "Rank 7 workspace[9] 0x7e962ba00000\n",
      "Rank 7 workspace[10] 0x7e962bc00000\n",
      "Rank 7 workspace[11] 0x7e962be00000\n",
      "Rank 7 workspace[12] 0x7e962da00000\n",
      "Rank 7 workspace[13] 0x7e962dc00000\n",
      "Rank 7 workspace[14] 0x7e962de00000\n",
      "Rank 7 workspace[15] 0x7e9629c00000\n",
      "Rank 7 workspace[16] 0x7e7f60000000\n",
      "Rank 7 workspace[17] 0x7e7f36000000\n",
      "Rank 7 workspace[18] 0x7e7f0c000000\n",
      "Rank 7 workspace[19] 0x7e7ee2000000\n",
      "Rank 7 workspace[20] 0x7e7eb8000000\n",
      "Rank 7 workspace[21] 0x7e7e8e000000\n",
      "Rank 7 workspace[22] 0x7e7e64000000\n",
      "Rank 7 workspace[23] 0x7e7f8a000000\n",
      "Rank 7 workspace[24] 0x7ead5f28c400\n",
      "[2025-12-09 20:11:39.100] [info] lamportInitialize start: buffer: 0x7cdce2000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 1 workspace[0] 0x7cdd60000000\n",
      "Rank 1 workspace[1] 0x7cdd7e000000\n",
      "Rank 1 workspace[2] 0x7cdd52000000\n",
      "Rank 1 workspace[3] 0x7cdd44000000\n",
      "Rank 1 workspace[4] 0x7cdd36000000\n",
      "Rank 1 workspace[5] 0x7cdd28000000\n",
      "Rank 1 workspace[6] 0x7cdd1a000000\n",
      "Rank 1 workspace[7] 0x7cdd0c000000\n",
      "Rank 1 workspace[8] 0x7cf301000000\n",
      "Rank 1 workspace[9] 0x7cf300e00000\n",
      "Rank 1 workspace[10] 0x7cf301200000\n",
      "Rank 1 workspace[11] 0x7cf301400000\n",
      "Rank 1 workspace[12] 0x7cf301600000\n",
      "Rank 1 workspace[13] 0x7cf301800000\n",
      "Rank 1 workspace[14] 0x7cf301a00000\n",
      "Rank 1 workspace[15] 0x7cf301c00000\n",
      "Rank 1 workspace[16] 0x7cdcb8000000\n",
      "Rank 1 workspace[17] 0x7cdce2000000\n",
      "Rank 1 workspace[18] 0x7cdc8e000000\n",
      "Rank 1 workspace[19] 0x7cdc64000000\n",
      "Rank 1 workspace[20] 0x7cdc3a000000\n",
      "Rank 1 workspace[21] 0x7cdc10000000\n",
      "Rank 1 workspace[22] 0x7cdbe6000000\n",
      "Rank 1 workspace[23] 0x7cdbbc000000\n",
      "Rank 1 workspace[24] 0x7d0ab928c400\n",
      "[2025-12-09 20:11:39.150] [info] lamportInitialize start: buffer: 0x76805e000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 5 workspace[0] 0x7680dc000000\n",
      "Rank 5 workspace[1] 0x7680ce000000\n",
      "Rank 5 workspace[2] 0x7680c0000000\n",
      "Rank 5 workspace[3] 0x7680b2000000\n",
      "Rank 5 workspace[4] 0x7680a4000000\n",
      "Rank 5 workspace[5] 0x7680fa000000\n",
      "Rank 5 workspace[6] 0x768096000000\n",
      "Rank 5 workspace[7] 0x768088000000\n",
      "Rank 5 workspace[8] 0x76970d000000\n",
      "Rank 5 workspace[9] 0x76970d200000\n",
      "Rank 5 workspace[10] 0x76970d400000\n",
      "Rank 5 workspace[11] 0x76970d600000\n",
      "Rank 5 workspace[12] 0x76970d800000\n",
      "Rank 5 workspace[13] 0x76970ce00000\n",
      "Rank 5 workspace[14] 0x76970da00000\n",
      "Rank 5 workspace[15] 0x76970dc00000\n",
      "Rank 5 workspace[16] 0x768034000000\n",
      "Rank 5 workspace[17] 0x76800a000000\n",
      "Rank 5 workspace[18] 0x767fe0000000\n",
      "Rank 5 workspace[19] 0x767fb6000000\n",
      "Rank 5 workspace[20] 0x767f8c000000\n",
      "Rank 5 workspace[21] 0x76805e000000\n",
      "Rank 5 workspace[22] 0x767f62000000\n",
      "Rank 5 workspace[23] 0x767f38000000\n",
      "Rank 5 workspace[24] 0x76aec328c400\n",
      "[2025-12-09 20:11:39.200] [info] lamportInitialize start: buffer: 0x7c5a76000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 2 workspace[0] 0x7c5af4000000\n",
      "Rank 2 workspace[1] 0x7c5ae6000000\n",
      "Rank 2 workspace[2] 0x7c5b12000000\n",
      "Rank 2 workspace[3] 0x7c5ad8000000\n",
      "Rank 2 workspace[4] 0x7c5aca000000\n",
      "Rank 2 workspace[5] 0x7c5abc000000\n",
      "Rank 2 workspace[6] 0x7c5aae000000\n",
      "Rank 2 workspace[7] 0x7c5aa0000000\n",
      "Rank 2 workspace[8] 0x7c70d1000000\n",
      "Rank 2 workspace[9] 0x7c70d1200000\n",
      "Rank 2 workspace[10] 0x7c70d0e00000\n",
      "Rank 2 workspace[11] 0x7c70d1400000\n",
      "Rank 2 workspace[12] 0x7c70d1600000\n",
      "Rank 2 workspace[13] 0x7c70d1800000\n",
      "Rank 2 workspace[14] 0x7c70d1a00000\n",
      "Rank 2 workspace[15] 0x7c70d1c00000\n",
      "Rank 2 workspace[16] 0x7c5a4c000000\n",
      "Rank 2 workspace[17] 0x7c5a22000000\n",
      "Rank 2 workspace[18] 0x7c5a76000000\n",
      "Rank 2 workspace[19] 0x7c59f8000000\n",
      "Rank 2 workspace[20] 0x7c59ce000000\n",
      "Rank 2 workspace[21] 0x7c59a4000000\n",
      "Rank 2 workspace[22] 0x7c597a000000\n",
      "Rank 2 workspace[23] 0x7c5950000000\n",
      "Rank 2 workspace[24] 0x7c888928c400\n",
      "[2025-12-09 20:11:39.250] [info] lamportInitialize start: buffer: 0x6ff45a000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 3 workspace[0] 0x6ff4d8000000\n",
      "Rank 3 workspace[1] 0x6ff4ca000000\n",
      "Rank 3 workspace[2] 0x6ff4bc000000\n",
      "Rank 3 workspace[3] 0x6ff4f6000000\n",
      "Rank 3 workspace[4] 0x6ff4ae000000\n",
      "Rank 3 workspace[5] 0x6ff4a0000000\n",
      "Rank 3 workspace[6] 0x6ff492000000\n",
      "Rank 3 workspace[7] 0x6ff484000000\n",
      "Rank 3 workspace[8] 0x700a91000000\n",
      "Rank 3 workspace[9] 0x700a91200000\n",
      "Rank 3 workspace[10] 0x700a91400000\n",
      "Rank 3 workspace[11] 0x700a90e00000\n",
      "Rank 3 workspace[12] 0x700a91600000\n",
      "Rank 3 workspace[13] 0x700a91800000\n",
      "Rank 3 workspace[14] 0x700a91a00000\n",
      "Rank 3 workspace[15] 0x700a91c00000\n",
      "Rank 3 workspace[16] 0x6ff430000000\n",
      "Rank 3 workspace[17] 0x6ff406000000\n",
      "Rank 3 workspace[18] 0x6ff3dc000000\n",
      "Rank 3 workspace[19] 0x6ff45a000000\n",
      "Rank 3 workspace[20] 0x6ff3b2000000\n",
      "Rank 3 workspace[21] 0x6ff388000000\n",
      "Rank 3 workspace[22] 0x6ff35e000000\n",
      "Rank 3 workspace[23] 0x6ff334000000\n",
      "Rank 3 workspace[24] 0x70224928c400\n",
      "[2025-12-09 20:11:39.300] [info] lamportInitialize start: buffer: 0x7bf936000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 0 workspace[0] 0x7bf9d2000000\n",
      "Rank 0 workspace[1] 0x7bf9b4000000\n",
      "Rank 0 workspace[2] 0x7bf9a6000000\n",
      "Rank 0 workspace[3] 0x7bf998000000\n",
      "Rank 0 workspace[4] 0x7bf98a000000\n",
      "Rank 0 workspace[5] 0x7bf97c000000\n",
      "Rank 0 workspace[6] 0x7bf96e000000\n",
      "Rank 0 workspace[7] 0x7bf960000000\n",
      "Rank 0 workspace[8] 0x7c0fe1c00000\n",
      "Rank 0 workspace[9] 0x7c0fe1e00000\n",
      "Rank 0 workspace[10] 0x7c0fe3a00000\n",
      "Rank 0 workspace[11] 0x7c0fe3c00000\n",
      "Rank 0 workspace[12] 0x7c0fe3e00000\n",
      "Rank 0 workspace[13] 0x7c0fe5a00000\n",
      "Rank 0 workspace[14] 0x7c0fe5c00000\n",
      "Rank 0 workspace[15] 0x7c0fe5e00000\n",
      "Rank 0 workspace[16] 0x7bf936000000\n",
      "Rank 0 workspace[17] 0x7bf90c000000\n",
      "Rank 0 workspace[18] 0x7bf8e2000000\n",
      "Rank 0 workspace[19] 0x7bf8b8000000\n",
      "Rank 0 workspace[20] 0x7bf88e000000\n",
      "Rank 0 workspace[21] 0x7bf864000000\n",
      "Rank 0 workspace[22] 0x7bf83a000000\n",
      "Rank 0 workspace[23] 0x7bf810000000\n",
      "Rank 0 workspace[24] 0x7c270728c400\n",
      "/home/shadeform/miniconda/envs/sglang/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[2025-12-09 20:11:39.350] [info] lamportInitialize start: buffer: 0x70bb12000000, size: 352321536\n",
      "set flag_ptr[3] = lamport_comm_size:  234881024\n",
      "Rank 4 workspace[0] 0x70bb90000000\n",
      "Rank 4 workspace[1] 0x70bb82000000\n",
      "Rank 4 workspace[2] 0x70bb74000000\n",
      "Rank 4 workspace[3] 0x70bb66000000\n",
      "Rank 4 workspace[4] 0x70bbae000000\n",
      "Rank 4 workspace[5] 0x70bb58000000\n",
      "Rank 4 workspace[6] 0x70bb4a000000\n",
      "Rank 4 workspace[7] 0x70bb3c000000\n",
      "Rank 4 workspace[8] 0x70d155000000\n",
      "Rank 4 workspace[9] 0x70d155200000\n",
      "Rank 4 workspace[10] 0x70d155400000\n",
      "Rank 4 workspace[11] 0x70d155600000\n",
      "Rank 4 workspace[12] 0x70d154e00000\n",
      "Rank 4 workspace[13] 0x70d155800000\n",
      "Rank 4 workspace[14] 0x70d155a00000\n",
      "Rank 4 workspace[15] 0x70d155c00000\n",
      "Rank 4 workspace[16] 0x70bae8000000\n",
      "Rank 4 workspace[17] 0x70babe000000\n",
      "Rank 4 workspace[18] 0x70ba94000000\n",
      "Rank 4 workspace[19] 0x70ba6a000000\n",
      "Rank 4 workspace[20] 0x70bb12000000\n",
      "Rank 4 workspace[21] 0x70ba40000000\n",
      "Rank 4 workspace[22] 0x70ba16000000\n",
      "Rank 4 workspace[23] 0x70b9ec000000\n",
      "Rank 4 workspace[24] 0x70e90d28c400\n",
      "[2025-12-09 20:11:39 TP1] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP6] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP7] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP4] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP2] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP0] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP5] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP3] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=4096,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP7] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP7] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP5] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP5] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP1] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP1] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP6] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP6] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP2] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP0] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP2] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP0] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP4] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP4] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP3] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=1024,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP3] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json\n",
      "[2025-12-09 20:11:39 TP6] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP6] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP1] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP1] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP7] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP7] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP2] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP2] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP0] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP5] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP0] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP5] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP4] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP4] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP3] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "[2025-12-09 20:11:39 TP3] Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /home/shadeform/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128]_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
      "Capturing batches (bs=1 avail_mem=5.27 GB): 100%|██████████| 6/6 [00:08<00:00,  1.44s/it] \n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 20531.56it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 72137.91it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 135300.13it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 36654.34it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 39568.91it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 8838.09it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 9741.25it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 155344.59it/s]\n",
      "[2025-12-09 20:11:50] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
      "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
      "                    \n",
      "SGLang server ready on port 34916 with served name 'mistralai/Mistral-Large-3-675B-Instruct-2512'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sglang.utils import launch_server_cmd, wait_for_server, terminate_process\n",
    "\n",
    "model_path = os.environ.get(\"ML3_MODEL\", \"mistralai/Mistral-Large-3-675B-Instruct-2512\")\n",
    "port = int(os.environ.get(\"SGLANG_PORT\", \"30000\"))\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"mistralai/Mistral-Large-3-675B-Instruct-2512\")\n",
    "os.environ[\"SGLANG_ENABLE_JIT_DEEPGEMM\"] = \"0\"\n",
    "server_cmd = f\"\"\"\n",
    "python3 -m sglang.launch_server \\\n",
    "    --model {model_path} \\\n",
    "    --host 0.0.0.0 --port {port} \\\n",
    "    --tensor-parallel-size 8 \\\n",
    "    --disable-radix-cache \\\n",
    "    --stream-interval 20 \\\n",
    "    --mem-fraction-static 0.95 \\\n",
    "    --max-running-requests 1024 \\\n",
    "    --cuda-graph-max-bs 16 \\\n",
    "    --served-model-name {MODEL_NAME} \\\n",
    "    --log-level warning \\\n",
    "    --chat-template mistral\n",
    "\"\"\"\n",
    "\n",
    "server_process, detected_port = launch_server_cmd(server_cmd)\n",
    "wait_for_server(f\"http://localhost:{detected_port}\")\n",
    "print(f\"SGLang server ready on port {detected_port} with served name '{MODEL_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a31b2-61a3-426c-9af9-fcb7a759c679",
   "metadata": {},
   "source": [
    "## Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a835dd-8659-4eea-9166-795e630de3a0",
   "metadata": {},
   "source": [
    "Once the server is running, connect using the OpenAI Python client. The endpoint exposes an OpenAI-compatible interface, so the standard OpenAI Python client will work without any additional adapters or client-side modifications. You simply point the client to your local server URL and provide the API key expected by vLLM (it can be any non-empty string unless you explicitly enforce authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38238534-2ef0-4620-b5b1-9c98dbb732e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SGLang server at http://localhost:34916/v1\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url = f\"http://localhost:{detected_port}/v1\"\n",
    "api_key = \"dummy\"  # SGLang server doesn't require an API key by default\n",
    "\n",
    "# Connect to SGLang server\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "print(f\"Connected to SGLang server at {base_url}\")\n",
    "# print(f\"Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963d689-223a-4efe-96c7-4dc4c3876e5a",
   "metadata": {},
   "source": [
    "## Testing some scenarios\n",
    "\n",
    "According to its authors, Mistral Large 3 is perfect for:\n",
    "\n",
    "* Long document understanding\n",
    "* Daily-driver AI assistants\n",
    "* Agentic and tool-use capabilities\n",
    "* Enterprise knowledge work\n",
    "* General coding assistant\n",
    "\n",
    "Let's test some of its features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddffb17-c48b-4b59-992b-901b038ae4c1",
   "metadata": {},
   "source": [
    "### Instruction following\n",
    "\n",
    "To guide the model toward a specific behavior or response style, you can supply a system prompt that defines rules, tone, formatting expectations, and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49a02f-99ae-4c3c-a434-05bfe40b75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Game Name: \"Time Warp Trek\"**\n",
      "\n",
      "**Objective:** Be the first to travel through 3 different historical eras, collect unique artifacts, and return to the present with the most \"Chrono Points\"!\n",
      "\n",
      "**Setup:** 2-4 players, a spiral board with 4 paths (one per era: Prehistoric, Medieval, Industrial, Future), each path has 10 spaces. Players start in the present (center).\n",
      "\n",
      "**Gameplay:**\n",
      "- Roll a 6-sided die to move forward in an era.\n",
      "- Land on artifact spaces to collect them (e.g., dinosaur bone, knight’s sword, steam engine, hologram).\n",
      "- Land on \"Time Rift\" spaces to jump to another era (but lose 1 artifact).\n",
      "- Complete an era by reaching its end, earning bonus points.\n",
      "- Return to the present by rolling a 6 or using a \"Time Portal\" card.\n",
      "\n",
      "**Winning:** After returning, calculate points: 1 per artifact, 5 per completed era, 2 for rare artifacts. Highest score wins! ⏳🚀\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def load_system_prompt(model_name: str = \"mistralai/Mistral-Large-3-675B-Instruct-2512\") -> str:\n",
    "    \"\"\"\n",
    "    Download and load the system prompt from Hugging Face.\n",
    "    The file is automatically cached after first download.\n",
    "    \"\"\"\n",
    "    prompt_path = hf_hub_download(repo_id=model_name, filename=\"SYSTEM_PROMPT.txt\")\n",
    "    with open(prompt_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "\n",
    "# Load system prompt from Hugging Face\n",
    "SYSTEM_PROMPT = load_system_prompt()\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model= \"mistralai/Mistral-Large-3-675B-Instruct-2512\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"Invent a fun board game and explain the rules in under 120 words.\"}\n",
    "    ],\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384a4d4-7d8a-44f9-85da-94cea2a094e0",
   "metadata": {},
   "source": [
    "### Vision reasoning\n",
    "\n",
    "Vision reasoning refers to the model’s ability to interpret visual inputs and apply logical inference on top of what it sees — not just recognizing objects, but understanding relationships, spatial context, cause-and-effect, and intent within an image. Instead of simply labelling elements, the model can describe scenes, infer actions, identify patterns, and answer questions that require comprehension rather than pattern-matching alone. This enables more advanced use cases such as analyzing diagrams, extracting information from charts, interpreting UI layouts, or evaluating photos for consistency and meaning. In short, vision reasoning bridges visual perception and conceptual understanding, allowing the model to think about images rather than merely see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e6f43-2cf8-4088-a499-a4e2bff6afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This scene appears to be a cluttered, old-fashioned atelier or workshop—perhaps a sculptor’s studio, given the presence of busts, tools, and raw materials. Here’s an unusual and creative scenario for what could happen next:\n",
      "\n",
      "---\n",
      "\n",
      "### **The Unfinished Busts Awaken**\n",
      "As the last flicker of the warm, dim lighting settles into the room, the studio’s eerie silence is broken by a faint *crack*. One of the half-finished clay busts on the right side of the image—the one with a blank face—suddenly shifts. Its hollow eyes fill with a strange, glowing mist, and the clay begins to ripple as if something beneath it is struggling to surface.\n",
      "\n",
      "Then, another bust joins in. And another. The unfinished sculptures, long dormant, start to *breathe*. Their chests expand slightly, their fingers twitch where they’ve been sculpted, and the tools scattered across the workbenches vibrate in response. The studio’s owner, who had stepped out for a late-night coffee, returns to find their workspace alive with silent, incomplete figures.\n",
      "\n",
      "But this isn’t a horror story—it’s a revelation. The busts weren’t just abandoned; they were *waiting*. Each one represents a historical figure whose spirit was trapped in the clay, unable to move on until their likeness was properly finished. The sculptor, now realizing their role, picks up a chisel and approaches the first bust. As they carve, the mist in its eyes solidifies into a face, and the figure speaks—not in words, but in *memories*.\n",
      "\n",
      "The studio becomes a time portal. The sculptor, guided by the whispers of the past, works feverishly to complete each bust, releasing the spirits one by one. Some share forgotten wisdom, others warn of impending events, and a few simply sigh in relief before their forms crumble into dust, their souls finally free.\n",
      "\n",
      "But then, the most unusual thing happens: the tools begin to *move on their own*. The scattered sketches on the walls rearrange into a pattern, revealing a hidden message. The studio itself is alive, and it’s been *testing* the sculptor all along. Now, with the spirits’ help, they must uncover the workshop’s true purpose—perhaps it’s a sanctuary for lost creativity, or a bridge between art and the afterlife.\n",
      "\n",
      "And as the last bust is completed, the studio’s lights dim once more. The tools return to their places. The next morning, the sculptor wakes up with clay under their fingernails and a new masterpiece in their hands—one they don’t remember creating. But the face is unmistakable: it’s their own, sculpted with an expression of profound understanding.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to refine any part of this scenario or explore a different direction? Maybe something more whimsical, or even sci-fi?\n"
     ]
    }
   ],
   "source": [
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "\n",
    "# Load system prompt from Hugging Face\n",
    "SYSTEM_PROMPT = load_system_prompt()\n",
    "\n",
    "# Feel free to replace with any image of your choice!\n",
    "IMAGE_URL = \"https://blogs.nvidia.com/wp-content/uploads/2020/11/marbles-at-night.jpg\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Describe what can happen next in this scene. Be creative and think of an unusual scenario\",\n",
    "            },\n",
    "            {\"type\": \"image_url\", \n",
    "             \"image_url\": {\"url\": IMAGE_URL}\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model= \"mistralai/Mistral-Large-3-675B-Instruct-2512\",\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019c348-4653-4ba3-b60a-0a0ffef3f22e",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Function calling allows the model to generate structured outputs that trigger real functions in your application, turning natural-language queries into executable actions. Instead of returning plain text, the model produces arguments in a predefined schema, enabling you to safely map intent to code paths — such as querying a database, retrieving documents, sending notifications, or performing calculations. This turns the model into a reasoning layer that interprets user requests, decides when a tool should be invoked, and returns well-formed call signatures that programs can act on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5074f6-ddc0-4fbd-970a-c481efbfeafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:42:44] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to depict the interior of a modern, industrial-style kitchen, likely in a restaurant. The signage on the wall in the background includes Japanese characters (「いらっしゃいませ」, which translates to \"Welcome\" in English). This suggests that the country shown in the image is Japan. Additionally, the overall design and layout of the kitchen are consistent with those commonly found in Japanese eateries. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-28 17:42:45] INFO _client.py:1025: HTTP Request: POST http://localhost:39763/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "TEMP = 0.15\n",
    "MAX_TOK = 100000\n",
    "IMAGE_URL = \"https://cdna.artstation.com/p/assets/images/images/050/827/584/large/rafael-chies-14.jpg?1655798602\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_population\",\n",
    "            \"description\": \"Get the up-to-date population of a given country.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"country\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The country to find the population of.\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unit for the population.\",\n",
    "                        \"enum\": [\"millions\", \"thousands\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"country\", \"unit\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rewrite\",\n",
    "            \"description\": \"Rewrite a given text for improved clarity\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The input text to rewrite\",\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Can you tell me which country is shown in this image?\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": IMAGE_URL,\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "assistant_message = resp.choices[0].message.content\n",
    "print(assistant_message, \"\\n\")\n",
    "\n",
    "messages.extend([\n",
    "    {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "    {\"role\": \"user\", \"content\": \"What is the population of that country in millions?\"},\n",
    "])\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=TEMP,\n",
    "    max_tokens=MAX_TOK,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819a1b2-a001-4be3-a34e-95805e385284",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "If you launched the server from this notebook, run the following cell to terminate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dbcb2c-6b11-4e1f-880d-213b917bb127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No running server process found to terminate.\n"
     ]
    }
   ],
   "source": [
    "if 'server_process' in globals() and server_process.poll() is None:\n",
    "    server_process.kill()\n",
    "    print(f\"Killed instruct server PID {server_process.pid}\")\n",
    "else:\n",
    "    print(\"No running server process found to terminate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da0f7e-8b2f-469b-b569-acc4659cf8ca",
   "metadata": {},
   "source": [
    "## Conclusion and resources\n",
    "\n",
    "Congratulations! You successfully deployed the **Mistral Large 3 675B Instruct** model using SGLang.\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "\n",
    "- Set up your environment and install SGLang.\n",
    "- Launch and manage an OpenAI-compatible server to run model.\n",
    "- Perform instruction following, vision reasoning, and function calling tasks using the OpenAI client.\n",
    "\n",
    "You can adapt tensor parallelism, ports, and sampling parameters to your hardware and application needs.\n",
    "\n",
    "Refer to the following resources if you want to learn more\n",
    "\n",
    "### Documentation\n",
    "- 📚 [Mistral Large 3 Model Card](https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512)\n",
    "- 🏗️ [NVIDIA SGLang Guide](https://docs.nvidia.com/deeplearning/frameworks/sglang-release-notes/overview.html)\n",
    "\n",
    "### Code and kernels\n",
    "- 💾 [Flashinfer kernel library](https://github.com/flashinfer-ai/flashinfer)\n",
    "- ⚡  [FlashMLA Implementation](https://github.com/deepseek-ai/FlashMLA)\n",
    "- 🧪 [Mistral Examples](https://github.com/mistralai)\n",
    "\n",
    "### Community\n",
    "- 📧 [NVIDIA Developer Forums](https://forums.developer.nvidia.com/)\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "**Authors:** [Katja Sirazitdinova](https://github.com/katjasrz), [Jay Rodge](https://github.com/jayrodge), [Mitesh Patel](https://github.com/patelmiteshn), Developer Advocates @ NVIDIA\n",
    "\n",
    "Special thanks to the Mistral and SGLang teams for their incredible work on these technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fade82-fb97-4ee3-b3f8-e3860260b921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
