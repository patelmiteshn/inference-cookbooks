{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running the Qwen3-Next with SGLang on NVIDIA GPUs\n",
        "\n",
        "This notebook provides a comprehensive guide on how to run models from the Qwen3-Next series using SGLang's high-performance, OpenAI-compatible server. It is divided into two parts, each demonstrating how to set up and interact with a different model variant.\n",
        "\n",
        "\n",
        "#### Launch on NVIDIA Brev\n",
        "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
        "\n",
        "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
        "\n",
        "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)\n",
        "\n",
        "\n",
        "## Part 1: Qwen3-Next-Instruct with SGLang\n",
        "\n",
        "This section covers the `Qwen/Qwen3-Next-80B-A3B-Instruct` model, demonstrating basic chat, streaming, and batch inference.\n",
        "\n",
        "- Model card: [Qwen/Qwen3-Next-80B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct)\n",
        "- SGLang docs: [Qwen3 usage](https://docs.sglang.ai/basic_usage/qwen3.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Part 1: Qwen3-Next-Instruct with SGLang](#Part-1:-Qwen3-Next-Instruct-with-SGLang)\n",
        "  - [Install Dependencies](#Install-Dependencies)\n",
        "  - [Launch SGLang Server](#Launch-SGLang-Server)\n",
        "  - [Client Setup](#Client-Setup)\n",
        "  - [Basic Chat Completion](#Basic-Chat-Completion)\n",
        "  - [Streaming Responses](#Streaming-Responses)\n",
        "  - [Batch Inference](#Batch-Inference)\n",
        "  - [Cleanup](#Cleanup)\n",
        "- [Part 2: Qwen3-Next-Thinking with SGLang](#Part-2:-Qwen3-Next-Thinking-with-SGLang)\n",
        "  - [Launch Thinking Server](#Launch-Thinking-Server)\n",
        "  - [Client Setup (Thinking)](#Client-Setup-(Thinking))\n",
        "  - [Basic Chat (Thinking)](#Basic-Chat-(Thinking))\n",
        "  - [Batch Inference (Thinking)](#Batch-Inference-(Thinking))\n",
        "  - [Cleanup (Thinking)](#Cleanup-(Thinking))\n",
        "- [Resource Notes](#Resource-Notes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "**Hardware:** This notebook is configured by default to run on a machine with **4 GPUs** (`--tp 4`) and sufficient VRAM to hold the 80B parameter model. If your hardware is different, be sure to adjust the `--tp` (tensor parallelism) and other resource-related flags in the server launch command below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (25.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U pip\n",
        "%pip install -U sglang[all] transformers accelerate huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch SGLang server\n",
        "\n",
        "We will launch an OpenAI-compatible server. Adjust `--tp` (tensor parallelism), `--cache-capacity`, and `--context-length` to fit your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:44:52.736000 90116 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:44:52.736000 90116 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:44:59.115000 90357 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:44:59.115000 90357 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:45:00.737000 90354 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:45:00.737000 90354 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:45:00.956000 90355 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:45:00.956000 90355 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:45:01.007000 90356 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:45:01.007000 90356 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "W0919 23:45:01.062000 90353 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:45:01.062000 90353 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank [Gloo] Rank 3 is connected to 13 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
            "\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 23:45:06 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:45:06 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:45:06 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:45:06 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 23:45:08 TP2] using attn output gate!\n",
            "[2025-09-19 23:45:08 TP1] using attn output gate!\n",
            "[2025-09-19 23:45:10 TP0] using attn output gate!\n",
            "[2025-09-19 23:45:10 TP3] using attn output gate!\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:25,  1.54it/s]\n",
            "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:01<00:27,  1.44it/s]\n",
            "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:02<00:27,  1.38it/s]\n",
            "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:02<00:27,  1.36it/s]\n",
            "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:03<00:26,  1.36it/s]\n",
            "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:04<00:26,  1.32it/s]\n",
            "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:05<00:25,  1.32it/s]\n",
            "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:05<00:24,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:06<00:24,  1.31it/s]\n",
            "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:07<00:23,  1.31it/s]\n",
            "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:08<00:17,  1.68it/s]\n",
            "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:09<00:17,  1.58it/s]\n",
            "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:09<00:17,  1.53it/s]\n",
            "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:10<00:18,  1.44it/s]\n",
            "Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:11<00:17,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:12<00:17,  1.40it/s]\n",
            "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:12<00:16,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:13<00:15,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:14<00:15,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:14<00:14,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:15<00:13,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:16<00:12,  1.42it/s]\n",
            "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:17<00:12,  1.40it/s]\n",
            "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:17<00:11,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:18<00:11,  1.36it/s]\n",
            "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:19<00:10,  1.35it/s]\n",
            "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:20<00:09,  1.37it/s]\n",
            "[2025-09-19 23:45:31 TP2] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:20<00:08,  1.34it/s]\n",
            "[2025-09-19 23:45:31 TP1] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:21<00:08,  1.35it/s]\n",
            "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:22<00:07,  1.32it/s]\n",
            "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:23<00:06,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:23<00:06,  1.28it/s]\n",
            "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:24<00:05,  1.26it/s]\n",
            "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:25<00:04,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:26<00:03,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:26<00:03,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:27<00:02,  1.32it/s]\n",
            "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:28<00:01,  1.34it/s]\n",
            "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:29<00:00,  1.35it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:29<00:00,  1.36it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:29<00:00,  1.37it/s]\n",
            "\n",
            "[2025-09-19 23:45:40 TP0] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 23:45:40 TP3] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 23:45:40 TP3] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:45:40 TP0] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:45:40 TP1] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:45:40 TP2] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:45:40 TP2] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:45:40 TP1] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:45:40 TP0] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:45:40 TP3] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "Capturing batches (bs=4 avail_mem=38.02 GB):   0%|          | 0/3 [00:00<?, ?it/s][2025-09-19 23:45:43 TP3] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:45:43 TP2] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:45:43 TP1] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:45:43 TP0] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "Capturing batches (bs=1 avail_mem=37.90 GB): 100%|██████████| 3/3 [00:06<00:00,  2.28s/it]\n",
            "\n",
            "\n",
            "                    NOTE: Typically, the server runs in a separate terminal.\n",
            "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
            "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
            "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
            "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
            "                    \n",
            "SGLang server ready on port 31745 with served name 'qwen3-next-instruct'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sglang.test.doc_patch import launch_server_cmd\n",
        "from sglang.utils import wait_for_server, terminate_process\n",
        "\n",
        "# Choose one model path. For this cookbook, default to the 80B instruct.\n",
        "model_path = os.environ.get(\"QWEN_MODEL\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n",
        "# Set a custom name for the model endpoint, which the client will use.\n",
        "served_name = os.environ.get(\"SERVED_NAME\", \"qwen3-next-instruct\")\n",
        "port = int(os.environ.get(\"SGLANG_PORT\", \"30000\"))\n",
        "\n",
        "server_cmd = f\"\"\"\n",
        "python -m sglang.launch_server \\\n",
        "  --model-path {model_path} \\\n",
        "  --host 0.0.0.0 --port {port} \\\n",
        "  --tp 4 \\\n",
        "  --max-lora-rank 0 \\\n",
        "  --kv-cache-dtype fp8_e4m3 \\\n",
        "  --chunked-prefill-size 8192 \\\n",
        "  --context-length 262144 \\\n",
        "  --served-model-name {served_name} \\\n",
        "  --log-level warning\n",
        "\"\"\"\n",
        "\n",
        "server_process, detected_port = launch_server_cmd(server_cmd)\n",
        "wait_for_server(f\"http://localhost:{detected_port}\")\n",
        "print(f\"SGLang server ready on port {detected_port} with served name '{served_name}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Client setup\n",
        "\n",
        "We'll use the OpenAI-compatible client to talk to SGLang. Set `OPENAI_BASE_URL` to your server address.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client configured to use server at: http://localhost:31745/v1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "base_url = f\"http://localhost:{detected_port}/v1\"\n",
        "api_key = \"EMPTY\"  # SGLang server doesn't require an API key by default\n",
        "\n",
        "client = OpenAI(base_url=base_url, api_key=api_key)\n",
        "print(f\"OpenAI client configured to use server at: {base_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic chat completion\n",
        "\n",
        "Use the chat template; the model supports instruct mode only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** The `extra_body` parameter is an SGLang-specific extension that allows you to pass additional sampling parameters not available in the standard OpenAI `create` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen3-Next is the latest iteration in Alibaba’s Qwen series, designed with enhanced reasoning, multilingual support, and improved efficiency for advanced AI applications.\n"
          ]
        }
      ],
      "source": [
        "resp = client.chat.completions.create(\n",
        "    model=served_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain Qwen3-Next in one sentence.\"}\n",
        "    ],\n",
        "    temperature=0.7,  # per model card best practices\n",
        "    top_p=0.8,\n",
        "    max_tokens=512,\n",
        "    extra_body={\n",
        "        \"top_k\": 20,\n",
        "        \"min_p\": 0.0,\n",
        "    },\n",
        ")\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data grows, model grows—  \n",
            "loss falls with power law grace,  \n",
            "scale brings emergent wit.\n"
          ]
        }
      ],
      "source": [
        "from contextlib import closing\n",
        "\n",
        "with closing(client.chat.completions.create(\n",
        "    model=served_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Stream a short haiku about scaling laws.\"}],\n",
        "    stream=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.8,\n",
        "    extra_body={\n",
        "        \"top_k\": 20,\n",
        "    },\n",
        ")) as stream:\n",
        "    for chunk in stream:\n",
        "        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch inference\n",
        "\n",
        "Send multiple requests concurrently via the `responses.create` API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending batch of requests concurrently...\n",
            "Batch completed in 6.09 seconds.\n",
            "\n",
            "--- Response for Prompt 1: \"Write a Python function to find the nth Fibonacci number with caching.\" ---\n",
            "Here's a Python function to find the nth Fibonacci number using caching (memoization):\n",
            "\n",
            "```python\n",
            "def fibonacci(n, cache=None):\n",
            "    \"\"\"\n",
            "    Find the nth Fibonacci number using caching (memoization).\n",
            "    \n",
            "    Args:\n",
            "        n (int): The position in the Fibonacci sequence (non-negative integer)\n",
            "        cache (dict): Optional cache dictionary to store computed values\n",
            "        \n",
            "    Returns:\n",
            "        int: The nth Fibonacci number\n",
            "        \n",
            "    Raises:\n",
            "        ValueError: If n is negative\n",
            "    \"\"\"\n",
            "    if n < 0:\n",
            "        raise ValueError(\"n must be a non-negative integer\")\n",
            "    \n",
            "    # Initialize cache if not provided\n",
            "    if cache is None:\n",
            "        cache = {}\n",
            "    \n",
            "    # Base cases\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    if n == 1:\n",
            "        return 1\n",
            "    \n",
            "    # Check if result is already in cache\n",
            "    if n in cache:\n",
            "        return cache[n]\n",
            "    \n",
            "    # Calculate Fibonacci number recursively with caching\n",
            "    cache[n] = fibonacci(n - 1, cache) + fibonacci(n - 2, cache)\n",
            "    return cache[n]\n",
            "\n",
            "\n",
            "# Alternative implementation using functools.lru_cache (more Pythonic)\n",
            "from functools import lru_cache\n",
            "\n",
            "@lru_cache(maxsize=None)\n",
            "def fibonacci_lru(n):\n",
            "    \"\"\"\n",
            "    Find the nth Fibonacci number using Python's built-in LRU cache.\n",
            "    \n",
            "    Args:\n",
            "        n (int): The position in the Fibonacci sequence (non-negative integer)\n",
            "        \n",
            "    Returns:\n",
            "        int: The nth Fibonacci number\n",
            "        \n",
            "    Raises:\n",
            "        ValueError: If n is negative\n",
            "    \"\"\"\n",
            "    if n < 0:\n",
            "        raise ValueError(\"n must be a non-negative integer\")\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    if n == 1:\n",
            "        return 1\n",
            "    return fibonacci_lru(n - 1) + fibonacci_lru(n - 2)\n",
            "\n",
            "\n",
            "# Example usage and test function\n",
            "if __name__ == \"__main__\":\n",
            "    # Test the function\n",
            "    print(\"Testing fibonacci function:\")\n",
            "    for i in range(11):\n",
            "        print(f\"F({i}) = {fibonacci(i)}\")\n",
            "    \n",
            "    # Test larger values\n",
            "    print(f\"\\nF(50) = {fibonacci(50)}\")\n",
            "    \n",
            "    # Test the LRU cache version\n",
            "    print(f\"\\nTesting fibonacci_lru function:\")\n",
            "    print(f\"F(50) = {fibonacci_lru(50)}\")\n",
            "    \n",
            "    # Demonstrate cache efficiency\n",
            "    cache = {}\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 2: \"Explain the concept of 'emergent abilities' in large language models in a single paragraph.\" ---\n",
            "Emergent abilities in large language models refer to sophisticated skills or behaviors that appear unexpectedly as the model scales in size—typically beyond a certain threshold of parameters, training data, or compute—rather than being explicitly programmed or gradually refined through smaller-scale training. These abilities, such as complex reasoning, few-shot learning, or multilingual translation, are often absent or minimal in smaller models but become robust and reliable in larger ones, suggesting that scale itself unlocks new computational capacities that are not linearly predictable from smaller variants. This phenomenon challenges traditional assumptions about model development, implying that some capabilities arise not from incremental improvements but from qualitative shifts in how large neural networks represent and process information, potentially due to the emergence of internal structures like latent task representations or improved generalization mechanisms.\n",
            "-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 3: \"Compose a short, rhyming poem about a cat discovering a quantum computer.\" ---\n",
            "A cat named Luna, sleek and sly,  \n",
            "Prowled through the lab where lasers fly.  \n",
            "She spotted a box, cold, gleaming, bright—  \n",
            "A quantum beast in velvet night.  \n",
            "\n",
            "No mouse inside, no yarn to chase,  \n",
            "Just humming lights in quantum space.  \n",
            "She patted the screen with paw so light—  \n",
            "And saw herself… in *all* the night.  \n",
            "\n",
            "One Luna slept. One purred. One flew.  \n",
            "One turned to mist, then turned to dew.  \n",
            "She blinked—then all the versions sighed…  \n",
            "And curled up warm beside her pride.  \n",
            "\n",
            "The scientists gasped, “It’s entangled!”  \n",
            "She yawned, then napped—unperturbed, unscandalized.  \n",
            "For cats know truths the humans miss:  \n",
            "Reality’s just… a nap, in six.\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Use an async client for concurrent requests\n",
        "async_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
        "\n",
        "batch_prompts = [\n",
        "    \"Write a Python function to find the nth Fibonacci number with caching.\",\n",
        "    \"Explain the concept of 'emergent abilities' in large language models in a single paragraph.\",\n",
        "    \"Compose a short, rhyming poem about a cat discovering a quantum computer.\",\n",
        "]\n",
        "\n",
        "async def send_request(prompt: str):\n",
        "    \"\"\"Sends a single chat completion request.\"\"\"\n",
        "    return await async_client.chat.completions.create(\n",
        "        model=served_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        max_tokens=512,\n",
        "        extra_body={\"top_k\": 20},\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Runs all requests concurrently and prints results.\"\"\"\n",
        "    print(\"Sending batch of requests concurrently...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create and run all tasks in parallel\n",
        "    tasks = [send_request(p) for p in batch_prompts]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"Batch completed in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "    # Print results\n",
        "    for i, resp in enumerate(responses):\n",
        "        print(f\"--- Response for Prompt {i+1}: \\\"{batch_prompts[i]}\\\" ---\")\n",
        "        if resp.choices:\n",
        "            print(resp.choices[0].message.content.strip())\n",
        "        else:\n",
        "            print(\"Empty response.\")\n",
        "        print(\"-\" * (40 + len(batch_prompts[i])))\n",
        "        print()\n",
        "\n",
        "# Run the async main function\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "If you launched the server from this notebook, run the following cell to terminate the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed instruct server PID 90116\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ],
      "source": [
        "# Shutdown of the instruct server\n",
        "if 'server_process' in globals() and server_process.poll() is None:\n",
        "    server_process.kill()\n",
        "    print(f\"Killed instruct server PID {server_process.pid}\")\n",
        "else:\n",
        "    print(\"No running instruct server process found to terminate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Qwen3-Next-Thinking with SGLang\n",
        "\n",
        "Now, we will shut down the first server and launch a new one with the `Qwen3-Next-80B-A3B-Thinking` model. This model is specialized for complex reasoning tasks.\n",
        "\n",
        "- Model card: [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)\n",
        "\n",
        "We will use a different port and served name to avoid conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:48:52.678000 97734 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:48:52.678000 97734 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:49:01.075000 97972 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:49:01.075000 97972 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:49:01.185000 97971 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:49:01.185000 97971 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 23:49:01.317000 97969 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:49:01.317000 97969 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "W0919 23:49:01.490000 97970 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:49:01.490000 97970 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "W0919 23:49:01.724000 97973 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 23:49:01.724000 97973 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 2 is connected to [Gloo] Rank 3 peer ranks. Expected number of connected peer ranks is : 33 is connected to 3\n",
            " peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 33 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 33\n",
            "\n",
            "[Gloo] Rank [Gloo] Rank 3 is connected to 32 peer ranks. Expected number of connected peer ranks is :  is connected to 33\n",
            " peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 23:49:05 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:49:05 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:49:05 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 23:49:06 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank [Gloo] Rank 3 is connected to 3 peer ranks. 2Expected number of connected peer ranks is :  is connected to 33\n",
            " peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 23:49:08 TP1] using attn output gate!\n",
            "[2025-09-19 23:49:08 TP2] using attn output gate!\n",
            "[2025-09-19 23:49:10 TP0] using attn output gate!\n",
            "[2025-09-19 23:49:10 TP3] using attn output gate!\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:27,  1.47it/s]\n",
            "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:01<00:28,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:02<00:28,  1.34it/s]\n",
            "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:02<00:27,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:03<00:26,  1.34it/s]\n",
            "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:04<00:27,  1.29it/s]\n",
            "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:05<00:26,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:06<00:25,  1.31it/s]\n",
            "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:06<00:24,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:07<00:24,  1.29it/s]\n",
            "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:08<00:17,  1.66it/s]\n",
            "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:09<00:17,  1.56it/s]\n",
            "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:09<00:17,  1.51it/s]\n",
            "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:10<00:18,  1.42it/s]\n",
            "Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:11<00:18,  1.36it/s]\n",
            "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:12<00:17,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:12<00:16,  1.36it/s]\n",
            "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:13<00:16,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:14<00:15,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:15<00:14,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:15<00:13,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:16<00:12,  1.41it/s]\n",
            "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:17<00:12,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:18<00:11,  1.35it/s]\n",
            "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:18<00:11,  1.35it/s]\n",
            "[2025-09-19 23:49:29 TP2] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 23:49:30 TP1] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:19<00:10,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:20<00:09,  1.35it/s]\n",
            "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:21<00:09,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:21<00:08,  1.34it/s]\n",
            "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:22<00:07,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:23<00:06,  1.29it/s]\n",
            "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:24<00:06,  1.27it/s]\n",
            "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:25<00:05,  1.25it/s]\n",
            "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:25<00:04,  1.29it/s]\n",
            "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:26<00:03,  1.32it/s]\n",
            "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:27<00:03,  1.29it/s]\n",
            "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:28<00:02,  1.30it/s]\n",
            "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:28<00:01,  1.33it/s]\n",
            "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:29<00:00,  1.34it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:30<00:00,  1.34it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:30<00:00,  1.35it/s]\n",
            "\n",
            "[2025-09-19 23:49:41 TP0] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 23:49:41 TP3] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 23:49:41 TP0] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:49:41 TP3] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:49:41 TP1] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:49:41 TP2] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 23:49:41 TP2] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:49:41 TP1] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:49:41 TP3] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 23:49:41 TP0] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "Capturing batches (bs=4 avail_mem=38.02 GB):   0%|          | 0/3 [00:00<?, ?it/s][2025-09-19 23:49:42 TP0] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:49:42 TP3] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:49:42 TP1] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 23:49:42 TP2] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "Capturing batches (bs=1 avail_mem=37.90 GB): 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
            "\n",
            "\n",
            "                    NOTE: Typically, the server runs in a separate terminal.\n",
            "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
            "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
            "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
            "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
            "                    \n",
            "SGLang server for Thinking model ready on port 32502 with served name 'qwen3-next-thinking'\n"
          ]
        }
      ],
      "source": [
        "model_path_thinking = \"Qwen/Qwen3-Next-80B-A3B-Thinking\"\n",
        "served_name_thinking = \"qwen3-next-thinking\"\n",
        "port_thinking = port + 1  \n",
        "\n",
        "server_cmd_thinking = f\"\"\"\n",
        "python -m sglang.launch_server \\\n",
        "  --model-path {model_path_thinking} \\\n",
        "  --host 0.0.0.0 --port {port_thinking} \\\n",
        "  --tp 4 \\\n",
        "  --max-lora-rank 0 \\\n",
        "  --kv-cache-dtype fp8_e4m3 \\\n",
        "  --chunked-prefill-size 8192 \\\n",
        "  --context-length 262144 \\\n",
        "  --served-model-name {served_name_thinking} \\\n",
        "  --log-level warning\n",
        "\"\"\"\n",
        "\n",
        "server_process_thinking, detected_port_thinking = launch_server_cmd(server_cmd_thinking)\n",
        "wait_for_server(f\"http://localhost:{detected_port_thinking}\")\n",
        "print(f\"SGLang server for Thinking model ready on port {detected_port_thinking} with served name '{served_name_thinking}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client configured for Thinking model at: http://localhost:32502/v1\n"
          ]
        }
      ],
      "source": [
        "# Client setup for Thinking model\n",
        "base_url_thinking = f\"http://localhost:{detected_port_thinking}/v1\"\n",
        "client_thinking = OpenAI(base_url=base_url_thinking, api_key=api_key)\n",
        "\n",
        "# Recommended parameters for the Thinking model, per its model card.\n",
        "# https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking#best-practices\n",
        "sampling_params_thinking = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"extra_body\": {\"top_k\": 20, \"min_p\": 0.0},\n",
        "}\n",
        "\n",
        "print(f\"OpenAI client configured for Thinking model at: {base_url_thinking}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic chat (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, the user is asking about the Hybrid Attention mechanism in Qwen3-Next. Hmm, I need to recall what I know about this. Wait, Qwen3-Next isn't a real model yet. The current version is Qwen3, but there's no official \"Qwen3-Next\" announced. Maybe the user is confused or there's a misunderstanding.\n",
            "\n",
            "I should check if there's any information about a model called Qwen3-Next. Let me think... No, as far as I know, Qwen3 is the latest version, and there's no \"Next\" version released. The user might be referring to a hypothetical model or maybe a typo. Alternatively, maybe they heard about some new feature in development but it's not public yet.\n",
            "\n",
            "But since I don't have information on Qwen3-Next, I need to clarify that. The correct response would be to point out that there's no such model as Qwen3-Next. The latest is Qwen3. Maybe the user meant Qwen3? But the question specifically says \"Qwen3-Next,\" which doesn't exist.\n",
            "\n",
            "Wait, perhaps the user is referring to a different model? Or maybe it's a mistake. For example, sometimes people confuse Qwen with other models like Llama or something else. But Qwen is Tongyi Lab's series. Let me confirm: Alibaba's Qwen series has Qwen, Qwen1.5, Qwen2, Qwen2.5, Qwen3. There's no Qwen3-Next officially. So the Hybrid Attention mechanism isn't part of a non-existent model.\n",
            "\n",
            "So the right approach is to explain that Qwen3-Next doesn't exist. The current latest is Qwen3. If the user is asking about attention mechanisms in Qwen3, I can talk about that. But the question specifically says \"Qwen3-Next,\" which isn't real.\n",
            "\n",
            "Alternatively, maybe the user is referring to a future version that's not yet released, but in that case, there's no public information. So I should state that there's no such model as Qwen3-Next, and the latest is Qwen3. If they meant Qwen3, then I can explain its attention mechanism.\n",
            "\n",
            "But the user is asking about \"Hybrid Attention\" in Qwen3-Next. Since that model doesn't exist, the answer should clarify that. Let me check if there's any mention of Hybrid Attention in Qwen3. From what\n"
          ]
        }
      ],
      "source": [
        "resp = client_thinking.chat.completions.create(\n",
        "    model=served_name_thinking,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in complex reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain the Hybrid Attention mechanism in the Qwen3-Next model in a few sentences.\"}\n",
        "    ],\n",
        "    max_tokens=512,\n",
        "    **sampling_params_thinking,\n",
        ")\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch inference (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending batch of requests to Thinking model concurrently...\n",
            "Batch completed in 5.41 seconds.\n",
            "\n",
            "--- Response for Prompt 1: \"Write a Python function to find the nth Fibonacci number with caching, and explain the time complexity.\" ---\n",
            "Okay, the user wants a Python function to find the nth Fibonacci number using caching, along with an explanation of the time complexity. Let me think about how to approach this.\n",
            "\n",
            "First, I recall that the Fibonacci sequence is defined as F(0) = 0, F(1) = 1, and F(n) = F(n-1) + F(n-2) for n > 1. The naive recursive approach has exponential time complexity because it recalculates the same values multiple times. Caching, or memoization, can optimize this by storing previously computed results.\n",
            "\n",
            "So, the plan is to create a function that uses a cache (like a dictionary) to store Fibonacci numbers as they're computed. When the function is called with a specific n, it checks if the result is already in the cache. If yes, it returns it; if not, it computes it recursively, stores it in the cache, and then returns it.\n",
            "\n",
            "In Python, I can use a decorator for memoization. The functools module has a lru_cache decorator that's perfect for this. Alternatively, I can implement the cache manually with a dictionary. Let me consider both approaches.\n",
            "\n",
            "Using lru_cache is straightforward. Let's see:\n",
            "\n",
            "from functools import lru_cache\n",
            "\n",
            "@lru_cache(maxsize=None)\n",
            "def fibonacci(n):\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "But maybe the user expects a manual implementation without using built-in decorators. Let me check the question again. It just says \"with caching\", so either way is acceptable, but perhaps explaining both would be good. However, for simplicity, using lru_cache is concise and clear.\n",
            "\n",
            "Alternatively, manually implementing the cache:\n",
            "\n",
            "def fibonacci(n, cache={}):\n",
            "    if n in cache:\n",
            "        return cache[n]\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    cache[n] = fibonacci(n-1, cache) + fibonacci(n-2, cache)\n",
            "    return cache[n]\n",
            "\n",
            "But wait, using a mutable default argument for cache might be problematic because the cache would persist across function calls. For example, if the function is called multiple times, the cache would accumulate results from all previous calls. That's actually good for efficiency if the function is called repeatedly, but in some contexts, you might not want that. However, for the purpose of this problem, it's acceptable. Alternatively, using a separate cache object that's passed around.\n",
            "\n",
            "But using lru_cache is cleaner. Let's go\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 2: \"Describe the difference between Gated DeltaNet and standard attention.\" ---\n",
            "Okay, the user is asking about the difference between Gated DeltaNet and standard attention. Let me start by recalling what I know about both.\n",
            "\n",
            "First, standard attention, like in Transformers, uses queries, keys, and values. The attention mechanism computes scores between queries and keys, then weights the values. The main issue here is the quadratic complexity with sequence length. For long sequences, this becomes computationally expensive. Also, the attention weights are normalized with softmax, which might not be the most efficient for certain tasks.\n",
            "\n",
            "Now, Gated DeltaNet. I remember it's a newer architecture. It's designed to handle long sequences more efficiently. The key points are the delta rule and gating mechanisms. DeltaNet uses a recurrence relation where each step updates the state based on the current input and the previous state. The \"delta\" part probably refers to how it processes changes or differences in the sequence. \n",
            "\n",
            "Wait, I think DeltaNet replaces the softmax attention with a recurrence that's linear in complexity. So instead of O(n²), it's O(n). That's a big deal for long sequences. The gating mechanism likely controls what information is retained or updated in the state. Maybe it uses gates like in LSTMs or GRUs to decide what to keep or forget.\n",
            "\n",
            "Let me check the components. Standard attention has self-attention layers where each token attends to all others. DeltaNet, on the other hand, processes sequentially with a hidden state that's updated per token. The \"gated\" part probably means there are learnable gates that modulate the state updates. So for each new token, the gate decides how much of the previous state to keep and how much new information to add.\n",
            "\n",
            "I should compare the computational aspects. Standard attention has quadratic complexity because of the matrix multiplication of Q and K. DeltaNet's recurrence is linear, so it scales better. Also, DeltaNet might not need to store all previous states explicitly, which saves memory.\n",
            "\n",
            "Another point is the context handling. Standard attention can access all tokens at once, which is good for global context but expensive. DeltaNet processes step-by-step, so it might have a limited context window unless the state captures long-term dependencies. But the recurrence is designed to maintain long-term memory efficiently.\n",
            "\n",
            "Gating in DeltaNet: probably similar to LSTM gates. There might be update and forget gates that control the flow of information into the hidden state. This could help in retaining important information over long sequences without the quadratic cost.\n",
            "\n",
            "Wait, I think DeltaNet uses a specific recurrence formula. Let\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 3: \"Compose a short, rhyming poem about a Mixture-of-Experts model.\" ---\n",
            "Okay, the user wants a short rhyming poem about a Mixture-of-Experts (MoE) model. Interesting request! They're probably familiar with machine learning concepts but want a creative twist—maybe to share with colleagues or just for fun. \n",
            "\n",
            "Hmm, I should keep it accurate but playful. MoE is all about specialized experts working together under a router, right? Need to highlight: \n",
            "- The router's role in selecting experts \n",
            "- How each expert handles specific tasks \n",
            "- The \"teamwork\" aspect without getting too technical \n",
            "\n",
            "Rhyme scheme... maybe AABB for simplicity? Short lines, 4-6 stanzas max. Avoid jargon like \"softmax\" or \"sparsity\" unless wrapped in metaphor. \n",
            "\n",
            "*Brainstorming lines*: \n",
            "\"Gatekeeper chooses, experts shine\" → too vague. \n",
            "\"Router sends the right task to the right brain\" → better! \"Brain\" for expert? Maybe \"specialist\" sounds more precise. \n",
            "\n",
            "Wait—should I mention \"sparse\" activation? Like \"only a few wake when the call is made\"? Yes! That's core to MoE efficiency. \n",
            "\n",
            "*Testing rhythm*: \n",
            "\"When data flows, the router sees / Which expert's skills will best agree\" → \"agree\" feels off. \"Which expert's skill will best agree\" no... \"best meet the need\"? \n",
            "\n",
            "*Alternative*: \n",
            "\"When data streams arrive, the gate decides / Which specialist's mind the task provides\" → \"provides\" doesn't fit. \"Which specialist's mind should take the ride\"? Clunky. \n",
            "\n",
            "*Better*: \n",
            "\"When data streams arrive, the gate decides / Which expert's mind the task requires\" → \"requires\" is okay but weak. \"Which expert's mind the task requires\" → \"requires\" feels passive. Maybe \"which expert's mind will best respond\"? \n",
            "\n",
            "*Finalizing*: \n",
            "\"Data flows in, the router sees / Which expert's skill will best please\" → \"please\" is cute but inaccurate. Experts don't \"please,\" they \"solve.\" \n",
            "\n",
            "*Ah!*: \n",
            "\"Data flows in, the router knows / Which expert's mind the task best shows\" → \"shows\" is odd. \"Which expert's mind the task best suits\"? \n",
            "\n",
            "*Yes*: \n",
            "\"Data flows in, the router knows / Which expert's mind the task best suits\" \n",
            "\n",
            "Now the expert part: \n",
            "\"Each has a specialty, a focused art / To solve the part that fits its heart\" →\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use an async client for concurrent requests\n",
        "async_client_thinking = AsyncOpenAI(base_url=base_url_thinking, api_key=api_key)\n",
        "\n",
        "batch_prompts_thinking = [\n",
        "    \"Write a Python function to find the nth Fibonacci number with caching, and explain the time complexity.\",\n",
        "    \"Describe the difference between Gated DeltaNet and standard attention.\",\n",
        "    \"Compose a short, rhyming poem about a Mixture-of-Experts model.\",\n",
        "]\n",
        "\n",
        "print(\"Sending batch of requests to Thinking model concurrently...\")\n",
        "start_time = time.time()\n",
        "\n",
        "tasks = [\n",
        "    async_client_thinking.chat.completions.create(\n",
        "        model=served_name_thinking,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": p}\n",
        "        ],\n",
        "        max_tokens=512,\n",
        "        **sampling_params_thinking,\n",
        "    ) for p in batch_prompts_thinking\n",
        "]\n",
        "responses = await asyncio.gather(*tasks)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Batch completed in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "for i, resp in enumerate(responses):\n",
        "    print(f\"--- Response for Prompt {i+1}: \\\"{batch_prompts_thinking[i]}\\\" ---\")\n",
        "    if resp.choices:\n",
        "        print(resp.choices[0].message.content.strip())\n",
        "    else:\n",
        "        print(\"Empty response.\")\n",
        "    print(\"-\" * (40 + len(batch_prompts_thinking[i])))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed thinking server PID 97734\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ],
      "source": [
        "# Shutdown of the thinking server\n",
        "if 'server_process_thinking' in globals() and server_process_thinking.poll() is None:\n",
        "    server_process_thinking.kill()\n",
        "    print(f\"Killed thinking server PID {server_process_thinking.pid}\")\n",
        "else:\n",
        "    print(\"No running thinking server process found to terminate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resource Notes\n",
        "\n",
        "- **Hardware**: Qwen3-Next-80B-A3B is a large model. Multi-GPU tensor parallel (`--tp`) is highly recommended for acceptable performance.\n",
        "- **Quantization**: For environments with limited resources, consider using quantized versions of the model (e.g., AWQ, GPTQ, INT4/INT8) if available. These can significantly reduce memory usage at the cost of some accuracy. SGLang supports various quantization formats.\n",
        "- **Offloading**: For development or low-throughput scenarios, you can explore smaller models from the Qwen3 family or use model offloading to run the 80B parameter model on systems with less VRAM.\n",
        "- **Network**: Ensure you have sufficient network and disk bandwidth for the initial model download, as the weights are very large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown\n"
        }
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "Congratulations! You successfully deployed the `Qwen3-Next` models using SGLang.\n",
        "\n",
        "In this notebook, you have learned how to:\n",
        "- Set up your environment and install the SGLang library.\n",
        "- Launch and manage an OpenAI-compatible SGLang server for the Instruct model.\n",
        "- Perform basic chat, streaming, and batch inference using the OpenAI client.\n",
        "- Launch a second SGLang server for the Thinking model and run inference.\n",
        "\n",
        "You can adapt tensor parallelism, ports, and sampling parameters to your hardware and application needs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
