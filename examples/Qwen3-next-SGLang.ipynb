{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running the Qwen3-Next with SGLang on NVIDIA GPUs\n",
        "\n",
        "This notebook provides a comprehensive guide on how to run models from the Qwen3-Next series using SGLang's high-performance, OpenAI-compatible server. It is divided into two parts, each demonstrating how to set up and interact with a different model variant.\n",
        "\n",
        "\n",
        "\n",
        "## Part 1: Qwen3-Next-Instruct with SGLang\n",
        "\n",
        "This section covers the `Qwen/Qwen3-Next-80B-A3B-Instruct` model, demonstrating basic chat, streaming, and batch inference.\n",
        "\n",
        "- Model card: [Qwen/Qwen3-Next-80B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct)\n",
        "- SGLang docs: [Qwen3 usage](https://docs.sglang.ai/basic_usage/qwen3.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "**Hardware:** This notebook is configured by default to run on a machine with **4 GPUs** (`--tp 4`) and sufficient VRAM to hold the 80B parameter model. If your hardware is different, be sure to adjust the `--tp` (tensor parallelism) and other resource-related flags in the server launch command below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (25.2)\n",
            "Requirement already satisfied: transformers in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (4.56.1)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: accelerate in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (1.10.1)\n",
            "Requirement already satisfied: huggingface_hub in /home/shadeform/.local/lib/python3.10/site-packages (0.35.0)\n",
            "Requirement already satisfied: sglang[all] in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (0.5.2)\n",
            "Requirement already satisfied: aiohttp in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (3.12.15)\n",
            "Requirement already satisfied: requests in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (2.32.5)\n",
            "Requirement already satisfied: tqdm in /home/shadeform/.local/lib/python3.10/site-packages (from sglang[all]) (4.67.1)\n",
            "Requirement already satisfied: numpy in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (2.2.6)\n",
            "Requirement already satisfied: IPython in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (8.37.0)\n",
            "Requirement already satisfied: setproctitle in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.3.7)\n",
            "Requirement already satisfied: filelock in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/shadeform/.local/lib/python3.10/site-packages (from huggingface_hub) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shadeform/.local/lib/python3.10/site-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/shadeform/.local/lib/python3.10/site-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: psutil in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from accelerate) (7.1.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from accelerate) (2.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (78.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from aiohttp->sglang[all]) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->sglang[all]) (3.10)\n",
            "Requirement already satisfied: decorator in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (1.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/shadeform/.local/lib/python3.10/site-packages (from IPython->sglang[all]) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (2.19.2)\n",
            "Requirement already satisfied: stack_data in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from IPython->sglang[all]) (5.14.3)\n",
            "Requirement already satisfied: wcwidth in /home/shadeform/.local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->sglang[all]) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from jedi>=0.16->IPython->sglang[all]) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pexpect>4.3->IPython->sglang[all]) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from requests->sglang[all]) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from requests->sglang[all]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from requests->sglang[all]) (2025.8.3)\n",
            "Requirement already satisfied: anthropic>=0.20.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.68.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from anthropic>=0.20.0->sglang[all]) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from httpx<1,>=0.25.0->anthropic>=0.20.0->sglang[all]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic>=0.20.0->sglang[all]) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.20.0->sglang[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.20.0->sglang[all]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.20.0->sglang[all]) (0.4.1)\n",
            "Requirement already satisfied: decord in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.6.0)\n",
            "Requirement already satisfied: openai==1.99.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.99.1)\n",
            "Requirement already satisfied: tiktoken in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.11.0)\n",
            "Requirement already satisfied: sgl-kernel==0.3.9.post2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.3.9.post2)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (2.8.0)\n",
            "Requirement already satisfied: torchvision in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.23.0)\n",
            "Requirement already satisfied: cuda-python in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (13.0.1)\n",
            "Requirement already satisfied: flashinfer_python==0.3.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.3.1)\n",
            "Requirement already satisfied: ninja in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (1.13.0)\n",
            "Requirement already satisfied: pynvml in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (13.0.1)\n",
            "Requirement already satisfied: einops in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (0.8.1)\n",
            "Requirement already satisfied: click in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (8.3.0)\n",
            "Requirement already satisfied: tabulate in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (0.9.0)\n",
            "Requirement already satisfied: nvidia-cudnn-frontend>=1.13.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from flashinfer_python==0.3.1->sglang[all]) (1.14.1)\n",
            "Requirement already satisfied: cuda-bindings~=13.0.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from cuda-python->sglang[all]) (13.0.1)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from cuda-python->sglang[all]) (1.2.3)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.0.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pynvml->flashinfer_python==0.3.1->sglang[all]) (13.580.82)\n",
            "Requirement already satisfied: blobfile==3.0.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (3.0.0)\n",
            "Requirement already satisfied: build in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.3.0)\n",
            "Requirement already satisfied: compressed-tensors in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.11.0)\n",
            "Requirement already satisfied: datasets in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (4.1.1)\n",
            "Requirement already satisfied: fastapi in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.116.2)\n",
            "Requirement already satisfied: hf_transfer in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.1.9)\n",
            "Requirement already satisfied: interegular in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.3.3)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.7.30)\n",
            "Requirement already satisfied: modelscope in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.30.0)\n",
            "Requirement already satisfied: msgspec in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.19.0)\n",
            "Requirement already satisfied: openai-harmony==0.0.4 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.0.4)\n",
            "Requirement already satisfied: orjson in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (3.11.3)\n",
            "Requirement already satisfied: outlines==0.1.11 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.1.11)\n",
            "Requirement already satisfied: partial_json_parser in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.2.1.1.post6)\n",
            "Requirement already satisfied: pillow in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (11.3.0)\n",
            "Requirement already satisfied: prometheus-client>=0.20.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.23.1)\n",
            "Requirement already satisfied: pybase64 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.4.2)\n",
            "Requirement already satisfied: python-multipart in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.0.20)\n",
            "Requirement already satisfied: pyzmq>=25.1.2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (27.1.0)\n",
            "Requirement already satisfied: sentencepiece in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.2.1)\n",
            "Requirement already satisfied: soundfile==0.13.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.13.1)\n",
            "Requirement already satisfied: scipy in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.15.3)\n",
            "Requirement already satisfied: timm==1.0.16 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (1.0.16)\n",
            "Requirement already satisfied: torchao==0.9.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.9.0)\n",
            "Requirement already satisfied: uvicorn in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.35.0)\n",
            "Requirement already satisfied: uvloop in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.21.0)\n",
            "Requirement already satisfied: xgrammar==0.1.24 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.1.24)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from blobfile==3.0.0->sglang[all]) (3.23.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from blobfile==3.0.0->sglang[all]) (6.0.1)\n",
            "Requirement already satisfied: lark in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (1.2.2)\n",
            "Requirement already satisfied: nest_asyncio in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (3.1.1)\n",
            "Requirement already satisfied: diskcache in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (5.6.3)\n",
            "Requirement already satisfied: referencing in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (4.25.1)\n",
            "Requirement already satisfied: pycountry in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (20250909)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from outlines==0.1.11->sglang[all]) (0.1.26)\n",
            "Requirement already satisfied: cffi>=1.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from soundfile==0.13.1->sglang[all]) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from cffi>=1.0->soundfile==0.13.1->sglang[all]) (2.23)\n",
            "Requirement already satisfied: pyproject_hooks in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from build->sglang[all]) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from build->sglang[all]) (2.2.1)\n",
            "Requirement already satisfied: frozendict in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from compressed-tensors->sglang[all]) (2.4.6)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from datasets->sglang[all]) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from datasets->sglang[all]) (0.4.0)\n",
            "Requirement already satisfied: pandas in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from datasets->sglang[all]) (2.3.2)\n",
            "Requirement already satisfied: xxhash in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from datasets->sglang[all]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from datasets->sglang[all]) (0.70.16)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from fastapi->sglang[all]) (0.48.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->sglang[all]) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->sglang[all]) (0.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pandas->datasets->sglang[all]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pandas->datasets->sglang[all]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from pandas->datasets->sglang[all]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->sglang[all]) (1.17.0)\n",
            "Requirement already satisfied: torch_memory_saver==0.0.8 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from sglang[all]) (0.0.8)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from stack_data->IPython->sglang[all]) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from stack_data->IPython->sglang[all]) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages (from stack_data->IPython->sglang[all]) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "# Run once per environment\n",
        "%pip install --upgrade pip\n",
        "%pip install -U sglang[all] transformers accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch SGLang server\n",
        "\n",
        "We will launch an OpenAI-compatible server. Adjust `--tp` (tensor parallelism), `--cache-capacity`, and `--context-length` to fit your hardware.\n",
        "\n",
        "- For long context, enable YaRN by passing `--json-model-override-args`.\n",
        "- For 80B, ensure sufficient VRAM and consider tensor parallel across multiple GPUs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Programmatic server launch (helper)\n",
        "\n",
        "Use `launch_server_cmd` to start SGLang from Python and `wait_for_server` to block until ready. This example mirrors the terminal command above and sets a served model name for client requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 17:55:40.645000 59745 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:40.645000 59745 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 17:55:48.057000 59948 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:48.057000 59948 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "W0919 17:55:48.142000 59949 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:48.142000 59949 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "All deep_gemm operations loaded successfully!\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0919 17:55:49.109000 59950 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:49.109000 59950 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "W0919 17:55:49.116000 59951 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:49.116000 59951 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "W0919 17:55:49.552000 59952 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0919 17:55:49.552000 59952 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank [Gloo] Rank 3 is connected to 23 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
            "\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 3\n",
            "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 17:55:52 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 17:55:52 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 17:55:52 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[2025-09-19 17:55:52 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
            "[2025-09-19 17:55:54 TP1] using attn output gate!\n",
            "[2025-09-19 17:55:55 TP2] using attn output gate!\n",
            "[2025-09-19 17:55:55 TP0] using attn output gate!\n",
            "[2025-09-19 17:55:55 TP3] using attn output gate!\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:17,  2.23it/s]\n",
            "Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:19,  2.02it/s]\n",
            "Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:19,  1.93it/s]\n",
            "Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:02<00:19,  1.93it/s]\n",
            "Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:02<00:18,  1.95it/s]\n",
            "Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:03<00:18,  1.92it/s]\n",
            "Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:03<00:18,  1.84it/s]\n",
            "Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:04<00:17,  1.87it/s]\n",
            "Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:04<00:16,  1.91it/s]\n",
            "Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:05<00:16,  1.90it/s]\n",
            "Loading safetensors checkpoint shards:  27% Completed | 11/41 [00:05<00:15,  1.88it/s]\n",
            "Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:06<00:15,  1.91it/s]\n",
            "Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:06<00:14,  1.95it/s]\n",
            "Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:07<00:13,  1.94it/s]\n",
            "Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:07<00:13,  1.94it/s]\n",
            "Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:08<00:09,  2.52it/s]\n",
            "Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:08<00:09,  2.36it/s]\n",
            "Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:09<00:09,  2.21it/s]\n",
            "Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:09<00:09,  2.12it/s]\n",
            "Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:10<00:09,  2.07it/s]\n",
            "Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:10<00:09,  1.99it/s]\n",
            "Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:11<00:09,  2.00it/s]\n",
            "Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:11<00:08,  2.00it/s]\n",
            "Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:12<00:08,  1.98it/s]\n",
            "Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:12<00:07,  1.96it/s]\n",
            "Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:13<00:07,  1.96it/s]\n",
            "Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:14<00:06,  1.88it/s]\n",
            "Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:14<00:06,  1.88it/s]\n",
            "Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:15<00:05,  1.90it/s]\n",
            "Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:15<00:05,  1.82it/s]\n",
            "Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:16<00:04,  1.85it/s]\n",
            "Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:16<00:04,  1.83it/s]\n",
            "Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:17<00:03,  1.83it/s]\n",
            "Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:17<00:03,  1.85it/s]\n",
            "Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:18<00:02,  1.88it/s]\n",
            "Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:18<00:02,  1.88it/s]\n",
            "Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:19<00:01,  1.89it/s]\n",
            "Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:19<00:01,  1.87it/s]\n",
            "Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:20<00:00,  1.82it/s]\n",
            "[2025-09-19 17:56:16 TP1] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 17:56:16 TP2] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:21<00:00,  1.83it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:21<00:00,  1.94it/s]\n",
            "\n",
            "[2025-09-19 17:56:16 TP0] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 17:56:17 TP3] Using FP8 KV cache but no scaling factors provided. Defaulting to scaling factors of 1.0. This may lead to less accurate results!\n",
            "[2025-09-19 17:56:17 TP3] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 17:56:17 TP0] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 17:56:17 TP1] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 17:56:17 TP2] Hybrid GDN model detected, disable radix cache\n",
            "[2025-09-19 17:56:17 TP0] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 17:56:17 TP1] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 17:56:17 TP3] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "[2025-09-19 17:56:17 TP2] Current Python version 3.10 is below the recommended 3.11 version. It is recommended to upgrade to Python 3.11 or higher for the best experience.\n",
            "Capturing batches (bs=4 avail_mem=38.02 GB):   0%|          | 0/3 [00:00<?, ?it/s][2025-09-19 17:56:18 TP2] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 17:56:18 TP1] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 17:56:18 TP3] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "[2025-09-19 17:56:18 TP0] Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /home/shadeform/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=512,N=128,device_name=NVIDIA_H100_PCIe.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton\n",
            "Capturing batches (bs=1 avail_mem=37.90 GB): 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n",
            "\n",
            "\n",
            "                    NOTE: Typically, the server runs in a separate terminal.\n",
            "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
            "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
            "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
            "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
            "                    \n",
            "SGLang server ready on port 30466 with served name 'qwen3-next'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sglang.test.doc_patch import launch_server_cmd\n",
        "from sglang.utils import wait_for_server, terminate_process\n",
        "\n",
        "# Choose one model path. For this cookbook, default to the 80B instruct.\n",
        "model_path = os.environ.get(\"QWEN_MODEL\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n",
        "# Set a custom name for the model endpoint, which the client will use.\n",
        "served_name = os.environ.get(\"SERVED_NAME\", \"qwen3-next-instruct\")\n",
        "port = int(os.environ.get(\"SGLANG_PORT\", \"30000\"))\n",
        "\n",
        "server_cmd = f\"\"\"\n",
        "python -m sglang.launch_server \\\n",
        "  --model-path \"{model_path}\" \\\n",
        "  --host 0.0.0.0 --port {port} \\\n",
        "  --tp 4 \\\n",
        "  --max-lora-rank 0 \\\n",
        "  --kv-cache-dtype fp8_e4m3 \\\n",
        "  --chunked-prefill-size 8192 \\\n",
        "  --context-length 262144 \\\n",
        "  --served-model-name {served_name} \\\n",
        "  --log-level warning\n",
        "\"\"\"\n",
        "\n",
        "server_process, detected_port = launch_server_cmd(server_cmd)\n",
        "wait_for_server(f\"http://localhost:{detected_port}\")\n",
        "print(f\"SGLang server ready on port {detected_port} with served name '{served_name}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Client setup\n",
        "\n",
        "We'll use the OpenAI-compatible client to talk to SGLang. Set `OPENAI_BASE_URL` to your server address.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client configured to use server at: http://localhost:35961/v1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "base_url = f\"http://localhost:{detected_port}/v1\"\n",
        "api_key = \"EMPTY\"  # SGLang server doesn't require an API key by default\n",
        "\n",
        "client = OpenAI(base_url=base_url, api_key=api_key)\n",
        "print(f\"OpenAI client configured to use server at: {base_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic chat completion\n",
        "\n",
        "Use the chat template; the model supports instruct mode only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** The `extra_body` parameter is an SGLang-specific extension that allows you to pass additional sampling parameters not available in the standard OpenAI `create` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen3-Next is the latest iteration in Alibaba’s Qwen series of large language models, featuring enhanced reasoning, multilingual support, and improved efficiency for complex tasks and real-world applications.\n"
          ]
        }
      ],
      "source": [
        "resp = client.chat.completions.create(\n",
        "    model=served_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain Qwen3-Next in one sentence.\"}\n",
        "    ],\n",
        "    temperature=0.7,  # per model card best practices\n",
        "    top_p=0.8,\n",
        "    max_tokens=512,\n",
        "    extra_body={\n",
        "        \"top_k\": 20,\n",
        "        \"min_p\": 0.0,\n",
        "    },\n",
        ")\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data grows, model grows—  \n",
            "loss drops with power law grace,  \n",
            "scaling’s quiet law.\n"
          ]
        }
      ],
      "source": [
        "from contextlib import closing\n",
        "\n",
        "with closing(client.chat.completions.create(\n",
        "    model=served_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Stream a short haiku about scaling laws.\"}],\n",
        "    stream=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.8,\n",
        "    extra_body={\n",
        "        \"top_k\": 20,\n",
        "    },\n",
        ")) as stream:\n",
        "    for chunk in stream:\n",
        "        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch inference\n",
        "\n",
        "Send multiple requests concurrently via the `responses.create` API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending batch of requests concurrently...\n",
            "Batch completed in 5.69 seconds.\n",
            "\n",
            "--- Response for Prompt 1: \"Give me 3 bullet points on Qwen3-Next.\" ---\n",
            "Actually, as of now, there is no official model named **Qwen3-Next** released by Alibaba’s Tongyi Lab. The latest publicly available model in the Qwen series is **Qwen3**, which was launched in May 2024.\n",
            "\n",
            "If you’re referring to **Qwen3**, here are 3 key bullet points about it:\n",
            "\n",
            "- **Enhanced Reasoning & Coding**: Qwen3 significantly improves logical reasoning, mathematical problem-solving, and code generation capabilities, outperforming its predecessors and competing models in benchmarks like GSM8K and HumanEval.\n",
            "\n",
            "- **Multilingual Support**: It offers robust support for over 100 languages, making it highly effective for global applications and cross-lingual tasks, including low-resource languages.\n",
            "\n",
            "- **Larger Context & Efficiency**: Qwen3 supports up to 32K tokens of context length and features optimized inference efficiency, enabling better performance on long-document understanding and complex multi-turn conversations.\n",
            "\n",
            "If “Qwen3-Next” refers to a future or internal prototype, no official details are available yet. Stay tuned to the [Qwen official website](https://qwenlm.github.io/) for updates.\n",
            "------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 2: \"Summarize MoE advantages in two sentences.\" ---\n",
            "Mixture of Experts (MoE) improves model efficiency by activating only a subset of specialized sub-networks (experts) for each input, reducing computational cost compared to dense models of similar capacity. This sparse activation enables scaling to vastly larger models with manageable inference costs, while maintaining or even enhancing performance through expert specialization.\n",
            "----------------------------------------------------------------------------------\n",
            "\n",
            "--- Response for Prompt 3: \"What is YaRN and why use it?\" ---\n",
            "**YaRN** (Yet another RoPE-based **N**eural **N**etwork **E**xtension) is a technique developed to **extend the context length** of transformer-based language models—particularly those that use **Rotary Position Embeddings (RoPE)**—without requiring retraining. It was introduced in 2023 by researchers from the University of California, Berkeley, and others, in response to the growing demand for models that can handle long-context inputs (e.g., 32K, 128K, or even 1M tokens).\n",
            "\n",
            "---\n",
            "\n",
            "### 🔍 What is YaRN?\n",
            "\n",
            "YaRN is an **inference-time method** that modifies the **RoPE position embeddings** used in models like LLaMA, Mistral, and others to **extrapolate** beyond their original training context window (e.g., from 4K to 32K or more).\n",
            "\n",
            "#### Key Idea:\n",
            "RoPE encodes absolute position information using sine/cosine functions with a frequency parameter. During training, the model learns to associate these frequencies with token positions up to a maximum length (e.g., 4096). But when you want to use the model for longer sequences (e.g., 32K), the original RoPE frequencies cause **positional information to become ambiguous or distorted**.\n",
            "\n",
            "YaRN solves this by:\n",
            "1. **Scaling the RoPE frequencies** to compress the original positional range into a smaller angular space.\n",
            "2. **Adjusting the base frequency** (`theta`) in a non-linear, adaptive way based on the target context length.\n",
            "3. **Using a temperature parameter** (`tau`) to control how aggressively the frequencies are scaled, preserving the model’s ability to distinguish between close positions.\n",
            "\n",
            "This allows the model to **extrapolate** position embeddings gracefully to much longer sequences—**without retraining**.\n",
            "\n",
            "---\n",
            "\n",
            "### ✅ Why Use YaRN?\n",
            "\n",
            "| Reason | Explanation |\n",
            "|--------|-------------|\n",
            "| **No retraining needed** | YaRN works purely at inference time. You can take a pretrained model (e.g., LLaMA-7B with 4K context) and instantly extend its context to 32K+ by adjusting RoPE parameters. |\n",
            "| **High performance** | YaRN significantly outperforms naive scaling methods (like linear or NTK-aware scaling) in benchmarks like LongBench, L-Eval, and others. |\n",
            "| **Preserves model quality** | Unlike simple linear extrapolation,\n",
            "--------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Use an async client for concurrent requests\n",
        "async_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
        "\n",
        "batch_prompts = [\n",
        "    \"Write a Python function to find the nth Fibonacci number with caching.\",\n",
        "    \"Explain the concept of 'emergent abilities' in large language models in a single paragraph.\",\n",
        "    \"Compose a short, rhyming poem about a cat discovering a quantum computer.\",\n",
        "]\n",
        "\n",
        "async def send_request(prompt: str):\n",
        "    \"\"\"Sends a single chat completion request.\"\"\"\n",
        "    return await async_client.chat.completions.create(\n",
        "        model=served_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        max_tokens=512,\n",
        "        extra_body={\"top_k\": 20},\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Runs all requests concurrently and prints results.\"\"\"\n",
        "    print(\"Sending batch of requests concurrently...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create and run all tasks in parallel\n",
        "    tasks = [send_request(p) for p in batch_prompts]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"Batch completed in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "    # Print results\n",
        "    for i, resp in enumerate(responses):\n",
        "        print(f\"--- Response for Prompt {i+1}: \\\"{batch_prompts[i]}\\\" ---\")\n",
        "        if resp.choices:\n",
        "            print(resp.choices[0].message.content.strip())\n",
        "        else:\n",
        "            print(\"Empty response.\")\n",
        "        print(\"-\" * (40 + len(batch_prompts[i])))\n",
        "        print()\n",
        "\n",
        "# Run the async main function\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "If you launched the server from this notebook, run the following cell to terminate the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated server process with PID: 37934\n"
          ]
        }
      ],
      "source": [
        "# Terminate the server process if it was started programmatically\n",
        "server_stopped = False\n",
        "if \"server_process\" in locals() and server_process.poll() is None:\n",
        "    terminate_process(server_process)\n",
        "    print(f\"Terminated server process with PID: {server_process.pid}\")\n",
        "    server_stopped = True\n",
        "\n",
        "if not server_stopped:\n",
        "    print(\"No running server process found to terminate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Qwen3-Next-Thinking with SGLang\n",
        "\n",
        "Now, we will shut down the first server and launch a new one with the `Qwen3-Next-80B-A3B-Thinking` model. This model is specialized for complex reasoning tasks.\n",
        "\n",
        "- Model card: [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)\n",
        "\n",
        "We will use a different port and served name to avoid conflicts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch server for the Thinking model\n",
        "model_path_thinking = \"Qwen/Qwen3-Next-80B-A3B-Thinking\"\n",
        "served_name_thinking = \"qwen3-next-thinking\"\n",
        "port_thinking = port + 1  # Use a different port\n",
        "\n",
        "server_cmd_thinking = f\"\"\"\n",
        "python -m sglang.launch_server \\\n",
        "  --model-path \"{model_path_thinking}\" \\\n",
        "  --host 0.0.0.0 --port {port_thinking} \\\n",
        "  --tp 4 \\\n",
        "  --max-lora-rank 0 \\\n",
        "  --kv-cache-dtype fp8_e4m3 \\\n",
        "  --chunked-prefill-size 8192 \\\n",
        "  --context-length 262144 \\\n",
        "  --served-model-name {served_name_thinking} \\\n",
        "  --log-level warning\n",
        "\"\"\"\n",
        "\n",
        "server_process_thinking, detected_port_thinking = launch_server_cmd(server_cmd_thinking)\n",
        "wait_for_server(f\"http://localhost:{detected_port_thinking}\")\n",
        "print(f\"SGLang server for Thinking model ready on port {detected_port_thinking} with served name '{served_name_thinking}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Client setup for Thinking model\n",
        "base_url_thinking = f\"http://localhost:{detected_port_thinking}/v1\"\n",
        "client_thinking = OpenAI(base_url=base_url_thinking, api_key=api_key)\n",
        "\n",
        "# Recommended parameters for the Thinking model, per its model card.\n",
        "# https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking#best-practices\n",
        "sampling_params_thinking = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"extra_body\": {\"top_k\": 20, \"min_p\": 0.0},\n",
        "}\n",
        "\n",
        "print(f\"OpenAI client configured for Thinking model at: {base_url_thinking}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic chat (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resp = client_thinking.chat.completions.create(\n",
        "    model=served_name_thinking,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in complex reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain the Hybrid Attention mechanism in the Qwen3-Next model in a few sentences.\"}\n",
        "    ],\n",
        "    max_tokens=512,\n",
        "    **sampling_params_thinking,\n",
        ")\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch inference (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use an async client for concurrent requests\n",
        "async_client_thinking = AsyncOpenAI(base_url=base_url_thinking, api_key=api_key)\n",
        "\n",
        "batch_prompts_thinking = [\n",
        "    \"Write a Python function to find the nth Fibonacci number with caching, and explain the time complexity.\",\n",
        "    \"Describe the difference between Gated DeltaNet and standard attention.\",\n",
        "    \"Compose a short, rhyming poem about a Mixture-of-Experts model.\",\n",
        "]\n",
        "\n",
        "async def send_request_thinking(prompt: str):\n",
        "    \"\"\"Sends a single chat completion request to the Thinking server.\"\"\"\n",
        "    return await async_client_thinking.chat.completions.create(\n",
        "        model=served_name_thinking,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=512,\n",
        "        **sampling_params_thinking,\n",
        "    )\n",
        "\n",
        "async def main_thinking():\n",
        "    \"\"\"Runs all requests concurrently and prints results.\"\"\"\n",
        "    print(\"Sending batch of requests to Thinking model concurrently...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    tasks = [send_request_thinking(p) for p in batch_prompts_thinking]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"Batch completed in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "    for i, resp in enumerate(responses):\n",
        "        print(f\"--- Response for Prompt {i+1}: \\\"{batch_prompts_thinking[i]}\\\" ---\")\n",
        "        if resp.choices:\n",
        "            print(resp.choices[0].message.content.strip())\n",
        "        else:\n",
        "            print(\"Empty response.\")\n",
        "        print(\"-\" * (40 + len(batch_prompts_thinking[i])))\n",
        "        print()\n",
        "\n",
        "# Run the async main function\n",
        "await main_thinking()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup (Thinking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No running Thinking server process found to terminate.\n"
          ]
        }
      ],
      "source": [
        "# Terminate the server process if it was started programmatically\n",
        "server_stopped = False\n",
        "if \"server_process_thinking\" in locals() and server_process_thinking.poll() is None:\n",
        "    terminate_process(server_process_thinking)\n",
        "    print(f\"Terminated Thinking server process with PID: {server_process_thinking.pid}\")\n",
        "    server_stopped = True\n",
        "\n",
        "if not server_stopped:\n",
        "    print(\"No running Thinking server process found to terminate.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resource notes and quantization options\n",
        "\n",
        "- Qwen3-Next-80B-A3B is large; multi-GPU tensor parallel (`--tp`) is recommended.\n",
        "- Consider FP8 KV cache and chunked prefill to reduce memory.\n",
        "- Quantized variants (e.g., AWQ/GPTQ/INT4/INT8) may be available from community fine-tunes; check model hub.\n",
        "- For CPU/off-GPU hosting, consider smaller Qwen3-family models or quantized 80B with reduced context.\n",
        "- Ensure network and disk bandwidth are sufficient for first-time model downloads.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sgl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
